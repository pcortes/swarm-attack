"""
Coder Agent for Feature Swarm.

This agent implements production code that makes tests pass. It reads
the test file generated by TestWriterAgent, understands the requirements,
and generates implementation code using Claude CLI.
"""

from __future__ import annotations

import json
import re
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from swarm_attack.agents.base import AgentResult, BaseAgent, SkillNotFoundError
from swarm_attack.llm_clients import ClaudeInvocationError, ClaudeTimeoutError
from swarm_attack.utils.fs import ensure_dir, file_exists, read_file, safe_write

if TYPE_CHECKING:
    from swarm_attack.config import SwarmConfig
    from swarm_attack.llm_clients import ClaudeCliRunner
    from swarm_attack.logger import SwarmLogger
    from swarm_attack.state_store import StateStore


class CoderAgent(BaseAgent):
    """
    Agent that implements code to make tests pass.

    Follows TDD principles: tests exist first, implementation comes second.
    """

    name = "coder"

    def __init__(
        self,
        config: SwarmConfig,
        logger: Optional[SwarmLogger] = None,
        llm_runner: Optional[ClaudeCliRunner] = None,
        state_store: Optional[StateStore] = None,
    ) -> None:
        """Initialize the Coder agent."""
        super().__init__(config, logger, llm_runner, state_store)
        self._skill_prompt: Optional[str] = None

    def _get_spec_path(self, feature_id: str) -> Path:
        """Get the path to the spec-final.md file."""
        return self.config.specs_path / feature_id / "spec-final.md"

    def _get_issues_path(self, feature_id: str) -> Path:
        """Get the path to the issues.json file."""
        return self.config.specs_path / feature_id / "issues.json"

    def _get_default_test_path(self, feature_id: str, issue_number: int) -> Path:
        """Get the default path for generated tests."""
        tests_dir = Path(self.config.repo_root) / "tests" / "generated" / feature_id
        return tests_dir / f"test_issue_{issue_number}.py"

    def _load_skill_prompt(self) -> str:
        """Load and cache the skill prompt."""
        if self._skill_prompt is None:
            self._skill_prompt = self.load_skill("coder")
        return self._skill_prompt

    def _load_issues(self, feature_id: str) -> dict[str, Any]:
        """Load the issues.json file for a feature."""
        issues_path = self._get_issues_path(feature_id)
        if not file_exists(issues_path):
            raise FileNotFoundError(f"Issues file not found at {issues_path}")

        content = read_file(issues_path)
        return json.loads(content)

    def _find_issue(
        self, issues_data: dict[str, Any], issue_number: int
    ) -> Optional[dict[str, Any]]:
        """Find a specific issue by its order number."""
        issues = issues_data.get("issues", [])
        for issue in issues:
            if issue.get("order") == issue_number:
                return issue
        return None

    def _extract_expected_modules(self, test_content: str) -> list[str]:
        """
        Extract module paths from import statements.

        Parses the test file to find what modules it imports,
        which tells us what files need to be created.

        Args:
            test_content: Content of the test file.

        Returns:
            List of module paths (e.g., ["src/auth/signup.py"]).
        """
        # Match: from src.auth.signup import signup
        import_pattern = r"from\s+([\w.]+)\s+import"
        matches = re.findall(import_pattern, test_content)

        modules = []
        for match in matches:
            # Skip standard library and pytest imports
            if match.startswith(("pytest", "unittest", "os", "sys", "typing")):
                continue
            # Convert module path to file path: src.auth.signup -> src/auth/signup.py
            module_path = match.replace(".", "/") + ".py"
            modules.append(module_path)

        return modules

    def _parse_file_outputs(self, llm_response: str) -> dict[str, str]:
        """
        Parse LLM response into file path -> content mapping.

        Supports multiple formats:
        - # FILE: path/to/file.py followed by content
        - #FILE: path/to/file.py (no space)
        - Code fences with path comments: # path/to/file.py

        Args:
            llm_response: Raw response from LLM.

        Returns:
            Dictionary mapping file paths to their contents.
        """
        files: dict[str, str] = {}

        if not llm_response.strip():
            return files

        # Pattern 1: # FILE: path/to/file.py followed by content until next FILE or end
        # Handles both "# FILE:" and "#FILE:" formats
        file_marker_pattern = r"#\s*FILE:\s*([^\n]+)\n([\s\S]*?)(?=#\s*FILE:|$)"
        matches = re.findall(file_marker_pattern, llm_response, re.IGNORECASE)

        if matches:
            for path, content in matches:
                path = path.strip()
                content = content.strip()
                if path and content:
                    files[path] = content
            return files

        # Pattern 2: Code fence with path comment at the start
        # ```python
        # # path/to/file.py
        # content
        # ```
        fence_pattern = r"```(?:python)?\s*\n#\s*([^\n]+\.py)\s*\n([\s\S]*?)```"
        fence_matches = re.findall(fence_pattern, llm_response)

        if fence_matches:
            for path, content in fence_matches:
                path = path.strip()
                content = content.strip()
                if path and content:
                    files[path] = content
            return files

        # Pattern 3: Plain code fence extraction (fallback)
        # Try to extract from standard code fences
        code_fence_pattern = r"```(?:python)?\s*([\s\S]*?)\s*```"
        code_matches = re.findall(code_fence_pattern, llm_response)

        if code_matches:
            # Look for path comment in the first line of each block
            for content in code_matches:
                lines = content.strip().split("\n")
                if lines and lines[0].strip().startswith("#"):
                    first_line = lines[0].strip()
                    # Check if first line looks like a path comment
                    path_match = re.match(r"#\s*(\S+\.py)\s*$", first_line)
                    if path_match:
                        path = path_match.group(1)
                        file_content = "\n".join(lines[1:]).strip()
                        if path and file_content:
                            files[path] = file_content

        return files

    def _build_prompt(
        self,
        feature_id: str,
        issue: dict[str, Any],
        spec_content: str,
        test_content: str,
        expected_modules: list[str],
    ) -> str:
        """Build the full prompt for Claude."""
        skill_prompt = self._load_skill_prompt()

        modules_str = "\n".join(f"- {m}" for m in expected_modules) if expected_modules else "- (Infer from test imports)"

        return f"""{skill_prompt}

---

## Context for This Task

**Feature ID:** {feature_id}

**Issue to Implement:**

Title: {issue.get('title', 'Unknown')}

{issue.get('body', 'No description')}

**Labels:** {', '.join(issue.get('labels', []))}
**Estimated Size:** {issue.get('estimated_size', 'medium')}

**Test File Content (your implementation must make these tests pass):**

```python
{test_content}
```

**Expected Module Paths (based on test imports):**
{modules_str}

**Engineering Spec Context:**

```markdown
{spec_content}
```

---

## Your Task

Implement production code that makes ALL tests in the test file pass.

Requirements:
1. Analyze the test file to understand expected behavior
2. Create the modules that tests import from
3. Implement all functions/classes that tests expect
4. Handle all edge cases that tests check for
5. Follow existing code patterns in the spec

Output Format:
- Use `# FILE: path/to/file.py` markers for each file
- Output ONLY implementation code, no explanations
- Create all necessary files to make tests pass
"""

    def run(self, context: dict[str, Any]) -> AgentResult:
        """
        Generate implementation code for a specific issue.

        Args:
            context: Dictionary containing:
                - feature_id: The feature identifier (required)
                - issue_number: The issue order number (required)
                - test_path: Optional path to test file (defaults to standard location)

        Returns:
            AgentResult with:
                - success: True if implementation was generated
                - output: Dict with files_created, files_modified, etc.
                - errors: List of any errors encountered
                - cost_usd: Cost of the LLM invocation
        """
        feature_id = context.get("feature_id")
        if not feature_id:
            return AgentResult.failure_result("Missing required context: feature_id")

        issue_number = context.get("issue_number")
        if issue_number is None:
            return AgentResult.failure_result("Missing required context: issue_number")

        self._log("coder_start", {
            "feature_id": feature_id,
            "issue_number": issue_number,
        })
        self.checkpoint("started")

        # Determine test file path
        test_path_str = context.get("test_path")
        if test_path_str:
            test_path = Path(test_path_str)
        else:
            test_path = self._get_default_test_path(feature_id, issue_number)

        # Check if test file exists
        if not file_exists(test_path):
            error = f"Test file not found at {test_path}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        # Read test file content
        try:
            test_content = read_file(test_path)
        except Exception as e:
            error = f"Failed to read test file: {e}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        # Extract expected modules from test imports
        expected_modules = self._extract_expected_modules(test_content)

        # Check if spec exists
        spec_path = self._get_spec_path(feature_id)
        if not file_exists(spec_path):
            error = f"Spec not found at {spec_path}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        # Read spec content
        try:
            spec_content = read_file(spec_path)
        except Exception as e:
            error = f"Failed to read spec: {e}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        # Load issues
        try:
            issues_data = self._load_issues(feature_id)
        except FileNotFoundError as e:
            error = str(e)
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)
        except json.JSONDecodeError as e:
            error = f"Failed to parse issues.json: {e}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        # Find the specific issue
        issue = self._find_issue(issues_data, issue_number)
        if not issue:
            error = f"Issue {issue_number} not found in issues.json"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        self.checkpoint("context_loaded")

        # Load skill prompt
        try:
            self._load_skill_prompt()
        except SkillNotFoundError as e:
            self._log("coder_error", {"error": str(e)}, level="error")
            return AgentResult.failure_result(str(e))

        # Build prompt and invoke Claude
        prompt = self._build_prompt(
            feature_id, issue, spec_content, test_content, expected_modules
        )

        try:
            result = self.llm.run(
                prompt,
                allowed_tools=["Read", "Glob", "Write", "Edit"],
            )
            cost = result.total_cost_usd
        except ClaudeTimeoutError as e:
            error = f"Claude timed out: {e}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)
        except ClaudeInvocationError as e:
            error = f"Claude invocation failed: {e}"
            self._log("coder_error", {"error": error}, level="error")
            return AgentResult.failure_result(error)

        self.checkpoint("llm_complete", cost_usd=cost)

        # Parse file outputs from response
        files = self._parse_file_outputs(result.text)

        # Write implementation files
        files_created: list[str] = []
        files_modified: list[str] = []

        for file_path, content in files.items():
            try:
                full_path = Path(self.config.repo_root) / file_path
                is_new = not file_exists(full_path)

                ensure_dir(full_path.parent)
                safe_write(full_path, content)

                if is_new:
                    files_created.append(file_path)
                else:
                    files_modified.append(file_path)
            except Exception as e:
                error = f"Failed to write file {file_path}: {e}"
                self._log("coder_error", {"error": error}, level="error")
                return AgentResult.failure_result(error, cost_usd=cost)

        self.checkpoint("files_written", cost_usd=0)

        # Generate summary
        implementation_summary = self._generate_summary(files_created, files_modified, issue)

        # Success
        self._log(
            "coder_complete",
            {
                "feature_id": feature_id,
                "issue_number": issue_number,
                "files_created": files_created,
                "files_modified": files_modified,
                "cost_usd": cost,
            },
        )

        return AgentResult.success_result(
            output={
                "feature_id": feature_id,
                "issue_number": issue_number,
                "files_created": files_created,
                "files_modified": files_modified,
                "implementation_summary": implementation_summary,
            },
            cost_usd=cost,
        )

    def _generate_summary(
        self,
        files_created: list[str],
        files_modified: list[str],
        issue: dict[str, Any],
    ) -> str:
        """Generate a summary of the implementation."""
        parts = []

        if files_created:
            parts.append(f"Created {len(files_created)} file(s): {', '.join(files_created)}")
        if files_modified:
            parts.append(f"Modified {len(files_modified)} file(s): {', '.join(files_modified)}")

        issue_title = issue.get("title", "Unknown issue")
        parts.append(f"Implementation for: {issue_title}")

        return ". ".join(parts)
