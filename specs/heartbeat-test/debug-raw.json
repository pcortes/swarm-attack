{
  "type": "result",
  "subtype": "success",
  "is_error": false,
  "duration_ms": 383889,
  "duration_api_ms": 389792,
  "num_turns": 1,
  "result": "<<<DISPOSITIONS_START>>>\n[\n  {\n    \"issue_id\": \"R2-1\",\n    \"original_issue\": \"Weekly and monthly rollups are deleted as soon as their week_start/month_start is older than 30 days, which wipes out the prior month's aggregates midway through the following month even though several days of that period are still within the allowed retention window. This undermines the PRD requirement to show monthly trends because users quickly lose access to the most recent full month view.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"The reviewer raises a valid point. The current retention logic deletes weekly/monthly rollups based on start date, which means a monthly rollup starting Nov 1 would be deleted on Dec 1 even though it contains data from Nov 30 (still within 30-day window). The PRD requires 'Show weekly/monthly activity trends' and the 30-day constraint should apply to the data covered, not the period start date. I will add a period_end column and adjust retention logic accordingly.\",\n    \"action_taken\": \"Added period_end columns to activity_weekly and activity_monthly tables. Updated retention function to delete rollups only when period_end < cutoff_date. Updated data models to include period_end fields. Added clarifying documentation.\",\n    \"resolved\": true,\n    \"semantic_key\": \"monthly_retention_period\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  },\n  {\n    \"issue_id\": \"R2-2\",\n    \"original_issue\": \"The Activity Collector only batches in memory (100 events or 30 seconds) and immediately POSTs to the API; there is no durable queue, retry policy, or background sync described. If the user goes offline or the request fails, those events are dropped permanently, which breaks the offline requirement and skews activity/peak-hour metrics.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"The reviewer correctly identifies a gap. The PRD states 'Must work offline after initial load' and while the spec addresses offline dashboard viewing, it doesn't address offline event collection. Events captured during offline periods would be lost, creating gaps in activity data. A durable client-side queue with background sync is needed for data integrity.\",\n    \"action_taken\": \"Added IndexedDB-backed event queue to Activity Collector SDK specification. Added Background Sync API integration for reliable event delivery. Updated SDK usage examples and configuration. Added retry policy with exponential backoff.\",\n    \"resolved\": true,\n    \"semantic_key\": \"collector_offline_queue\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  },\n  {\n    \"issue_id\": \"R2-3\",\n    \"original_issue\": \"All aggregation tables store UTC timestamps and the spec never explains how hourly/day buckets or peak-hour analysis will respect each user's timezone preference. Without a timezone-aware aggregation plan, peak hours and daily activity charts will be incorrect for non-UTC users, violating the 'highlight peak productivity hours' goal.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"The reviewer is correct. The spec stores all data in UTC and mentions user timezone preferences exist, but never explains how peak hours and daily boundaries are computed in the user's local time. A user in PST seeing '10 AM' as their peak hour based on UTC data would actually be seeing 2 AM local time. This must be addressed to meet the PRD goal of highlighting peak productivity hours.\",\n    \"action_taken\": \"Added comprehensive timezone handling strategy in new section 7.4. API endpoints now convert UTC data to user timezone at read time. Peak hours analysis explicitly computes local-time buckets. Added timezone conversion utilities specification.\",\n    \"resolved\": true,\n    \"semantic_key\": \"aggregation_timezone_handling\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  }\n]\n<<<DISPOSITIONS_END>>>\n\n<<<SPEC_START>>>\n# Engineering Spec: User Activity Dashboard\n\n## 1. Overview\n\n### 1.1 Purpose\n\nThe User Activity Dashboard provides users with visual insights into their activity patterns over time. It enables users to understand when they are most productive, track weekly and monthly trends, and export their data for personal analysis. The dashboard operates offline after initial load, respecting the 30-day data retention limit.\n\n### 1.2 Scope\n\n**In Scope:**\n- Daily, weekly, and monthly activity visualization with interactive charts\n- Peak productivity hours analysis and display\n- CSV data export functionality\n- Offline-first architecture with service worker caching\n- Durable offline event collection with background sync\n- Mobile-responsive design\n- 30-day rolling data window with automatic cleanup across all storage layers\n- Real-time activity data aggregation\n- Local storage for offline access with 30-day retention enforcement\n- Timezone-aware data presentation\n\n**Out of Scope:**\n- Historical data beyond 30 days\n- Cross-device activity synchronization\n- Machine learning-based productivity predictions\n- Team/organization-level analytics\n- Integration with external calendar services\n- Activity data from third-party applications\n\n### 1.3 Success Criteria\n\n| Criterion | Metric | Target |\n|-----------|--------|--------|\n| Dashboard load time | Initial render (P95) | < 2 seconds |\n| User engagement | Weekly active users viewing dashboard | > 30% |\n| Export success | CSV export completion rate | > 99% |\n| Offline reliability | Dashboard loads without network | 100% after initial visit |\n| Event collection reliability | Events captured offline delivered | > 99.5% |\n| Data freshness | Activity update delay | < 5 minutes |\n| Mobile usability | Lighthouse mobile score | > 90 |\n\n---\n\n## 2. Architecture\n\n### 2.1 High-Level Design\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         User Activity Dashboard                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502   User's Browser    \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                           \u2502                           \u2502\n        \u25bc                           \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Service Worker   \u2502   \u2502   React App       \u2502   \u2502   IndexedDB       \u2502\n\u2502  (Offline Cache   \u2502   \u2502   (Dashboard UI)  \u2502   \u2502   (Local Storage  \u2502\n\u2502   + Background    \u2502   \u2502                   \u2502   \u2502    + Event Queue) \u2502\n\u2502   Sync)           \u2502   \u2502                   \u2502   \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    REST API Layer       \u2502\n                    \u2502   /api/v1/activity/*    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                       \u2502                           \u2502\n        \u25bc                       \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Activity         \u2502   \u2502  Aggregation      \u2502   \u2502  Export           \u2502\n\u2502  Collector        \u2502   \u2502  Service          \u2502   \u2502  Service          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     PostgreSQL          \u2502\n                    \u2502  (Activity Events)      \u2502\n                    \u2502  (30-day retention)     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 2.2 Components\n\n| Component | Responsibility | Technology |\n|-----------|---------------|------------|\n| Dashboard UI | Interactive charts and visualizations | React + D3.js/Chart.js |\n| Service Worker | Cache dashboard assets for offline use, background sync for events | Workbox |\n| IndexedDB Store | Store activity data locally for offline access (30-day max), durable event queue | idb library |\n| Activity Collector | Capture user activity events with offline support | JavaScript SDK |\n| Aggregation Service | Pre-compute hourly/daily/weekly/monthly rollups | Python background job |\n| Export Service | Generate CSV files from activity data | Python + pandas |\n| Activity Store | Persist raw and aggregated activity data | PostgreSQL with partitioning |\n| Retention Worker | Purge data older than 30 days from all tables | PostgreSQL scheduled job |\n\n### 2.3 Data Flow\n\n1. **Activity Collection**\n   - User actions trigger activity events in the application\n   - Activity Collector queues events in IndexedDB (durable storage)\n   - When online, batches sent to `/api/v1/activity/events` endpoint\n   - When offline, events remain queued until connectivity restored\n   - Background Sync API triggers delivery when online\n   - Events stored in `activity_events` table with user_id, timestamp, event_type\n\n2. **Aggregation Pipeline**\n   - Hourly job runs every 5 minutes: aggregates raw events into hourly buckets\n   - Daily job runs at 00:30 UTC: aggregates hourly data into daily rollups\n   - Weekly job runs at 01:00 UTC: aggregates daily data into weekly rollups\n   - Monthly job runs at 01:30 UTC: aggregates daily data into monthly rollups\n   - All aggregations stored in UTC; converted to user timezone at read time\n\n3. **Dashboard Loading**\n   - Client requests aggregated data from `/api/v1/activity/summary`\n   - Server converts UTC data to user's timezone preference\n   - Server returns pre-computed aggregations (fast query via composite indexes)\n   - Client caches response in IndexedDB with timestamp\n   - Service Worker caches static assets\n   - Subsequent loads use cached data if offline\n\n4. **Data Export**\n   - User requests export via `/api/v1/activity/export`\n   - Server streams CSV data for the requested date range\n   - Maximum export range: 30 days (entire retention window)\n   - Export files deleted after 24 hours\n\n5. **Retention Enforcement (All Storage Layers)**\n   - Daily scheduled job at 02:00 UTC\n   - Deletes partitions older than 30 days from activity_events\n   - Deletes rows older than 30 days from activity_hourly, activity_daily\n   - Deletes weekly rollups where period_end < cutoff_date\n   - Deletes monthly rollups where period_end < cutoff_date\n   - Deletes expired export files from storage\n   - Updates `activity_retention_log` for audit trail\n   - Client-side: IndexedDB cache entries expire after 30 days\n\n---\n\n## 3. Data Models\n\n### 3.1 New Models\n\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, date, time\nfrom enum import Enum\nfrom typing import Optional\nfrom decimal import Decimal\nimport uuid\n\n\nclass ActivityEventType(Enum):\n    \"\"\"Types of user activity events.\"\"\"\n    PAGE_VIEW = \"page_view\"\n    FEATURE_USE = \"feature_use\"\n    SESSION_START = \"session_start\"\n    SESSION_END = \"session_end\"\n    ACTION = \"action\"\n    NAVIGATION = \"navigation\"\n\n\nclass TimeGranularity(Enum):\n    \"\"\"Time granularity for activity aggregations.\"\"\"\n    HOURLY = \"hourly\"\n    DAILY = \"daily\"\n    WEEKLY = \"weekly\"\n    MONTHLY = \"monthly\"\n\n\nclass ExportStatus(Enum):\n    \"\"\"Status of an export job.\"\"\"\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    EXPIRED = \"expired\"\n\n\n@dataclass\nclass ActivityEvent:\n    \"\"\"Raw activity event from user action.\"\"\"\n    event_id: str                           # UUID\n    user_id: str                            # User who performed action\n    event_type: ActivityEventType           # Type of activity\n    event_name: str                         # Specific event identifier\n    timestamp: datetime                     # When event occurred (UTC)\n    session_id: Optional[str] = None        # Browser session identifier\n    page_path: Optional[str] = None         # URL path if applicable\n    metadata: dict = field(default_factory=dict)  # Additional context\n    duration_ms: Optional[int] = None       # Duration for timed events\n    client_timestamp: Optional[datetime] = None  # Original client time for offline events\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"event_id\": self.event_id,\n            \"user_id\": self.user_id,\n            \"event_type\": self.event_type.value,\n            \"event_name\": self.event_name,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"session_id\": self.session_id,\n            \"page_path\": self.page_path,\n            \"metadata\": self.metadata,\n            \"duration_ms\": self.duration_ms,\n            \"client_timestamp\": self.client_timestamp.isoformat() if self.client_timestamp else None,\n        }\n\n\n@dataclass\nclass HourlyActivityRollup:\n    \"\"\"Hourly aggregation of user activity.\"\"\"\n    user_id: str\n    hour_start: datetime                    # Start of the hour (truncated, UTC)\n    event_count: int                        # Total events in this hour\n    session_count: int                      # Distinct sessions\n    active_minutes: int                     # Minutes with at least one event\n    page_views: int                         # Count of page_view events\n    feature_uses: int                       # Count of feature_use events\n    top_pages: list[str] = field(default_factory=list)  # Top 5 pages by views\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    updated_at: datetime = field(default_factory=datetime.utcnow)\n\n    @property\n    def productivity_score(self) -> float:\n        \"\"\"Calculate productivity score (0-100) for this hour.\"\"\"\n        if self.active_minutes == 0:\n            return 0.0\n        # Score based on activity density and feature engagement\n        density = min(self.active_minutes / 60.0, 1.0) * 50\n        engagement = min(self.feature_uses / 10.0, 1.0) * 50\n        return round(density + engagement, 2)\n\n\n@dataclass\nclass DailyActivityRollup:\n    \"\"\"Daily aggregation of user activity.\"\"\"\n    user_id: str\n    date: date                              # Calendar date (UTC)\n    total_events: int                       # Total events for the day\n    total_sessions: int                     # Total distinct sessions\n    active_hours: int                       # Hours with at least one event\n    active_minutes: int                     # Total active minutes\n    peak_hour: int                          # Hour (0-23) with most activity (UTC)\n    peak_hour_events: int                   # Event count in peak hour\n    page_views: int\n    feature_uses: int\n    first_activity: Optional[time] = None   # Time of first event (UTC)\n    last_activity: Optional[time] = None    # Time of last event (UTC)\n    hourly_breakdown: list[int] = field(default_factory=lambda: [0] * 24)  # Events per hour (UTC)\n\n    @property\n    def productivity_score(self) -> float:\n        \"\"\"Calculate daily productivity score (0-100).\"\"\"\n        if self.active_hours == 0:\n            return 0.0\n        # Score based on active hours and engagement\n        hours_score = min(self.active_hours / 8.0, 1.0) * 40\n        engagement_score = min(self.feature_uses / 50.0, 1.0) * 40\n        consistency_score = self._consistency_bonus() * 20\n        return round(hours_score + engagement_score + consistency_score, 2)\n\n    def _consistency_bonus(self) -> float:\n        \"\"\"Bonus for consistent activity throughout the day.\"\"\"\n        if sum(self.hourly_breakdown) == 0:\n            return 0.0\n        non_zero_hours = sum(1 for h in self.hourly_breakdown if h > 0)\n        return min(non_zero_hours / 8.0, 1.0)\n\n\n@dataclass\nclass WeeklyActivityRollup:\n    \"\"\"Weekly aggregation of user activity.\"\"\"\n    user_id: str\n    week_start: date                        # Monday of the week\n    week_end: date                          # Sunday of the week (inclusive)\n    total_events: int\n    total_sessions: int\n    active_days: int                        # Days with at least one event\n    total_active_hours: int\n    total_active_minutes: int\n    peak_day: int                           # Day of week (0=Mon) with most activity\n    peak_day_events: int\n    daily_breakdown: list[int] = field(default_factory=lambda: [0] * 7)  # Events per day\n    hourly_heatmap: list[list[int]] = field(\n        default_factory=lambda: [[0] * 24 for _ in range(7)]\n    )  # 7 days x 24 hours (UTC)\n\n    @property\n    def average_daily_score(self) -> float:\n        \"\"\"Average productivity score across active days.\"\"\"\n        if self.active_days == 0:\n            return 0.0\n        return round(sum(self.daily_breakdown) / self.active_days / 100, 2)\n\n\n@dataclass\nclass MonthlyActivityRollup:\n    \"\"\"Monthly aggregation of user activity.\"\"\"\n    user_id: str\n    month_start: date                       # First day of the month\n    month_end: date                         # Last day of the month (inclusive)\n    total_events: int                       # Total events for the month\n    total_sessions: int                     # Total distinct sessions\n    active_days: int                        # Days with at least one event\n    total_active_hours: int                 # Total hours with activity\n    total_active_minutes: int               # Total active minutes\n    peak_week: int                          # Week of month (1-5) with most activity\n    peak_week_events: int                   # Event count in peak week\n    peak_day_of_week: int                   # Day of week (0=Mon) with most activity overall\n    page_views: int\n    feature_uses: int\n    weekly_breakdown: list[int] = field(default_factory=lambda: [0] * 5)  # Events per week (max 5 weeks)\n    daily_breakdown: list[int] = field(default_factory=lambda: [0] * 31)  # Events per day of month\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    updated_at: datetime = field(default_factory=datetime.utcnow)\n\n    @property\n    def productivity_score(self) -> float:\n        \"\"\"Calculate monthly productivity score (0-100).\"\"\"\n        if self.active_days == 0:\n            return 0.0\n        # Score based on active days, engagement, and consistency\n        days_score = min(self.active_days / 20.0, 1.0) * 35  # ~20 working days ideal\n        engagement_score = min(self.feature_uses / 200.0, 1.0) * 35\n        consistency_score = self._consistency_bonus() * 30\n        return round(days_score + engagement_score + consistency_score, 2)\n\n    def _consistency_bonus(self) -> float:\n        \"\"\"Bonus for consistent activity throughout the month.\"\"\"\n        days_in_month = sum(1 for d in self.daily_breakdown if d >= 0)  # Actual days in month\n        if days_in_month == 0:\n            return 0.0\n        active_ratio = self.active_days / min(days_in_month, 22)  # Compare to ~22 working days\n        return min(active_ratio, 1.0)\n\n    @property\n    def average_daily_events(self) -> float:\n        \"\"\"Average events per active day.\"\"\"\n        if self.active_days == 0:\n            return 0.0\n        return round(self.total_events / self.active_days, 1)\n\n\n@dataclass\nclass PeakHoursAnalysis:\n    \"\"\"Analysis of user's peak productivity hours.\"\"\"\n    user_id: str\n    analysis_period_days: int               # Number of days analyzed\n    timezone: str                           # User's timezone for this analysis\n    peak_hours: list[int]                   # Top 3 most productive hours (0-23, local time)\n    peak_hours_scores: list[float]          # Productivity scores for peak hours\n    peak_days: list[int]                    # Top 3 most productive days (0=Mon)\n    average_start_time: time                # Average first activity time (local)\n    average_end_time: time                  # Average last activity time (local)\n    most_active_period: str                 # \"morning\", \"afternoon\", \"evening\", \"night\"\n    consistency_score: float                # How consistent the pattern is (0-100)\n    generated_at: datetime = field(default_factory=datetime.utcnow)\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for API response.\"\"\"\n        return {\n            \"user_id\": self.user_id,\n            \"analysis_period_days\": self.analysis_period_days,\n            \"timezone\": self.timezone,\n            \"peak_hours\": self.peak_hours,\n            \"peak_hours_scores\": self.peak_hours_scores,\n            \"peak_days\": self.peak_days,\n            \"average_start_time\": self.average_start_time.isoformat() if self.average_start_time else None,\n            \"average_end_time\": self.average_end_time.isoformat() if self.average_end_time else None,\n            \"most_active_period\": self.most_active_period,\n            \"consistency_score\": self.consistency_score,\n            \"generated_at\": self.generated_at.isoformat(),\n        }\n\n\n@dataclass\nclass ExportJob:\n    \"\"\"Represents a data export request.\"\"\"\n    export_id: str                          # UUID\n    user_id: str\n    status: ExportStatus\n    start_date: date                        # Export range start\n    end_date: date                          # Export range end\n    file_format: str = \"csv\"                # Only CSV supported initially\n    include_raw_events: bool = False        # Include raw events or just aggregates\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    completed_at: Optional[datetime] = None\n    file_path: Optional[str] = None         # Path to generated file\n    file_size_bytes: Optional[int] = None\n    row_count: Optional[int] = None\n    error_message: Optional[str] = None\n    expires_at: Optional[datetime] = None   # When download link expires (24 hours)\n\n\n@dataclass\nclass UserDashboardPreferences:\n    \"\"\"User preferences for dashboard display.\"\"\"\n    user_id: str\n    default_view: TimeGranularity = TimeGranularity.WEEKLY\n    timezone: str = \"UTC\"\n    week_starts_on: int = 0                 # 0=Monday, 6=Sunday\n    chart_type: str = \"bar\"                 # \"bar\", \"line\", \"area\"\n    show_productivity_score: bool = True\n    show_peak_hours: bool = True\n    show_comparison: bool = True            # Compare to previous period\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    updated_at: datetime = field(default_factory=datetime.utcnow)\n```\n\n### 3.2 Schema Changes\n\n**PostgreSQL Migrations:**\n\n```sql\n-- Migration: 001_create_activity_events\n-- Using table partitioning for efficient retention management\nCREATE TABLE activity_events (\n    event_id UUID NOT NULL,\n    user_id VARCHAR(64) NOT NULL,\n    event_type VARCHAR(50) NOT NULL,\n    event_name VARCHAR(255) NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    session_id VARCHAR(64),\n    page_path VARCHAR(500),\n    metadata JSONB DEFAULT '{}',\n    duration_ms INT,\n    client_timestamp TIMESTAMPTZ,  -- Original client time for offline events\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    PRIMARY KEY (event_id, timestamp)\n) PARTITION BY RANGE (timestamp);\n\n-- Create partitions for current and next 30 days\n-- Additional partitions created by scheduled job\nCREATE TABLE activity_events_default PARTITION OF activity_events DEFAULT;\n\n-- Indexes for common queries (user_id first for efficient filtering)\nCREATE INDEX idx_activity_events_user_ts ON activity_events(user_id, timestamp DESC);\nCREATE INDEX idx_activity_events_type ON activity_events(event_type, timestamp DESC);\nCREATE INDEX idx_activity_events_session ON activity_events(session_id, timestamp DESC);\n\n\n-- Migration: 002_create_activity_hourly\nCREATE TABLE activity_hourly (\n    user_id VARCHAR(64) NOT NULL,\n    hour_start TIMESTAMPTZ NOT NULL,\n    event_count INT NOT NULL DEFAULT 0,\n    session_count INT NOT NULL DEFAULT 0,\n    active_minutes INT NOT NULL DEFAULT 0,\n    page_views INT NOT NULL DEFAULT 0,\n    feature_uses INT NOT NULL DEFAULT 0,\n    top_pages JSONB DEFAULT '[]',\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    PRIMARY KEY (user_id, hour_start)\n);\n\n-- Primary access pattern: user's hourly data for a time range\nCREATE INDEX idx_activity_hourly_user_ts ON activity_hourly(user_id, hour_start DESC);\n-- Secondary: time-based queries for retention cleanup\nCREATE INDEX idx_activity_hourly_ts ON activity_hourly(hour_start DESC);\n\n\n-- Migration: 003_create_activity_daily\nCREATE TABLE activity_daily (\n    user_id VARCHAR(64) NOT NULL,\n    date DATE NOT NULL,\n    total_events INT NOT NULL DEFAULT 0,\n    total_sessions INT NOT NULL DEFAULT 0,\n    active_hours INT NOT NULL DEFAULT 0,\n    active_minutes INT NOT NULL DEFAULT 0,\n    peak_hour SMALLINT,\n    peak_hour_events INT DEFAULT 0,\n    page_views INT NOT NULL DEFAULT 0,\n    feature_uses INT NOT NULL DEFAULT 0,\n    first_activity TIME,\n    last_activity TIME,\n    hourly_breakdown INT[] DEFAULT ARRAY_FILL(0, ARRAY[24]),\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    PRIMARY KEY (user_id, date)\n);\n\n-- Primary access pattern: user's daily data for a date range\nCREATE INDEX idx_activity_daily_user_date ON activity_daily(user_id, date DESC);\n-- Secondary: date-based queries for retention cleanup\nCREATE INDEX idx_activity_daily_date ON activity_daily(date DESC);\n\n\n-- Migration: 004_create_activity_weekly\nCREATE TABLE activity_weekly (\n    user_id VARCHAR(64) NOT NULL,\n    week_start DATE NOT NULL,  -- Always a Monday\n    week_end DATE NOT NULL,    -- Always a Sunday (inclusive)\n    total_events INT NOT NULL DEFAULT 0,\n    total_sessions INT NOT NULL DEFAULT 0,\n    active_days INT NOT NULL DEFAULT 0,\n    total_active_hours INT NOT NULL DEFAULT 0,\n    total_active_minutes INT NOT NULL DEFAULT 0,\n    peak_day SMALLINT,\n    peak_day_events INT DEFAULT 0,\n    daily_breakdown INT[] DEFAULT ARRAY_FILL(0, ARRAY[7]),\n    hourly_heatmap INT[][] DEFAULT ARRAY_FILL(0, ARRAY[7, 24]),\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    PRIMARY KEY (user_id, week_start),\n    CONSTRAINT valid_week_start CHECK (EXTRACT(DOW FROM week_start) = 1),  -- Must be Monday\n    CONSTRAINT valid_week_end CHECK (week_end = week_start + INTERVAL '6 days')\n);\n\n-- Primary access pattern: user's weekly data for a date range\nCREATE INDEX idx_activity_weekly_user_date ON activity_weekly(user_id, week_start DESC);\n-- Secondary: date-based queries for retention cleanup (uses week_end)\nCREATE INDEX idx_activity_weekly_end ON activity_weekly(week_end DESC);\n\n\n-- Migration: 004b_create_activity_monthly\nCREATE TABLE activity_monthly (\n    user_id VARCHAR(64) NOT NULL,\n    month_start DATE NOT NULL,  -- Always first day of month\n    month_end DATE NOT NULL,    -- Always last day of month (inclusive)\n    total_events INT NOT NULL DEFAULT 0,\n    total_sessions INT NOT NULL DEFAULT 0,\n    active_days INT NOT NULL DEFAULT 0,\n    total_active_hours INT NOT NULL DEFAULT 0,\n    total_active_minutes INT NOT NULL DEFAULT 0,\n    peak_week SMALLINT,                     -- Week of month (1-5)\n    peak_week_events INT DEFAULT 0,\n    peak_day_of_week SMALLINT,              -- Day of week (0=Mon) with most activity\n    page_views INT NOT NULL DEFAULT 0,\n    feature_uses INT NOT NULL DEFAULT 0,\n    weekly_breakdown INT[] DEFAULT ARRAY_FILL(0, ARRAY[5]),  -- Up to 5 weeks per month\n    daily_breakdown INT[] DEFAULT ARRAY_FILL(0, ARRAY[31]),  -- Up to 31 days\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    PRIMARY KEY (user_id, month_start),\n    CONSTRAINT valid_month_start CHECK (EXTRACT(DAY FROM month_start) = 1),  -- Must be first of month\n    CONSTRAINT valid_month_end CHECK (\n        month_end = (month_start + INTERVAL '1 month' - INTERVAL '1 day')::DATE\n    )\n);\n\n-- Primary access pattern: user's monthly data\nCREATE INDEX idx_activity_monthly_user_date ON activity_monthly(user_id, month_start DESC);\n-- Secondary: date-based queries for retention cleanup (uses month_end)\nCREATE INDEX idx_activity_monthly_end ON activity_monthly(month_end DESC);\n\n\n-- Migration: 005_create_export_jobs\nCREATE TABLE export_jobs (\n    export_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id VARCHAR(64) NOT NULL,\n    status VARCHAR(20) NOT NULL DEFAULT 'pending',\n    start_date DATE NOT NULL,\n    end_date DATE NOT NULL,\n    file_format VARCHAR(10) NOT NULL DEFAULT 'csv',\n    include_raw_events BOOLEAN NOT NULL DEFAULT false,\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    completed_at TIMESTAMPTZ,\n    file_path VARCHAR(500),\n    file_size_bytes BIGINT,\n    row_count INT,\n    error_message TEXT,\n    expires_at TIMESTAMPTZ,  -- 24 hours after completion\n\n    CONSTRAINT valid_status CHECK (status IN ('pending', 'processing', 'completed', 'failed', 'expired')),\n    CONSTRAINT valid_date_range CHECK (end_date >= start_date),\n    CONSTRAINT max_date_range CHECK (end_date - start_date <= 30)\n);\n\nCREATE INDEX idx_export_jobs_user ON export_jobs(user_id, created_at DESC);\nCREATE INDEX idx_export_jobs_status ON export_jobs(status) WHERE status IN ('pending', 'processing');\nCREATE INDEX idx_export_jobs_expires ON export_jobs(expires_at) WHERE status = 'completed';\n\n\n-- Migration: 006_create_user_dashboard_preferences\nCREATE TABLE user_dashboard_preferences (\n    user_id VARCHAR(64) PRIMARY KEY,\n    default_view VARCHAR(20) NOT NULL DEFAULT 'weekly',\n    timezone VARCHAR(50) NOT NULL DEFAULT 'UTC',\n    week_starts_on SMALLINT NOT NULL DEFAULT 0,\n    chart_type VARCHAR(20) NOT NULL DEFAULT 'bar',\n    show_productivity_score BOOLEAN NOT NULL DEFAULT true,\n    show_peak_hours BOOLEAN NOT NULL DEFAULT true,\n    show_comparison BOOLEAN NOT NULL DEFAULT true,\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n\n    CONSTRAINT valid_default_view CHECK (default_view IN ('hourly', 'daily', 'weekly', 'monthly')),\n    CONSTRAINT valid_week_starts CHECK (week_starts_on >= 0 AND week_starts_on <= 6),\n    CONSTRAINT valid_chart_type CHECK (chart_type IN ('bar', 'line', 'area'))\n);\n\n\n-- Migration: 007_create_retention_log\nCREATE TABLE activity_retention_log (\n    id SERIAL PRIMARY KEY,\n    execution_date DATE NOT NULL,\n    table_name VARCHAR(100) NOT NULL,\n    partition_dropped VARCHAR(100),\n    rows_deleted BIGINT,\n    execution_time_ms INT,\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\n\n-- Migration: 008_create_partition_and_retention_function\nCREATE OR REPLACE FUNCTION manage_activity_data_retention()\nRETURNS void AS $$\nDECLARE\n    partition_date DATE;\n    partition_name TEXT;\n    old_partition_name TEXT;\n    cutoff_date DATE;\n    cutoff_timestamp TIMESTAMPTZ;\n    deleted_count BIGINT;\n    start_time TIMESTAMPTZ;\nBEGIN\n    cutoff_date := CURRENT_DATE - INTERVAL '30 days';\n    cutoff_timestamp := cutoff_date::TIMESTAMPTZ;\n    \n    -- 1. Create partition for tomorrow (activity_events)\n    partition_date := CURRENT_DATE + INTERVAL '1 day';\n    partition_name := 'activity_events_' || TO_CHAR(partition_date, 'YYYYMMDD');\n\n    IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relname = partition_name) THEN\n        EXECUTE FORMAT(\n            'CREATE TABLE %I PARTITION OF activity_events FOR VALUES FROM (%L) TO (%L)',\n            partition_name,\n            partition_date,\n            partition_date + INTERVAL '1 day'\n        );\n    END IF;\n\n    -- 2. Drop old partitions from activity_events (>30 days)\n    FOR old_partition_name IN\n        SELECT tablename FROM pg_tables\n        WHERE tablename LIKE 'activity_events_%'\n        AND tablename ~ '^activity_events_[0-9]{8}$'\n        AND TO_DATE(SUBSTRING(tablename FROM '[0-9]{8}$'), 'YYYYMMDD') < cutoff_date\n    LOOP\n        start_time := clock_timestamp();\n        EXECUTE FORMAT('DROP TABLE IF EXISTS %I', old_partition_name);\n        \n        INSERT INTO activity_retention_log (execution_date, table_name, partition_dropped, execution_time_ms)\n        VALUES (CURRENT_DATE, 'activity_events', old_partition_name, \n                EXTRACT(MILLISECONDS FROM clock_timestamp() - start_time)::INT);\n    END LOOP;\n\n    -- 3. Delete old rows from activity_hourly (>30 days)\n    start_time := clock_timestamp();\n    DELETE FROM activity_hourly WHERE hour_start < cutoff_timestamp;\n    GET DIAGNOSTICS deleted_count = ROW_COUNT;\n    IF deleted_count > 0 THEN\n        INSERT INTO activity_retention_log (execution_date, table_name, rows_deleted, execution_time_ms)\n        VALUES (CURRENT_DATE, 'activity_hourly', deleted_count,\n                EXTRACT(MILLISECONDS FROM clock_timestamp() - start_time)::INT);\n    END IF;\n\n    -- 4. Delete old rows from activity_daily (>30 days)\n    start_time := clock_timestamp();\n    DELETE FROM activity_daily WHERE date < cutoff_date;\n    GET DIAGNOSTICS deleted_count = ROW_COUNT;\n    IF deleted_count > 0 THEN\n        INSERT INTO activity_retention_log (execution_date, table_name, rows_deleted, execution_time_ms)\n        VALUES (CURRENT_DATE, 'activity_daily', deleted_count,\n                EXTRACT(MILLISECONDS FROM clock_timestamp() - start_time)::INT);\n    END IF;\n\n    -- 5. Delete old rows from activity_weekly (only when week_end < cutoff)\n    -- This ensures we keep weekly rollups until ALL days in the week are >30 days old\n    start_time := clock_timestamp();\n    DELETE FROM activity_weekly WHERE week_end < cutoff_date;\n    GET DIAGNOSTICS deleted_count = ROW_COUNT;\n    IF deleted_count > 0 THEN\n        INSERT INTO activity_retention_log (execution_date, table_name, rows_deleted, execution_time_ms)\n        VALUES (CURRENT_DATE, 'activity_weekly', deleted_count,\n                EXTRACT(MILLISECONDS FROM clock_timestamp() - start_time)::INT);\n    END IF;\n\n    -- 6. Delete old rows from activity_monthly (only when month_end < cutoff)\n    -- This ensures we keep monthly rollups until ALL days in the month are >30 days old\n    start_time := clock_timestamp();\n    DELETE FROM activity_monthly WHERE month_end < cutoff_date;\n    GET DIAGNOSTICS deleted_count = ROW_COUNT;\n    IF deleted_count > 0 THEN\n        INSERT INTO activity_retention_log (execution_date, table_name, rows_deleted, execution_time_ms)\n        VALUES (CURRENT_DATE, 'activity_monthly', deleted_count,\n                EXTRACT(MILLISECONDS FROM clock_timestamp() - start_time)::INT);\n    END IF;\n\n    -- 7. Mark expired exports and delete old export files\n    UPDATE export_jobs \n    SET status = 'expired' \n    WHERE status = 'completed' AND expires_at < NOW();\n    \n    -- Note: Physical file deletion handled by separate cleanup job\n    -- that reads expired export_jobs and deletes from storage\n    \nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule daily at 02:00 UTC (requires pg_cron extension)\n-- SELECT cron.schedule('manage-activity-retention', '0 2 * * *', 'SELECT manage_activity_data_retention()');\n\n\n-- Migration: 009_create_export_file_cleanup_function\nCREATE OR REPLACE FUNCTION cleanup_expired_export_files()\nRETURNS TABLE(file_path VARCHAR, deleted BOOLEAN) AS $$\nBEGIN\n    -- Returns list of expired export file paths for application to delete from storage\n    -- Application should call this, delete files, then update records\n    RETURN QUERY\n    SELECT ej.file_path, false AS deleted\n    FROM export_jobs ej\n    WHERE ej.status = 'expired' \n    AND ej.file_path IS NOT NULL;\nEND;\n$$ LANGUAGE plpgsql;\n```\n\n---\n\n## 4. API Design\n\n### 4.1 Endpoints\n\n| Method | Path | Description |\n|--------|------|-------------|\n| POST | /api/v1/activity/events | Batch submit activity events |\n| GET | /api/v1/activity/summary | Get activity summary for dashboard |\n| GET | /api/v1/activity/daily | Get daily activity breakdown |\n| GET | /api/v1/activity/weekly | Get weekly activity with heatmap |\n| GET | /api/v1/activity/monthly | Get monthly activity overview |\n| GET | /api/v1/activity/peak-hours | Get peak productivity hours analysis |\n| POST | /api/v1/activity/export | Request data export |\n| GET | /api/v1/activity/export/{export_id} | Get export job status |\n| GET | /api/v1/activity/export/{export_id}/download | Download exported file |\n| GET | /api/v1/activity/preferences | Get dashboard preferences |\n| PUT | /api/v1/activity/preferences | Update dashboard preferences |\n\n### 4.2 Request/Response Schemas\n\n**POST /api/v1/activity/events**\n\nRequest:\n```json\n{\n  \"events\": [\n    {\n      \"event_type\": \"page_view\",\n      \"event_name\": \"dashboard_viewed\",\n      \"timestamp\": \"2025-12-15T10:30:00Z\",\n      \"client_timestamp\": \"2025-12-15T10:30:00Z\",\n      \"session_id\": \"sess_abc123\",\n      \"page_path\": \"/dashboard\",\n      \"metadata\": {\n        \"referrer\": \"/home\"\n      }\n    },\n    {\n      \"event_type\": \"feature_use\",\n      \"event_name\": \"report_generated\",\n      \"timestamp\": \"2025-12-15T10:32:00Z\",\n      \"client_timestamp\": \"2025-12-15T10:32:00Z\",\n      \"session_id\": \"sess_abc123\",\n      \"duration_ms\": 2500,\n      \"metadata\": {\n        \"report_type\": \"weekly\"\n      }\n    }\n  ],\n  \"batch_id\": \"batch_xyz123\",\n  \"queued_at\": \"2025-12-15T10:30:00Z\"\n}\n```\n\nResponse:\n```json\n{\n  \"accepted\": 2,\n  \"rejected\": 0,\n  \"batch_id\": \"batch_xyz123\",\n  \"errors\": []\n}\n```\n\n**GET /api/v1/activity/summary**\n\nQuery Parameters:\n- `period` (string, optional): \"day\", \"week\", \"month\" (default: \"week\")\n- `timezone` (string, optional): IANA timezone (default: user preference or \"UTC\")\n\nResponse:\n```json\n{\n  \"user_id\": \"usr_123456\",\n  \"period\": \"week\",\n  \"period_start\": \"2025-12-09\",\n  \"period_end\": \"2025-12-15\",\n  \"timezone\": \"America/New_York\",\n  \"summary\": {\n    \"total_events\": 847,\n    \"total_sessions\": 23,\n    \"active_days\": 5,\n    \"active_hours\": 42,\n    \"active_minutes\": 1680,\n    \"productivity_score\": 72.5\n  },\n  \"comparison\": {\n    \"previous_period_events\": 792,\n    \"events_change_percent\": 6.9,\n    \"previous_period_score\": 68.2,\n    \"score_change_percent\": 6.3\n  },\n  \"peak_hours\": {\n    \"top_hours\": [10, 14, 11],\n    \"top_hours_local\": [\"10:00 AM\", \"2:00 PM\", \"11:00 AM\"],\n    \"most_active_period\": \"morning\"\n  },\n  \"charts\": {\n    \"daily_breakdown\": [\n      {\"date\": \"2025-12-09\", \"events\": 156, \"score\": 71.2},\n      {\"date\": \"2025-12-10\", \"events\": 189, \"score\": 78.5},\n      {\"date\": \"2025-12-11\", \"events\": 167, \"score\": 73.1},\n      {\"date\": \"2025-12-12\", \"events\": 0, \"score\": 0},\n      {\"date\": \"2025-12-13\", \"events\": 0, \"score\": 0},\n      {\"date\": \"2025-12-14\", \"events\": 145, \"score\": 65.8},\n      {\"date\": \"2025-12-15\", \"events\": 190, \"score\": 75.4}\n    ]\n  },\n  \"generated_at\": \"2025-12-15T11:00:00Z\",\n  \"cache_ttl_seconds\": 300\n}\n```\n\n**GET /api/v1/activity/weekly**\n\nQuery Parameters:\n- `week_start` (date, optional): Start of week (default: current week)\n- `timezone` (string, optional): IANA timezone\n\nResponse:\n```json\n{\n  \"user_id\": \"usr_123456\",\n  \"week_start\": \"2025-12-09\",\n  \"week_end\": \"2025-12-15\",\n  \"timezone\": \"America/New_York\",\n  \"totals\": {\n    \"events\": 847,\n    \"sessions\": 23,\n    \"active_days\": 5,\n    \"active_hours\": 42,\n    \"active_minutes\": 1680\n  },\n  \"peak_day\": {\n    \"day_of_week\": 1,\n    \"day_name\": \"Tuesday\",\n    \"events\": 189\n  },\n  \"daily_breakdown\": [\n    {\"day\": 0, \"name\": \"Monday\", \"events\": 156, \"hours\": 8},\n    {\"day\": 1, \"name\": \"Tuesday\", \"events\": 189, \"hours\": 9},\n    {\"day\": 2, \"name\": \"Wednesday\", \"events\": 167, \"hours\": 8},\n    {\"day\": 3, \"name\": \"Thursday\", \"events\": 0, \"hours\": 0},\n    {\"day\": 4, \"name\": \"Friday\", \"events\": 0, \"hours\": 0},\n    {\"day\": 5, \"name\": \"Saturday\", \"events\": 145, \"hours\": 7},\n    {\"day\": 6, \"name\": \"Sunday\", \"events\": 190, \"hours\": 10}\n  ],\n  \"hourly_heatmap\": [\n    [0, 0, 0, 0, 0, 0, 8, 12, 18, 25, 32, 28, 15, 22, 30, 28, 18, 12, 8, 5, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 10, 15, 22, 30, 35, 32, 18, 25, 35, 32, 20, 15, 10, 6, 0, 0, 0, 0]\n  ],\n  \"hourly_heatmap_timezone\": \"America/New_York\"\n}\n```\n\n**GET /api/v1/activity/monthly**\n\nQuery Parameters:\n- `month_start` (date, optional): First day of month (default: current month)\n- `timezone` (string, optional): IANA timezone\n\nResponse:\n```json\n{\n  \"user_id\": \"usr_123456\",\n  \"month_start\": \"2025-12-01\",\n  \"month_end\": \"2025-12-31\",\n  \"timezone\": \"America/New_York\",\n  \"totals\": {\n    \"events\": 3420,\n    \"sessions\": 89,\n    \"active_days\": 18,\n    \"active_hours\": 156,\n    \"active_minutes\": 6240,\n    \"productivity_score\": 74.2\n  },\n  \"comparison\": {\n    \"previous_month_events\": 3180,\n    \"events_change_percent\": 7.5,\n    \"previous_month_score\": 71.8,\n    \"score_change_percent\": 3.3\n  },\n  \"peak_week\": {\n    \"week_number\": 2,\n    \"week_start\": \"2025-12-09\",\n    \"events\": 920\n  },\n  \"peak_day_of_week\": {\n    \"day_of_week\": 1,\n    \"day_name\": \"Tuesday\",\n    \"average_events\": 210\n  },\n  \"weekly_breakdown\": [\n    {\"week\": 1, \"start\": \"2025-12-01\", \"events\": 780, \"active_days\": 4},\n    {\"week\": 2, \"start\": \"2025-12-08\", \"events\": 920, \"active_days\": 5},\n    {\"week\": 3, \"start\": \"2025-12-15\", \"events\": 870, \"active_days\": 5},\n    {\"week\": 4, \"start\": \"2025-12-22\", \"events\": 650, \"active_days\": 3},\n    {\"week\": 5, \"start\": \"2025-12-29\", \"events\": 200, \"active_days\": 1}\n  ],\n  \"daily_breakdown\": [\n    {\"day\": 1, \"events\": 145, \"active\": true},\n    {\"day\": 2, \"events\": 178, \"active\": true},\n    {\"day\": 3, \"events\": 0, \"active\": false}\n  ],\n  \"trends\": {\n    \"best_day\": {\"date\": \"2025-12-10\", \"events\": 215, \"score\": 82.3},\n    \"average_daily_events\": 190,\n    \"most_productive_time\": \"10:00 AM - 12:00 PM\"\n  },\n  \"generated_at\": \"2025-12-15T11:00:00Z\",\n  \"cache_ttl_seconds\": 600\n}\n```\n\n**GET /api/v1/activity/peak-hours**\n\nQuery Parameters:\n- `days` (int, optional): Number of days to analyze (default: 30, max: 30)\n- `timezone` (string, optional): IANA timezone (default: user preference)\n\nResponse:\n```json\n{\n  \"user_id\": \"usr_123456\",\n  \"analysis_period_days\": 30,\n  \"timezone\": \"America/New_York\",\n  \"peak_hours\": [10, 14, 11],\n  \"peak_hours_labels\": [\"10:00 AM\", \"2:00 PM\", \"11:00 AM\"],\n  \"peak_hours_scores\": [85.2, 78.4, 76.1],\n  \"peak_days\": [1, 2, 0],\n  \"peak_days_labels\": [\"Tuesday\", \"Wednesday\", \"Monday\"],\n  \"patterns\": {\n    \"most_active_period\": \"morning\",\n    \"average_start_time\": \"08:30\",\n    \"average_end_time\": \"17:45\",\n    \"consistency_score\": 72.3\n  },\n  \"insights\": [\n    {\n      \"type\": \"peak_hour\",\n      \"message\": \"You're most productive at 10:00 AM with 85% productivity score\"\n    },\n    {\n      \"type\": \"consistency\",\n      \"message\": \"Your activity pattern is fairly consistent (72% consistency)\"\n    },\n    {\n      \"type\": \"suggestion\",\n      \"message\": \"Consider scheduling important tasks between 10 AM and 2 PM\"\n    }\n  ],\n  \"generated_at\": \"2025-12-15T11:00:00Z\"\n}\n```\n\n**POST /api/v1/activity/export**\n\nRequest:\n```json\n{\n  \"start_date\": \"2025-11-15\",\n  \"end_date\": \"2025-12-15\",\n  \"include_raw_events\": false\n}\n```\n\nResponse:\n```json\n{\n  \"export_id\": \"exp_xyz789\",\n  \"status\": \"pending\",\n  \"start_date\": \"2025-11-15\",\n  \"end_date\": \"2025-12-15\",\n  \"include_raw_events\": false,\n  \"estimated_rows\": 4200,\n  \"created_at\": \"2025-12-15T11:00:00Z\"\n}\n```\n\n**GET /api/v1/activity/export/{export_id}**\n\nResponse:\n```json\n{\n  \"export_id\": \"exp_xyz789\",\n  \"status\": \"completed\",\n  \"start_date\": \"2025-11-15\",\n  \"end_date\": \"2025-12-15\",\n  \"include_raw_events\": false,\n  \"created_at\": \"2025-12-15T11:00:00Z\",\n  \"completed_at\": \"2025-12-15T11:00:15Z\",\n  \"file_size_bytes\": 125000,\n  \"row_count\": 4156,\n  \"download_url\": \"/api/v1/activity/export/exp_xyz789/download\",\n  \"expires_at\": \"2025-12-16T11:00:15Z\"\n}\n```\n\n**GET /api/v1/activity/preferences**\n\nResponse:\n```json\n{\n  \"user_id\": \"usr_123456\",\n  \"default_view\": \"weekly\",\n  \"timezone\": \"America/New_York\",\n  \"week_starts_on\": 0,\n  \"chart_type\": \"bar\",\n  \"show_productivity_score\": true,\n  \"show_peak_hours\": true,\n  \"show_comparison\": true\n}\n```\n\n### 4.3 Error Responses\n\n```json\n{\n  \"error\": {\n    \"code\": \"INVALID_DATE_RANGE\",\n    \"message\": \"Export date range cannot exceed 30 days\",\n    \"details\": {\n      \"requested_days\": 45,\n      \"max_days\": 30\n    }\n  }\n}\n```\n\n| Error Code | HTTP Status | Description |\n|------------|-------------|-------------|\n| INVALID_REQUEST | 400 | Malformed request body |\n| INVALID_DATE_RANGE | 400 | Date range exceeds 30 days or is invalid |\n| INVALID_TIMEZONE | 400 | Unknown timezone identifier |\n| EXPORT_NOT_FOUND | 404 | Export job does not exist |\n| EXPORT_EXPIRED | 410 | Export download link has expired |\n| EXPORT_IN_PROGRESS | 409 | Export is still being processed |\n| RATE_LIMITED | 429 | Too many export requests (max 5/day) |\n| INTERNAL_ERROR | 500 | Internal server error |\n\n---\n\n## 5. Implementation Plan\n\n### 5.1 Tasks\n\n| # | Task | Dependencies | Size | Description |\n|---|------|--------------|------|-------------|\n| 1 | Create data models | None | M | Implement all dataclasses in `models/activity_models.py` |\n| 2 | Database migrations | None | M | Create PostgreSQL tables with partitioning |\n| 3 | Activity event repository | 1, 2 | M | Batch insert and query for raw events |\n| 4 | Hourly aggregation repository | 1, 2 | S | CRUD for hourly rollups |\n| 5 | Daily aggregation repository | 1, 2 | S | CRUD for daily rollups |\n| 6 | Weekly aggregation repository | 1, 2 | S | CRUD for weekly rollups |\n| 6b | Monthly aggregation repository | 1, 2 | S | CRUD for monthly rollups |\n| 7 | Activity collector SDK | 1 | M | JavaScript SDK for client-side event collection with offline support |\n| 7b | Offline event queue | 7 | M | IndexedDB-backed durable queue with background sync |\n| 8 | Event ingestion endpoint | 3 | S | POST /api/v1/activity/events |\n| 9 | Hourly aggregation job | 3, 4 | M | Background job to compute hourly rollups |\n| 10 | Daily aggregation job | 4, 5 | M | Background job to compute daily rollups |\n| 11 | Weekly aggregation job | 5, 6 | M | Background job to compute weekly rollups |\n| 11b | Monthly aggregation job | 5, 6b | M | Background job to compute monthly rollups from daily data |\n| 12 | Summary API endpoint | 4, 5, 6, 6b | M | GET /api/v1/activity/summary |\n| 12b | Timezone conversion utilities | 1 | S | Helper functions for UTC to local time conversion |\n| 13 | Daily API endpoint | 5, 12b | S | GET /api/v1/activity/daily |\n| 14 | Weekly API endpoint | 6, 12b | S | GET /api/v1/activity/weekly |\n| 15 | Monthly API endpoint | 6b, 12b | S | GET /api/v1/activity/monthly |\n| 16 | Peak hours analysis service | 4, 5, 12b | M | Compute peak hours from aggregated data (timezone-aware) |\n| 17 | Peak hours API endpoint | 16 | S | GET /api/v1/activity/peak-hours |\n| 18 | Export service | 3, 5, 6, 6b | M | Generate CSV exports |\n| 19 | Export API endpoints | 18 | S | POST/GET export endpoints |\n| 20 | Preferences repository | 1, 2 | S | CRUD for user preferences |\n| 21 | Preferences API endpoints | 20 | S | GET/PUT preferences endpoints |\n| 22 | Dashboard React component | None | L | Main dashboard UI with charts |\n| 23 | Activity charts component | 22 | M | D3.js/Chart.js visualizations |\n| 24 | Heatmap component | 22 | M | Weekly activity heatmap |\n| 25 | Peak hours component | 22, 23 | S | Peak hours visualization |\n| 26 | Export UI component | 22, 19 | S | Export request and download UI |\n| 27 | Service worker setup | 22, 7b | M | Workbox configuration for offline + background sync |\n| 28 | IndexedDB cache layer | 22 | M | Client-side data caching with 30-day expiration |\n| 29 | Mobile responsive styles | 22-26 | M | Responsive CSS for all viewports |\n| 30 | Retention management job | 2 | M | Daily job for partition rotation and aggregate cleanup |\n| 30b | Export file cleanup job | 30 | S | Delete expired export files from storage |\n| 31 | Unit tests | 1-21 | L | Backend unit tests |\n| 32 | Frontend tests | 22-29 | L | React component tests |\n| 33 | Integration tests | 31 | M | API integration tests |\n| 34 | Performance tests | 33 | M | Load time and throughput testing |\n\n### 5.2 File Changes\n\n**New Files:**\n```\nsrc/\n\u251c\u2500\u2500 activity/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models.py                     # Task 1: Data models\n\u2502   \u251c\u2500\u2500 timezone_utils.py             # Task 12b: Timezone conversion utilities\n\u2502   \u251c\u2500\u2500 repositories/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 events_repository.py      # Task 3: Raw events storage\n\u2502   \u2502   \u251c\u2500\u2500 hourly_repository.py      # Task 4: Hourly aggregates\n\u2502   \u2502   \u251c\u2500\u2500 daily_repository.py       # Task 5: Daily aggregates\n\u2502   \u2502   \u251c\u2500\u2500 weekly_repository.py      # Task 6: Weekly aggregates\n\u2502   \u2502   \u251c\u2500\u2500 monthly_repository.py     # Task 6b: Monthly aggregates\n\u2502   \u2502   \u2514\u2500\u2500 preferences_repository.py # Task 20: User preferences\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 aggregation_service.py    # Tasks 9-11, 11b: Aggregation logic\n\u2502   \u2502   \u251c\u2500\u2500 peak_hours_service.py     # Task 16: Peak hours analysis\n\u2502   \u2502   \u2514\u2500\u2500 export_service.py         # Task 18: CSV export\n\u2502   \u251c\u2500\u2500 jobs/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 hourly_aggregation.py     # Task 9: Hourly job\n\u2502   \u2502   \u251c\u2500\u2500 daily_aggregation.py      # Task 10: Daily job\n\u2502   \u2502   \u251c\u2500\u2500 weekly_aggregation.py     # Task 11: Weekly job\n\u2502   \u2502   \u251c\u2500\u2500 monthly_aggregation.py    # Task 11b: Monthly job\n\u2502   \u2502   \u251c\u2500\u2500 retention_manager.py      # Task 30: All-table retention\n\u2502   \u2502   \u2514\u2500\u2500 export_cleanup.py         # Task 30b: Export file cleanup\n\u2502   \u2514\u2500\u2500 api/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 routes.py                 # Tasks 8, 12-15, 17, 19, 21\n\u2502       \u251c\u2500\u2500 schemas.py                # Request/response schemas\n\u2502       \u2514\u2500\u2500 handlers.py               # Request handlers\n\u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 001_create_activity_events.sql\n\u2502   \u251c\u2500\u2500 002_create_activity_hourly.sql\n\u2502   \u251c\u2500\u2500 003_create_activity_daily.sql\n\u2502   \u251c\u2500\u2500 004_create_activity_weekly.sql\n\u2502   \u251c\u2500\u2500 004b_create_activity_monthly.sql\n\u2502   \u251c\u2500\u2500 005_create_export_jobs.sql\n\u2502   \u251c\u2500\u2500 006_create_user_dashboard_preferences.sql\n\u2502   \u251c\u2500\u2500 007_create_retention_log.sql\n\u2502   \u251c\u2500\u2500 008_create_retention_function.sql\n\u2502   \u2514\u2500\u2500 009_create_export_cleanup_function.sql\nfrontend/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ActivityDashboard.tsx     # Task 22: Main dashboard\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ActivityChart.tsx         # Task 23: Charts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ActivityHeatmap.tsx       # Task 24: Heatmap\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 PeakHoursCard.tsx         # Task 25: Peak hours\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 MonthlyTrendsCard.tsx     # Task 22: Monthly view\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ExportButton.tsx          # Task 26: Export UI\n\u2502   \u2502   \u2514\u2500\u2500 common/\n\u2502   \u2502       \u2514\u2500\u2500 OfflineIndicator.tsx      # Offline status\n\u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u251c\u2500\u2500 useActivityData.ts            # Data fetching hook\n\u2502   \u2502   \u2514\u2500\u2500 useOfflineCache.ts            # IndexedDB hook with expiration\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 activityCollector.ts          # Task 7: Event collection SDK\n\u2502   \u2502   \u251c\u2500\u2500 eventQueue.ts                 # Task 7b: Durable offline queue\n\u2502   \u2502   \u2514\u2500\u2500 indexedDBCache.ts             # Task 28: IndexedDB with 30-day TTL\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u2502   \u2514\u2500\u2500 dashboard.css                 # Task 29: Responsive styles\n\u2502   \u2514\u2500\u2500 sw.ts                             # Task 27: Service worker with background sync\ntests/\n\u251c\u2500\u2500 activity/\n\u2502   \u251c\u2500\u2500 test_models.py                    # Task 31\n\u2502   \u251c\u2500\u2500 test_repositories.py              # Task 31\n\u2502   \u251c\u2500\u2500 test_aggregation_service.py       # Task 31\n\u2502   \u251c\u2500\u2500 test_monthly_aggregation.py       # Task 31: Monthly-specific tests\n\u2502   \u251c\u2500\u2500 test_peak_hours_service.py        # Task 31\n\u2502   \u251c\u2500\u2500 test_export_service.py            # Task 31\n\u2502   \u251c\u2500\u2500 test_retention_manager.py         # Task 31: Retention tests\n\u2502   \u251c\u2500\u2500 test_timezone_utils.py            # Task 31: Timezone conversion tests\n\u2502   \u251c\u2500\u2500 test_api.py                       # Task 31\n\u2502   \u2514\u2500\u2500 integration/\n\u2502       \u251c\u2500\u2500 test_full_pipeline.py         # Task 33\n\u2502       \u251c\u2500\u2500 test_retention_compliance.py  # Task 33: 30-day enforcement\n\u2502       \u251c\u2500\u2500 test_offline_sync.py          # Task 33: Offline event delivery\n\u2502       \u2514\u2500\u2500 test_performance.py           # Task 34\nfrontend/\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 components/\n    \u2502   \u251c\u2500\u2500 ActivityDashboard.test.tsx    # Task 32\n    \u2502   \u251c\u2500\u2500 ActivityChart.test.tsx        # Task 32\n    \u2502   \u251c\u2500\u2500 ActivityHeatmap.test.tsx      # Task 32\n    \u2502   \u2514\u2500\u2500 MonthlyTrendsCard.test.tsx    # Task 32\n    \u2514\u2500\u2500 hooks/\n        \u251c\u2500\u2500 useActivityData.test.ts       # Task 32\n        \u251c\u2500\u2500 useOfflineCache.test.ts       # Task 32: Cache expiration tests\n        \u2514\u2500\u2500 eventQueue.test.ts            # Task 32: Offline queue tests\n```\n\n**Modified Files:**\n```\nsrc/\n\u251c\u2500\u2500 config.py                         # Add activity dashboard configuration\n\u251c\u2500\u2500 main.py                           # Register activity API routes\n\u2514\u2500\u2500 jobs/scheduler.py                 # Add aggregation and retention job schedules\nfrontend/\n\u251c\u2500\u2500 src/App.tsx                       # Add dashboard route\n\u2514\u2500\u2500 public/manifest.json              # Add service worker registration\n```\n\n---\n\n## 6. Testing Strategy\n\n### 6.1 Unit Tests\n\n| Component | Test Cases | Coverage Target |\n|-----------|------------|-----------------|\n| Data models | Serialization, validation, productivity score calculation | 95% |\n| Repositories | CRUD operations, batch inserts, queries | 85% |\n| Aggregation service | Hourly/daily/weekly/monthly rollup accuracy | 90% |\n| Peak hours service | Pattern detection, score calculation, timezone handling | 90% |\n| Export service | CSV generation, date range validation | 85% |\n| Retention manager | All-table cleanup, compliance verification, period_end logic | 95% |\n| Timezone utilities | UTC conversion, DST handling, edge cases | 95% |\n| API handlers | Request validation, error responses | 85% |\n| React components | Rendering, user interactions | 80% |\n| Service worker | Caching, offline behavior, background sync | 80% |\n| IndexedDB cache | Storage, expiration, cleanup | 85% |\n| Event queue | Persistence, retry logic, deduplication | 90% |\n\n**Key Unit Tests:**\n\n```python\n# test_aggregation_service.py\ndef test_hourly_aggregation_counts_events():\n    \"\"\"Hourly rollup accurately counts events.\"\"\"\n    events = [\n        ActivityEvent(event_type=ActivityEventType.PAGE_VIEW, timestamp=datetime(2025, 12, 15, 10, 15)),\n        ActivityEvent(event_type=ActivityEventType.PAGE_VIEW, timestamp=datetime(2025, 12, 15, 10, 30)),\n        ActivityEvent(event_type=ActivityEventType.FEATURE_USE, timestamp=datetime(2025, 12, 15, 10, 45)),\n    ]\n\n    rollup = aggregate_hourly(events, hour_start=datetime(2025, 12, 15, 10, 0))\n\n    assert rollup.event_count == 3\n    assert rollup.page_views == 2\n    assert rollup.feature_uses == 1\n\ndef test_daily_aggregation_finds_peak_hour():\n    \"\"\"Daily rollup correctly identifies peak hour.\"\"\"\n    hourly_rollups = [\n        HourlyActivityRollup(hour_start=datetime(2025, 12, 15, 9, 0), event_count=50),\n        HourlyActivityRollup(hour_start=datetime(2025, 12, 15, 10, 0), event_count=100),\n        HourlyActivityRollup(hour_start=datetime(2025, 12, 15, 11, 0), event_count=75),\n    ]\n\n    rollup = aggregate_daily(hourly_rollups, date=date(2025, 12, 15))\n\n    assert rollup.peak_hour == 10\n    assert rollup.peak_hour_events == 100\n\ndef test_productivity_score_calculation():\n    \"\"\"Productivity score correctly weights activity density and engagement.\"\"\"\n    rollup = HourlyActivityRollup(\n        user_id=\"user_123\",\n        hour_start=datetime(2025, 12, 15, 10, 0),\n        event_count=50,\n        active_minutes=45,\n        feature_uses=8\n    )\n\n    score = rollup.productivity_score\n\n    # 45/60 * 50 = 37.5 (density) + 8/10 * 50 = 40 (engagement) = 77.5\n    assert score == 77.5\n\ndef test_weekly_heatmap_structure():\n    \"\"\"Weekly rollup generates correct heatmap dimensions.\"\"\"\n    rollup = aggregate_weekly(daily_rollups, week_start=date(2025, 12, 9))\n\n    assert len(rollup.hourly_heatmap) == 7  # 7 days\n    assert all(len(day) == 24 for day in rollup.hourly_heatmap)  # 24 hours each\n\n\n# test_monthly_aggregation.py\ndef test_monthly_aggregation_from_daily():\n    \"\"\"Monthly rollup correctly aggregates daily data.\"\"\"\n    daily_rollups = [\n        DailyActivityRollup(user_id=\"user_123\", date=date(2025, 12, 1), total_events=150),\n        DailyActivityRollup(user_id=\"user_123\", date=date(2025, 12, 2), total_events=180),\n        DailyActivityRollup(user_id=\"user_123\", date=date(2025, 12, 3), total_events=0),\n        # ... more days\n    ]\n\n    rollup = aggregate_monthly(daily_rollups, month_start=date(2025, 12, 1))\n\n    assert rollup.total_events == sum(d.total_events for d in daily_rollups)\n    assert rollup.active_days == 2  # Days with events > 0\n    assert rollup.month_end == date(2025, 12, 31)  # Correct month end\n\ndef test_monthly_weekly_breakdown():\n    \"\"\"Monthly rollup correctly splits events into weeks.\"\"\"\n    rollup = aggregate_monthly(daily_rollups, month_start=date(2025, 12, 1))\n\n    assert len(rollup.weekly_breakdown) == 5  # Max 5 weeks\n    assert sum(rollup.weekly_breakdown) == rollup.total_events\n\ndef test_monthly_peak_week_identification():\n    \"\"\"Monthly rollup correctly identifies peak week.\"\"\"\n    rollup = aggregate_monthly(daily_rollups_with_peak_in_week_2, month_start=date(2025, 12, 1))\n\n    assert rollup.peak_week == 2\n    assert rollup.peak_week_events > 0\n\n\n# test_peak_hours_service.py\ndef test_peak_hours_identifies_top_three():\n    \"\"\"Peak hours analysis returns top 3 most productive hours.\"\"\"\n    analysis = analyze_peak_hours(user_id=\"user_123\", days=30, timezone=\"UTC\")\n\n    assert len(analysis.peak_hours) == 3\n    assert analysis.peak_hours[0] != analysis.peak_hours[1]\n    assert analysis.peak_hours_scores[0] >= analysis.peak_hours_scores[1]\n\ndef test_peak_hours_respects_timezone():\n    \"\"\"Peak hours are computed in user's local timezone.\"\"\"\n    # UTC data has peak at 15:00 UTC\n    analysis_utc = analyze_peak_hours(user_id=\"user_123\", days=30, timezone=\"UTC\")\n    # For PST (UTC-8), same data peak should be at 07:00 local\n    analysis_pst = analyze_peak_hours(user_id=\"user_123\", days=30, timezone=\"America/Los_Angeles\")\n\n    # The peak hour shifts by timezone offset\n    assert analysis_pst.peak_hours[0] != analysis_utc.peak_hours[0]\n    assert analysis_pst.timezone == \"America/Los_Angeles\"\n\ndef test_most_active_period_classification():\n    \"\"\"Most active period correctly classified based on peak hours.\"\"\"\n    # Mock data with peak at 10 AM local\n    analysis = analyze_peak_hours_with_mock(peak_hour=10)\n    assert analysis.most_active_period == \"morning\"\n\n    # Mock data with peak at 3 PM local\n    analysis = analyze_peak_hours_with_mock(peak_hour=15)\n    assert analysis.most_active_period == \"afternoon\"\n\n\n# test_timezone_utils.py\ndef test_utc_to_local_conversion():\n    \"\"\"UTC timestamps correctly convert to local time.\"\"\"\n    utc_time = datetime(2025, 12, 15, 18, 0, tzinfo=timezone.utc)\n    local_time = convert_to_timezone(utc_time, \"America/New_York\")\n    \n    # EST is UTC-5\n    assert local_time.hour == 13\n\ndef test_hourly_breakdown_timezone_shift():\n    \"\"\"Hourly breakdown array shifts correctly for timezone.\"\"\"\n    utc_breakdown = [0]*6 + [10, 20, 30, 40, 50, 45, 35, 25, 20, 15, 10, 5] + [0]*6\n    local_breakdown = shift_hourly_breakdown(utc_breakdown, \"America/New_York\")\n    \n    # EST is UTC-5, so hour 6 UTC becomes hour 1 local\n    assert local_breakdown[1] == utc_breakdown[6]\n\ndef test_dst_transition_handling():\n    \"\"\"Correctly handles daylight saving time transitions.\"\"\"\n    # March 9, 2025: DST starts in US\n    utc_time = datetime(2025, 3, 9, 10, 0, tzinfo=timezone.utc)\n    local_time = convert_to_timezone(utc_time, \"America/New_York\")\n    \n    # After DST, EDT is UTC-4\n    assert local_time.hour == 6  # 10 - 4 = 6\n\n\n# test_export_service.py\ndef test_csv_export_includes_headers():\n    \"\"\"CSV export includes correct column headers.\"\"\"\n    csv_content = generate_export(user_id=\"user_123\", start_date=date(2025, 12, 1), end_date=date(2025, 12, 15))\n\n    lines = csv_content.split(\"\\n\")\n    headers = lines[0].split(\",\")\n\n    assert \"date\" in headers\n    assert \"total_events\" in headers\n    assert \"productivity_score\" in headers\n\ndef test_export_respects_30_day_limit():\n    \"\"\"Export rejects date ranges over 30 days.\"\"\"\n    with pytest.raises(InvalidDateRangeError):\n        generate_export(\n            user_id=\"user_123\",\n            start_date=date(2025, 11, 1),\n            end_date=date(2025, 12, 15)\n        )\n\n\n# test_retention_manager.py\ndef test_retention_deletes_old_hourly_data():\n    \"\"\"Retention job removes hourly data older than 30 days.\"\"\"\n    # Insert data 31 days old\n    old_timestamp = datetime.utcnow() - timedelta(days=31)\n    insert_hourly_rollup(user_id=\"user_123\", hour_start=old_timestamp)\n\n    run_retention_job()\n\n    # Verify deletion\n    result = query_hourly_rollup(user_id=\"user_123\", hour_start=old_timestamp)\n    assert result is None\n\ndef test_retention_deletes_old_daily_data():\n    \"\"\"Retention job removes daily data older than 30 days.\"\"\"\n    old_date = date.today() - timedelta(days=31)\n    insert_daily_rollup(user_id=\"user_123\", date=old_date)\n\n    run_retention_job()\n\n    result = query_daily_rollup(user_id=\"user_123\", date=old_date)\n    assert result is None\n\ndef test_retention_uses_week_end_for_weekly():\n    \"\"\"Retention job uses week_end, not week_start, for weekly deletion.\"\"\"\n    # Week starting 35 days ago but ending 29 days ago should be kept\n    old_week_start = date.today() - timedelta(days=35)\n    week_end = old_week_start + timedelta(days=6)  # 29 days ago\n    insert_weekly_rollup(user_id=\"user_123\", week_start=old_week_start, week_end=week_end)\n\n    run_retention_job()\n\n    # Should still exist because week_end is within 30 days\n    result = query_weekly_rollup(user_id=\"user_123\", week_start=old_week_start)\n    assert result is not None\n\ndef test_retention_deletes_weekly_when_week_end_old():\n    \"\"\"Retention job deletes weekly rollup when week_end is older than 30 days.\"\"\"\n    # Week ending 31 days ago\n    old_week_start = date.today() - timedelta(days=37)\n    week_end = old_week_start + timedelta(days=6)  # 31 days ago\n    insert_weekly_rollup(user_id=\"user_123\", week_start=old_week_start, week_end=week_end)\n\n    run_retention_job()\n\n    result = query_weekly_rollup(user_id=\"user_123\", week_start=old_week_start)\n    assert result is None\n\ndef test_retention_uses_month_end_for_monthly():\n    \"\"\"Retention job uses month_end, not month_start, for monthly deletion.\"\"\"\n    # Month starting Nov 1 (45 days ago) but ending Nov 30 (15 days ago) should be kept\n    month_start = date(2025, 11, 1)\n    month_end = date(2025, 11, 30)\n    insert_monthly_rollup(user_id=\"user_123\", month_start=month_start, month_end=month_end)\n\n    run_retention_job()  # Assuming today is Dec 15\n\n    # Should still exist because month_end is within 30 days\n    result = query_monthly_rollup(user_id=\"user_123\", month_start=month_start)\n    assert result is not None\n\ndef test_retention_preserves_recent_data():\n    \"\"\"Retention job preserves data within 30-day window.\"\"\"\n    recent_date = date.today() - timedelta(days=15)\n    insert_daily_rollup(user_id=\"user_123\", date=recent_date)\n\n    run_retention_job()\n\n    result = query_daily_rollup(user_id=\"user_123\", date=recent_date)\n    assert result is not None\n\ndef test_export_file_cleanup():\n    \"\"\"Expired export files are deleted from storage.\"\"\"\n    # Create completed export with expired timestamp\n    export = create_export_job(\n        user_id=\"user_123\",\n        status=\"completed\",\n        expires_at=datetime.utcnow() - timedelta(hours=1)\n    )\n    \n    run_export_cleanup_job()\n    \n    # Verify file deleted and status updated\n    result = get_export_job(export.export_id)\n    assert result.status == \"expired\"\n    assert not file_exists(result.file_path)\n\n\n# test_offline_queue.py (frontend)\ndef test_events_queued_when_offline():\n    \"\"\"Events are stored in IndexedDB when offline.\"\"\"\n    set_network_status(offline=True)\n    collector.track('page_view', 'test_page')\n    \n    queued = get_queued_events()\n    assert len(queued) == 1\n    assert queued[0].event_name == 'test_page'\n\ndef test_queued_events_sent_on_reconnect():\n    \"\"\"Queued events are sent when connection restored.\"\"\"\n    set_network_status(offline=True)\n    collector.track('page_view', 'test_page')\n    \n    set_network_status(offline=False)\n    trigger_background_sync()\n    \n    queued = get_queued_events()\n    assert len(queued) == 0  # All sent\n\ndef test_retry_on_failed_delivery():\n    \"\"\"Failed deliveries are retried with backoff.\"\"\"\n    mock_api_failure()\n    collector.track('page_view', 'test_page')\n    \n    # First attempt fails\n    await_retry(attempt=1)\n    \n    mock_api_success()\n    # Second attempt succeeds\n    await_retry(attempt=2)\n    \n    queued = get_queued_events()\n    assert len(queued) == 0\n\ndef test_event_deduplication():\n    \"\"\"Duplicate events are not redelivered.\"\"\"\n    collector.track('page_view', 'test_page', event_id='evt_123')\n    \n    # Simulate network failure after server received but before ack\n    mock_api_failure_after_processing()\n    trigger_background_sync()\n    \n    # Re-sending same event should be deduplicated server-side\n    delivered = get_delivered_events()\n    assert len([e for e in delivered if e.event_id == 'evt_123']) == 1\n```\n\n### 6.2 Integration Tests\n\n| Scenario | Description | Validation |\n|----------|-------------|------------|\n| Event ingestion pipeline | Submit events \u2192 aggregation \u2192 query | Events reflected in hourly rollup within 5 mins |\n| Dashboard load performance | Request summary with 30 days of data | Response time < 500ms |\n| Monthly aggregation pipeline | Daily data \u2192 monthly rollup \u2192 API query | Monthly data accurate and queryable |\n| Export full workflow | Request export \u2192 poll status \u2192 download | CSV downloaded successfully |\n| Offline dashboard | Load dashboard \u2192 disconnect \u2192 reload | Dashboard renders from cache |\n| Offline event collection | Track events offline \u2192 reconnect \u2192 verify delivery | All events delivered with correct timestamps |\n| Retention enforcement (all tables) | Insert 31-day-old data in all tables \u2192 run retention | All old data deleted |\n| Retention with period_end | Insert weekly/monthly with recent end dates | Rollups preserved until period_end exceeds 30 days |\n| Client cache expiration | Cache data \u2192 wait 30 days \u2192 verify deletion | IndexedDB entries expired |\n| Concurrent aggregation | Multiple users aggregating simultaneously | No data corruption |\n| Mobile responsiveness | Load dashboard on mobile viewport | All elements visible and usable |\n| Timezone consistency | Set timezone \u2192 check all endpoints | All times in correct timezone |\n| Peak hours across timezones | Compare peak hours for UTC vs PST user | Hours correctly shifted |\n\n### 6.3 Edge Cases\n\n| Edge Case | Expected Behavior |\n|-----------|-------------------|\n| No activity data | Dashboard shows empty state with helpful message |\n| Single day of activity | Weekly/monthly views show one data point, others empty |\n| Activity at midnight | Correctly assigned to appropriate day |\n| Timezone crossing midnight | Respects user's timezone for day boundaries |\n| Export with no data in range | Returns CSV with headers only |\n| Concurrent export requests | Queued and processed sequentially |\n| 10,000 events in single batch | Batch split and processed in chunks |\n| User changes timezone | Historical data re-rendered in new timezone |\n| Exactly 30 days of data | All data included, none dropped |\n| Activity on Feb 29 (leap year) | Correctly handled in aggregations |\n| Service worker update | User notified, data refreshed |\n| IndexedDB storage full | Graceful degradation, API-only mode |\n| Month with 28/29/30/31 days | Monthly breakdown array handles variable length |\n| Partial month of data | Monthly totals accurate for available days |\n| Data exactly at retention boundary | 30-day-old data preserved, 31-day-old deleted |\n| Weekly rollup spanning retention boundary | Preserved until week_end is >30 days old |\n| Monthly rollup spanning retention boundary | Preserved until month_end is >30 days old |\n| Offline for multiple days | All events queued and delivered on reconnect |\n| Background sync fails repeatedly | Events retained, retried with exponential backoff |\n| DST transition | Timezone conversions handle spring-forward/fall-back |\n| User in half-hour timezone (e.g., IST) | Hourly buckets correctly shifted |\n\n---\n\n## 7. Risks and Mitigations\n\n| Risk | Impact | Likelihood | Mitigation |\n|------|--------|------------|------------|\n| Dashboard load time exceeds 2s | High | Medium | Pre-computed aggregations, CDN caching, lazy loading, composite indexes |\n| Aggregation jobs fall behind | Medium | Low | Horizontal scaling, catchup mode, monitoring alerts |\n| Large export files timeout | Medium | Medium | Streaming response, chunked downloads, progress API |\n| IndexedDB storage limits | Low | Low | LRU eviction, configurable retention in client, 30-day max |\n| Timezone handling errors | Medium | Medium | Consistent UTC storage, comprehensive timezone utils, DST tests |\n| Service worker cache stale | Medium | Low | Version-based cache invalidation, manual refresh option |\n| Peak hours inaccurate with sparse data | Low | Medium | Minimum data threshold, confidence intervals |\n| Retention job deletes wrong data | High | Low | Dry-run mode, audit logging, comprehensive tests, period_end validation |\n| Mobile chart performance | Medium | Medium | Canvas rendering, reduced data points on mobile |\n| Event collection battery drain | Medium | Low | Batching, visibility API, configurable frequency |\n| Compliance violation (>30 day data) | High | Low | All-table retention, client cache expiration, audit logging, period_end columns |\n| Offline events lost | Medium | Low | Durable IndexedDB queue, background sync, retry with backoff |\n| Duplicate events on retry | Low | Medium | Event ID deduplication on server, idempotent ingestion |\n\n### 7.1 Performance Optimization Strategy\n\nTo meet the < 2 second load time requirement:\n\n1. **Pre-computation**: All aggregations (hourly/daily/weekly/monthly) computed in background jobs, never on-demand\n2. **Composite Indexes**: All aggregation tables indexed by (user_id, date/timestamp DESC) for fast user-specific queries\n3. **Response caching**: 5-minute TTL on summary endpoints with ETag support\n4. **CDN caching**: Static assets cached at edge locations\n5. **Lazy loading**: Charts rendered progressively, heatmap loaded after initial render\n6. **Data minimization**: API returns only data needed for current view\n7. **Timezone conversion at read time**: Store UTC, convert on API response to avoid recomputation\n\n**Performance Budget:**\n\n| Asset/Operation | Target | Cumulative |\n|-----------------|--------|------------|\n| HTML + critical CSS | 50ms | 50ms |\n| JavaScript bundle | 200ms | 250ms |\n| API request (cached) | 50ms | 300ms |\n| API request (uncached) | 300ms | 600ms |\n| Chart rendering | 400ms | 1000ms |\n| Full interactivity | 500ms | 1500ms |\n| Buffer | 500ms | 2000ms |\n\n### 7.2 Offline Strategy\n\nThe dashboard uses a \"stale-while-revalidate\" strategy:\n\n1. **First Visit**: Fetch from API, cache in IndexedDB with timestamp, register service worker\n2. **Subsequent Visits**:\n   - Immediately render from IndexedDB cache\n   - Fetch fresh data in background\n   - Update UI when fresh data arrives\n3. **Offline Visit**: Render entirely from IndexedDB, show \"offline\" indicator\n4. **Cache Expiration**: IndexedDB data expires after 30 days (compliance) or 24 hours of no updates (freshness)\n\n```typescript\n// Service worker strategy\nconst CACHE_NAME = 'activity-dashboard-v1';\nconst API_CACHE_TTL = 5 * 60 * 1000; // 5 minutes\n\n// Cache static assets with cache-first strategy\nworkbox.routing.registerRoute(\n  /\\.(js|css|png|svg)$/,\n  new workbox.strategies.CacheFirst({\n    cacheName: CACHE_NAME,\n  })\n);\n\n// Cache API responses with stale-while-revalidate\nworkbox.routing.registerRoute(\n  /\\/api\\/v1\\/activity\\//,\n  new workbox.strategies.StaleWhileRevalidate({\n    cacheName: 'activity-api-cache',\n    plugins: [\n      new workbox.expiration.ExpirationPlugin({\n        maxAgeSeconds: API_CACHE_TTL / 1000,\n      }),\n    ],\n  })\n);\n\n// Register background sync for event delivery\nworkbox.routing.registerRoute(\n  /\\/api\\/v1\\/activity\\/events/,\n  new workbox.strategies.NetworkOnly({\n    plugins: [\n      new workbox.backgroundSync.BackgroundSyncPlugin('activity-events-queue', {\n        maxRetentionTime: 24 * 60, // Retry for up to 24 hours (in minutes)\n      }),\n    ],\n  }),\n  'POST'\n);\n```\n\n```typescript\n// IndexedDB cache with 30-day retention compliance\nconst CACHE_MAX_AGE_DAYS = 30;\n\ninterface CachedData {\n  data: any;\n  cachedAt: number;  // Unix timestamp\n  expiresAt: number; // Unix timestamp (cachedAt + 30 days)\n}\n\nasync function getCachedData(key: string): Promise<any | null> {\n  const cached = await db.get('activity-cache', key);\n  if (!cached) return null;\n  \n  // Enforce 30-day retention\n  if (Date.now() > cached.expiresAt) {\n    await db.delete('activity-cache', key);\n    return null;\n  }\n  \n  return cached.data;\n}\n\nasync function setCachedData(key: string, data: any): Promise<void> {\n  const now = Date.now();\n  await db.put('activity-cache', {\n    data,\n    cachedAt: now,\n    expiresAt: now + (CACHE_MAX_AGE_DAYS * 24 * 60 * 60 * 1000),\n  }, key);\n}\n\n// Cleanup job runs on app init\nasync function cleanupExpiredCache(): Promise<void> {\n  const now = Date.now();\n  const keys = await db.getAllKeys('activity-cache');\n  for (const key of keys) {\n    const cached = await db.get('activity-cache', key);\n    if (cached && now > cached.expiresAt) {\n      await db.delete('activity-cache', key);\n    }\n  }\n}\n```\n\n### 7.3 Retention Compliance Strategy\n\nTo ensure compliance with the 30-day data retention constraint:\n\n1. **Server-Side (PostgreSQL)**:\n   - Raw events: Table partitioning with automatic partition drops\n   - Hourly/Daily aggregation tables: Daily DELETE job for rows older than 30 days\n   - Weekly aggregation tables: DELETE based on week_end < cutoff (not week_start)\n   - Monthly aggregation tables: DELETE based on month_end < cutoff (not month_start)\n   - Export files: 24-hour expiration with cleanup job\n   - Audit logging: All deletions recorded in `activity_retention_log`\n\n2. **Client-Side (IndexedDB)**:\n   - All cached entries include `expiresAt` timestamp (30 days from cache time)\n   - Cleanup runs on app initialization\n   - Expired entries return null and are deleted\n\n3. **Monitoring**:\n   - Alert if retention job fails\n   - Dashboard showing oldest data in each table\n   - Compliance report generated weekly\n\n### 7.4 Timezone Handling Strategy\n\nAll data is stored in UTC and converted to user's local timezone at read time:\n\n1. **Storage (UTC)**:\n   - All timestamps stored as TIMESTAMPTZ in PostgreSQL (UTC)\n   - Aggregation jobs use UTC boundaries for bucketing\n   - Hourly breakdowns stored as 24-element arrays (UTC hours 0-23)\n\n2. **Read-Time Conversion**:\n   - API endpoints accept `timezone` parameter (defaults to user preference)\n   - Server converts all timestamps to requested timezone before response\n   - Hourly breakdown arrays are rotated by timezone offset\n\n3. **Peak Hours Analysis**:\n   - Peak hours computed by converting UTC hourly data to local time\n   - DST transitions handled using pytz/zoneinfo library\n   - Analysis result includes `timezone` field indicating which timezone was used\n\n4. **Client-Side**:\n   - Charts display times in user's local timezone\n   - Heatmaps shift hour columns based on timezone offset\n   - All displayed times labeled with timezone indicator\n\n**Timezone Conversion Utilities:**\n\n```python\n# timezone_utils.py\nfrom datetime import datetime, time\nfrom zoneinfo import ZoneInfo\nfrom typing import List\n\ndef convert_to_timezone(utc_dt: datetime, timezone: str) -> datetime:\n    \"\"\"Convert UTC datetime to specified timezone.\"\"\"\n    utc_tz = ZoneInfo(\"UTC\")\n    target_tz = ZoneInfo(timezone)\n    \n    if utc_dt.tzinfo is None:\n        utc_dt = utc_dt.replace(tzinfo=utc_tz)\n    \n    return utc_dt.astimezone(target_tz)\n\ndef get_timezone_offset_hours(timezone: str, reference_date: datetime = None) -> int:\n    \"\"\"Get timezone offset from UTC in hours (handles DST).\"\"\"\n    if reference_date is None:\n        reference_date = datetime.utcnow()\n    \n    utc_tz = ZoneInfo(\"UTC\")\n    target_tz = ZoneInfo(timezone)\n    \n    utc_dt = reference_date.replace(tzinfo=utc_tz)\n    local_dt = utc_dt.astimezone(target_tz)\n    \n    offset = local_dt.utcoffset()\n    return int(offset.total_seconds() // 3600)\n\ndef shift_hourly_breakdown(utc_breakdown: List[int], timezone: str, reference_date: datetime = None) -> List[int]:\n    \"\"\"Shift a 24-element hourly breakdown array to local timezone.\"\"\"\n    offset = get_timezone_offset_hours(timezone, reference_date)\n    \n    # Positive offset means local time is ahead of UTC\n    # So hour 0 UTC becomes hour +offset local\n    shifted = [0] * 24\n    for utc_hour in range(24):\n        local_hour = (utc_hour + offset) % 24\n        shifted[local_hour] = utc_breakdown[utc_hour]\n    \n    return shifted\n\ndef compute_local_peak_hour(utc_hourly_scores: List[float], timezone: str) -> int:\n    \"\"\"Compute peak hour in local timezone from UTC scores.\"\"\"\n    local_scores = shift_hourly_breakdown(utc_hourly_scores, timezone)\n    return local_scores.index(max(local_scores))\n\ndef classify_time_period(hour: int) -> str:\n    \"\"\"Classify hour into time period (morning/afternoon/evening/night).\"\"\"\n    if 5 <= hour < 12:\n        return \"morning\"\n    elif 12 <= hour < 17:\n        return \"afternoon\"\n    elif 17 <= hour < 21:\n        return \"evening\"\n    else:\n        return \"night\"\n```\n\n---\n\n## 8. Open Questions\n\n1. **Activity Tracking Opt-In**: Should activity tracking require explicit user consent? Are there GDPR/CCPA implications for storing activity data?\n\n2. **Cross-Device Data**: Should activity from multiple devices be combined? If so, how do we handle device identification without compromising privacy?\n\n3. **Productivity Score Formula**: Is the current productivity score calculation (activity density + feature engagement) appropriate, or should it be configurable per user?\n\n4. **Export Frequency Limits**: What is the appropriate rate limit for exports (currently proposed: 5 per day)?\n\n5. **Heatmap Color Scale**: Should the heatmap use absolute values or relative-to-user-max values for color intensity?\n\n6. **Activity Event Granularity**: What level of detail should be captured in event metadata? More detail enables better insights but increases storage costs.\n\n7. **Real-Time Updates**: Should the dashboard update in real-time (WebSocket) or on-demand (manual refresh)?\n\n8. **Historical Comparison**: Should we support comparing current period to the same period last month/year, even though we only retain 30 days?\n\n9. **Mobile App Activity**: If there's a mobile app, should mobile activity be included in the dashboard?\n\n---\n\n## 9. Appendix\n\n### 9.1 Sample Dashboard UI Wireframe\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Activity Dashboard                              [Weekly \u25bc] [Export]    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 847          \u2502  \u2502 42           \u2502  \u2502 72.5         \u2502  \u2502 +6.9%        \u2502\u2502\n\u2502  \u2502 Total Events \u2502  \u2502 Active Hours \u2502  \u2502 Productivity \u2502  \u2502 vs Last Week \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Daily Activity                                                      \u2502 \u2502\n\u2502  \u2502  200\u2502                    \u2588\u2588                                         \u2502 \u2502\n\u2502  \u2502     \u2502        \u2588\u2588          \u2588\u2588                      \u2588\u2588      \u2588\u2588         \u2502 \u2502\n\u2502  \u2502  100\u2502  \u2588\u2588    \u2588\u2588    \u2588\u2588    \u2588\u2588                \u2588\u2588    \u2588\u2588      \u2588\u2588         \u2502 \u2502\n\u2502  \u2502     \u2502  \u2588\u2588    \u2588\u2588    \u2588\u2588    \u2588\u2588                \u2588\u2588    \u2588\u2588      \u2588\u2588         \u2502 \u2502\n\u2502  \u2502    0\u2502\u2500\u2500Mon\u2500\u2500\u2500Tue\u2500\u2500\u2500Wed\u2500\u2500\u2500Thu\u2500\u2500\u2500Fri\u2500\u2500\u2500Sat\u2500\u2500\u2500Sun\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Peak Hours (EST)                \u2502  \u2502 Weekly Heatmap               \u2502  \u2502\n\u2502  \u2502                                 \u2502  \u2502                               \u2502  \u2502\n\u2502  \u2502  \ud83c\udfc6 10:00 AM  (85.2)           \u2502  \u2502     0  4  8  12 16 20        \u2502  \u2502\n\u2502  \u2502  \ud83e\udd48 2:00 PM   (78.4)           \u2502  \u2502 Mon \u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2591\u2591          \u2502  \u2502\n\u2502  \u2502  \ud83e\udd49 11:00 AM  (76.1)           \u2502  \u2502 Tue \u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591          \u2502  \u2502\n\u2502  \u2502                                 \u2502  \u2502 Wed \u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2591\u2591          \u2502  \u2502\n\u2502  \u2502  Most active: Morning           \u2502  \u2502 Thu \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591          \u2502  \u2502\n\u2502  \u2502  Average: 8:30 AM - 5:45 PM     \u2502  \u2502 Fri \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 Sat \u2591\u2591\u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591          \u2502  \u2502\n\u2502                                        \u2502 Sun \u2591\u2591\u2591\u2591\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591          \u2502  \u2502\n\u2502                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 9.2 Monthly View Wireframe\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Activity Dashboard                             [Monthly \u25bc] [Export]    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 3,420        \u2502  \u2502 18           \u2502  \u2502 74.2         \u2502  \u2502 +7.5%        \u2502\u2502\n\u2502  \u2502 Total Events \u2502  \u2502 Active Days  \u2502  \u2502 Productivity \u2502  \u2502 vs Last Month\u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Weekly Breakdown - December 2025                                    \u2502 \u2502\n\u2502  \u2502 1000\u2502                                                               \u2502 \u2502\n\u2502  \u2502     \u2502        \u2588\u2588\u2588\u2588                                                   \u2502 \u2502\n\u2502  \u2502  500\u2502  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588                                       \u2502 \u2502\n\u2502  \u2502     \u2502  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588  \u2588\u2588                                   \u2502 \u2502\n\u2502  \u2502    0\u2502\u2500\u2500Wk1\u2500\u2500\u2500Wk2\u2500\u2500\u2500Wk3\u2500\u2500\u2500Wk4\u2500\u2500\u2500Wk5\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Daily Activity - December 2025                                      \u2502 \u2502\n\u2502  \u2502                                                                      \u2502 \u2502\n\u2502  \u2502  S  M  T  W  T  F  S                                                \u2502 \u2502\n\u2502  \u2502     1  2  3  4  5  6    \u2588\u2588 High (>150 events)                       \u2502 \u2502\n\u2502  \u2502  7  8  9 10 11 12 13    \u2593\u2593 Medium (50-150)                          \u2502 \u2502\n\u2502  \u2502 14 15 16 17 18 19 20    \u2591\u2591 Low (<50)                                \u2502 \u2502\n\u2502  \u2502 21 22 23 24 25 26 27    \u00b7\u00b7 No activity                              \u2502 \u2502\n\u2502  \u2502 28 29 30 31                                                          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Monthly Insights                \u2502  \u2502 Best Days                    \u2502  \u2502\n\u2502  \u2502                                 \u2502  \u2502                               \u2502  \u2502\n\u2502  \u2502 Peak Week: Week 2 (920 events) \u2502  \u2502 1. Dec 10 - 215 events       \u2502  \u2502\n\u2502  \u2502 Most Active: Tuesdays          \u2502  \u2502 2. Dec 17 - 198 events       \u2502  \u2502\n\u2502  \u2502 Avg Daily: 190 events          \u2502  \u2502 3. Dec 9  - 189 events       \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 9.3 CSV Export Format\n\n```csv\ndate,day_of_week,total_events,total_sessions,active_hours,active_minutes,productivity_score,peak_hour,page_views,feature_uses\n2025-12-15,Sunday,190,5,10,420,75.4,10,120,45\n2025-12-14,Saturday,145,4,7,280,65.8,14,95,32\n2025-12-11,Wednesday,167,5,8,360,73.1,11,110,38\n2025-12-10,Tuesday,189,6,9,400,78.5,10,125,42\n2025-12-09,Monday,156,4,8,340,71.2,10,100,35\n```\n\n### 9.4 Activity Collector SDK Usage\n\n```typescript\nimport { ActivityCollector } from '@app/activity-collector';\n\n// Initialize collector with offline support\nconst collector = new ActivityCollector({\n  endpoint: '/api/v1/activity/events',\n  batchSize: 100,\n  flushInterval: 30000, // 30 seconds\n  sessionTimeout: 1800000, // 30 minutes\n  offlineStorage: {\n    enabled: true,\n    maxQueueSize: 10000, // Max events to queue offline\n    retryStrategy: 'exponential', // 'exponential' | 'linear' | 'fixed'\n    maxRetries: 10,\n    baseRetryDelay: 1000, // 1 second initial delay\n    maxRetryDelay: 300000, // 5 minutes max delay\n  },\n});\n\n// Track page view\ncollector.track('page_view', 'dashboard_viewed', {\n  page_path: '/dashboard',\n  referrer: document.referrer,\n});\n\n// Track feature use with duration\ncollector.trackTimed('feature_use', 'report_generated', {\n  report_type: 'weekly',\n});\n// ... later\ncollector.endTimed('report_generated');\n\n// Manual flush (e.g., before page unload)\ncollector.flush();\n\n// Check queue status\nconst queueStatus = await collector.getQueueStatus();\nconsole.log(`${queueStatus.pendingEvents} events pending delivery`);\n\n// Force retry of failed events\nawait collector.retryFailedEvents();\n```\n\n**Offline Event Queue Implementation:**\n\n```typescript\n// eventQueue.ts\nimport { openDB, DBSchema, IDBPDatabase } from 'idb';\n\ninterface EventQueueDB extends DBSchema {\n  'pending-events': {\n    key: string;\n    value: {\n      eventId: string;\n      event: ActivityEvent;\n      queuedAt: number;\n      attempts: number;\n      lastAttemptAt: number | null;\n      nextRetryAt: number;\n    };\n    indexes: { 'by-next-retry': number };\n  };\n  'sent-events': {\n    key: string;\n    value: {\n      eventId: string;\n      sentAt: number;\n    };\n  };\n}\n\nclass EventQueue {\n  private db: IDBPDatabase<EventQueueDB> | null = null;\n  private readonly MAX_QUEUE_SIZE = 10000;\n  private readonly DEDUP_WINDOW_MS = 24 * 60 * 60 * 1000; // 24 hours\n\n  async init(): Promise<void> {\n    this.db = await openDB<EventQueueDB>('activity-event-queue', 1, {\n      upgrade(db) {\n        const pendingStore = db.createObjectStore('pending-events', {\n          keyPath: 'eventId',\n        });\n        pendingStore.createIndex('by-next-retry', 'nextRetryAt');\n        \n        db.createObjectStore('sent-events', { keyPath: 'eventId' });\n      },\n    });\n  }\n\n  async enqueue(event: ActivityEvent): Promise<void> {\n    if (!this.db) throw new Error('Queue not initialized');\n\n    // Check for duplicate (already sent recently)\n    const existing = await this.db.get('sent-events', event.event_id);\n    if (existing && Date.now() - existing.sentAt < this.DEDUP_WINDOW_MS) {\n      return; // Skip duplicate\n    }\n\n    // Check queue size limit\n    const count = await this.db.count('pending-events');\n    if (count >= this.MAX_QUEUE_SIZE) {\n      // Remove oldest events to make room\n      const oldest = await this.db.getAllFromIndex(\n        'pending-events',\n        'by-next-retry',\n        IDBKeyRange.upperBound(Date.now()),\n        count - this.MAX_QUEUE_SIZE + 1\n      );\n      for (const item of oldest) {\n        await this.db.delete('pending-events', item.eventId);\n      }\n    }\n\n    await this.db.put('pending-events', {\n      eventId: event.event_id,\n      event,\n      queuedAt: Date.now(),\n      attempts: 0,\n      lastAttemptAt: null,\n      nextRetryAt: Date.now(),\n    });\n  }\n\n  async getEventsToSend(limit: number = 100): Promise<ActivityEvent[]> {\n    if (!this.db) throw new Error('Queue not initialized');\n\n    const now = Date.now();\n    const items = await this.db.getAllFromIndex(\n      'pending-events',\n      'by-next-retry',\n      IDBKeyRange.upperBound(now),\n      limit\n    );\n\n    return items.map(item => item.event);\n  }\n\n  async markSent(eventIds: string[]): Promise<void> {\n    if (!this.db) throw new Error('Queue not initialized');\n\n    const tx = this.db.transaction(['pending-events', 'sent-events'], 'readwrite');\n    const now = Date.now();\n\n    for (const eventId of eventIds) {\n      await tx.objectStore('pending-events').delete(eventId);\n      await tx.objectStore('sent-events').put({ eventId, sentAt: now });\n    }\n\n    await tx.done;\n  }\n\n  async markFailed(eventId: string, config: RetryConfig): Promise<void> {\n    if (!this.db) throw new Error('Queue not initialized');\n\n    const item = await this.db.get('pending-events', eventId);\n    if (!item) return;\n\n    const newAttempts = item.attempts + 1;\n    if (newAttempts >= config.maxRetries) {\n      // Give up after max retries\n      await this.db.delete('pending-events', eventId);\n      return;\n    }\n\n    // Calculate next retry with exponential backoff\n    const delay = Math.min(\n      config.baseRetryDelay * Math.pow(2, newAttempts),\n      config.maxRetryDelay\n    );\n\n    await this.db.put('pending-events', {\n      ...item,\n      attempts: newAttempts,\n      lastAttemptAt: Date.now(),\n      nextRetryAt: Date.now() + delay,\n    });\n  }\n\n  async cleanupOldSentRecords(): Promise<void> {\n    if (!this.db) throw new Error('Queue not initialized');\n\n    const cutoff = Date.now() - this.DEDUP_WINDOW_MS;\n    const allSent = await this.db.getAll('sent-events');\n    \n    for (const record of allSent) {\n      if (record.sentAt < cutoff) {\n        await this.db.delete('sent-events', record.eventId);\n      }\n    }\n  }\n}\n```\n\n### 9.5 Configuration Schema\n\n```yaml\nactivity_dashboard:\n  # Event collection\n  collection:\n    batch_size: 100\n    flush_interval_seconds: 30\n    max_events_per_batch: 500\n    offline:\n      enabled: true\n      max_queue_size: 10000\n      max_retries: 10\n      base_retry_delay_ms: 1000\n      max_retry_delay_ms: 300000\n\n  # Aggregation jobs\n  aggregation:\n    hourly_interval_minutes: 5\n    daily_run_time: \"00:30\"  # UTC\n    weekly_run_time: \"01:00\"  # UTC\n    monthly_run_time: \"01:30\"  # UTC\n\n  # Data retention (PRD constraint: 30 days max)\n  retention:\n    max_days: 30\n    cleanup_time: \"02:00\"  # UTC\n    tables:\n      - name: activity_events    # Partition-based\n        retention_column: timestamp\n      - name: activity_hourly    # Row deletion\n        retention_column: hour_start\n      - name: activity_daily     # Row deletion\n        retention_column: date\n      - name: activity_weekly    # Row deletion, uses period_end\n        retention_column: week_end\n      - name: activity_monthly   # Row deletion, uses period_end\n        retention_column: month_end\n    export_expiry_hours: 24\n    client_cache_max_days: 30\n\n  # Timezone handling\n  timezone:\n    default: \"UTC\"\n    supported_timezones: \"all\"  # Or list specific ones\n    convert_at: \"read_time\"  # \"read_time\" | \"write_time\"\n\n  # Export settings\n  export:\n    max_range_days: 30\n    max_exports_per_day: 5\n    download_expiry_hours: 24\n\n  # API caching\n  caching:\n    summary_ttl_seconds: 300\n    peak_hours_ttl_seconds: 3600\n    monthly_ttl_seconds: 600\n\n  # Performance\n  performance:\n    max_heatmap_cells: 168  # 7 days * 24 hours\n    chart_data_points_mobile: 7\n    chart_data_points_desktop: 30\n```\n\n### 9.6 Monitoring Metrics\n\n| Metric | Type | Labels | Description |\n|--------|------|--------|-------------|\n| `activity_events_ingested_total` | Counter | event_type | Total events ingested |\n| `activity_events_batch_size` | Histogram | - | Size of ingested batches |\n| `activity_events_offline_delivered_total` | Counter | - | Events delivered after being queued offline |\n| `activity_events_queue_depth` | Gauge | - | Current offline event queue depth |\n| `activity_aggregation_duration_seconds` | Histogram | granularity | Time to complete aggregation |\n| `activity_aggregation_lag_seconds` | Gauge | granularity | Delay from event to aggregation |\n| `activity_dashboard_load_duration_seconds` | Histogram | cache_status | Dashboard load time |\n| `activity_api_request_duration_seconds` | Histogram | endpoint, timezone | API response time |\n| `activity_export_requests_total` | Counter | status | Export requests by outcome |\n| `activity_export_file_size_bytes` | Histogram | - | Size of generated exports |\n| `activity_offline_loads_total` | Counter | - | Dashboard loads from offline cache |\n| `activity_retention_rows_deleted_total` | Counter | table | Rows deleted by retention job per table |\n| `activity_retention_job_duration_seconds` | Histogram | - | Time for retention job to complete |\n| `activity_storage_bytes` | Gauge | table | Storage used per table |\n| `activity_oldest_data_days` | Gauge | table | Age of oldest data in each table (compliance) |\n| `activity_timezone_conversions_total` | Counter | timezone | API requests by timezone |\n<<<SPEC_END>>>\n\n<<<RUBRIC_START>>>\n{\n  \"round\": 2,\n  \"previous_scores\": {\"clarity\": 0.83, \"coverage\": 0.72, \"architecture\": 0.7, \"risk\": 0.65},\n  \"current_scores\": {\"clarity\": 0.88, \"coverage\": 0.88, \"architecture\": 0.85, \"risk\": 0.82},\n  \"issues_accepted\": 3,\n  \"issues_rejected\": 0,\n  \"issues_deferred\": 0,\n  \"issues_partial\": 0,\n  \"continue_debate\": false,\n  \"ready_for_approval\": true,\n  \"meta\": {\n    \"recommend_human_review\": false,\n    \"review_reason\": null\n  },\n  \"dispute_resolutions\": []\n}\n<<<RUBRIC_END>>>",
  "session_id": "914136dc-8ad2-4fc7-a609-1bffe04557a5",
  "total_cost_usd": 1.0082985,
  "usage": {
    "input_tokens": 2,
    "cache_creation_input_tokens": 44938,
    "cache_read_input_tokens": 0,
    "output_tokens": 28717,
    "server_tool_use": {
      "web_search_requests": 0,
      "web_fetch_requests": 0
    },
    "service_tier": "standard",
    "cache_creation": {
      "ephemeral_1h_input_tokens": 0,
      "ephemeral_5m_input_tokens": 44938
    }
  },
  "modelUsage": {
    "claude-haiku-4-5-20251001": {
      "inputTokens": 1021,
      "outputTokens": 142,
      "cacheReadInputTokens": 0,
      "cacheCreationInputTokens": 0,
      "webSearchRequests": 0,
      "costUSD": 0.001731,
      "contextWindow": 200000
    },
    "claude-opus-4-5-20251101": {
      "inputTokens": 1151,
      "outputTokens": 28798,
      "cacheReadInputTokens": 0,
      "cacheCreationInputTokens": 44938,
      "webSearchRequests": 0,
      "costUSD": 1.0065675,
      "contextWindow": 200000
    }
  },
  "permission_denials": [],
  "uuid": "e7e2d966-5f4f-4ccf-b5af-3783a04c8c6a"
}