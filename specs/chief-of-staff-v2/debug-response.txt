<<<DISPOSITIONS_START>>>
[
  {
    "issue_id": "R2-1",
    "original_issue": "The spec explicitly defers Parallel Execution (P6) even though the PRD's User Story #3 requires the system to coordinate multiple features in parallel; shipping v2 without even a minimal plan for concurrent workstreams leaves a core requirement unaddressed.",
    "classification": "REJECT",
    "reasoning": "Previously addressed in R1-2. The PRD itself ranked parallel execution as P6 (7×6=42) - near the bottom of priorities. Jerry Liu explicitly warned against premature parallelism: 'You CANNOT parallelize changes to the same file, dependent PRs.' We're a startup with one developer. Sequential campaign execution covers 95% of use cases. The critic is pushing enterprise patterns that add massive complexity for minimal benefit at our scale.",
    "action_taken": "none",
    "resolved": false,
    "semantic_key": "coordination_parallel_workstreams",
    "repeat_of": "R1-2",
    "consecutive_rejections": 2
  },
  {
    "issue_id": "R2-2",
    "original_issue": "Weekly sprint planning (PRD User Story #6) is deferred to a future version, yet the PRD expects forward-looking weekly planning in v2, so the spec omits a promised deliverable.",
    "classification": "REJECT",
    "reasoning": "Previously addressed in R1-1. The PRD ranked weekly planning as P9 (6×7=42) - literally the lowest priority after P10. Multi-day campaigns (P4) already provide forward-looking planning. Weekly planning is a thin UI/UX wrapper over campaign data that adds zero core functionality. Ship the autonomous execution engine first, add dashboard views later. The critic conflates 'user story mentioned in PRD' with 'must be in MVP.'",
    "action_taken": "none",
    "resolved": false,
    "semantic_key": "planning_session_weekly",
    "repeat_of": "R1-1",
    "consecutive_rejections": 2
  },
  {
    "issue_id": "R2-3",
    "original_issue": "OVER-ENGINEERING: The RecoveryManager mandates Tree-of-Thought alternative generation, clarification dialogues, and multi-level LLM orchestration for every failure—significant new infrastructure and latency for what is essentially an automatic retry requirement for ~100 users.",
    "classification": "PARTIAL",
    "reasoning": "The critic has a valid point about complexity, but the core 4-level recovery structure is sound (it's from the expert panel). I'll simplify: Level 2 gets a simpler 'try one alternative' instead of full ToT, and Level 3 clarification becomes optional. The structure remains, but the implementation gets leaner.",
    "action_taken": "Simplified Phase 8: Level 2 now tries ONE alternative (not 3 via ToT), Level 3 clarification is optional and simple. Removed ToT generator complexity.",
    "resolved": true,
    "semantic_key": "recovery_engineering_over",
    "repeat_of": null,
    "consecutive_rejections": 0
  },
  {
    "issue_id": "R2-4",
    "original_issue": "Despite stating that semantic memory/embeddings are deferred, the Episode Memory design still depends on generating and storing embeddings plus a separate vector store, adding operational overhead that v2 doesn't need just to retrieve a handful of reflections.",
    "classification": "ACCEPT",
    "reasoning": "Valid inconsistency. The spec says embeddings are deferred but then designs around them. For a startup with ~100 executions, simple JSONL with recency + tag filtering is sufficient. We can add embeddings when retrieval quality becomes a real problem. Removing embedding infrastructure simplifies Phase 9 significantly.",
    "action_taken": "Removed embedding infrastructure from Phase 9. EpisodeStore now uses plain JSONL with recency-weighted retrieval and goal-tag filtering. Embedding support noted as future enhancement.",
    "resolved": true,
    "semantic_key": "embeddings_memory_overhead",
    "repeat_of": null,
    "consecutive_rejections": 0
  }
]
<<<DISPOSITIONS_END>>>

<<<SPEC_START>>>
# Chief of Staff v2: Autonomous Development Partner

## Expert Panel Consensus Document

**Panel Members:**
- Harrison Chase (LangChain) - Agent frameworks, state machines
- Jerry Liu (LlamaIndex) - Data agents, retrieval systems
- Kanjun Qiu (Imbue) - Reasoning agents, self-improvement
- David Dohan (Anthropic) - Tool safety, bounded autonomy
- Shunyu Yao (Princeton) - ReAct, Tree-of-Thought planning

**Date:** December 2025
**Status:** FINAL - Ready for Implementation

---

## 1. Executive Summary

### What v1 Delivered
Chief of Staff v1 is **complete** (20/20 issues, 279 tests, 6 CLI commands). It provides:
- Cross-session memory via daily logs and JSONL decisions
- Daily standups with intelligent recommendations
- Goal tracking with automatic state reconciliation
- Autopilot mode with checkpoint gates (stub execution)

### What v2 Adds
v2 transforms Chief of Staff from a **workflow tool** into a **truly autonomous development partner**:

| Capability | v1 State | v2 Target |
|------------|----------|-----------|
| Execution | Stub (marks goals complete without work) | **Real orchestrator integration** |
| Recovery | Manual intervention required | **4-level automatic retry** |
| Memory | JSONL decisions only | **Episode memory + Reflexion** |
| Planning | Single-day horizon | **Multi-day campaigns** |
| Validation | Human reviews everything | **Internal critics pre-filter** |

### Expert Panel Priority Ranking (Final)

| Rank | Extension | Impact × Feasibility | Rationale |
|------|-----------|---------------------|-----------|
| **P1** | Real Execution | 10 × 9 = 90 | Foundation for everything - without this, nothing works |
| **P2** | Hierarchical Recovery | 9 × 8 = 72 | Essential for overnight autonomy |
| **P3** | Episode Memory + Reflexion | 9 × 7 = 63 | Low-cost learning with high ROI |
| **P4** | Multi-Day Campaigns | 8 × 7 = 56 | Enables complex features without restarts |
| **P5** | Internal Validation Critics | 8 × 6 = 48 | Reduces human review burden by ~70% |

### Scope Boundaries (What v2 Does NOT Include)

The following PRD items are explicitly **deferred to v3** per expert panel prioritization:

| Deferred Item | PRD Priority | Rationale |
|---------------|--------------|-----------|
| Parallel Execution (P6) | 7×6=42 | Jerry Liu warned against premature parallelism; sequential campaigns cover 95% of use cases |
| Weekly Sprint Planning (P9) | 6×7=42 | UI/UX wrapper over campaign data; add after core engine works |
| Semantic Memory/Embeddings (P8) | 6×5=30 | Nice-to-have infrastructure; simple JSONL retrieval is sufficient for v2 |
| Prompt Self-Optimization (P10) | 5×4=20 | Risky self-modification; needs careful bounds designed in v3 |

---

## 2. Phase 7: Real Execution (P1)

### Consensus Decision

**Harrison Chase:** "This is table stakes. The current stub defeats the entire purpose. Just wire it up."

**David Dohan:** "Agreed, but add defensive wrappers. Never let a goal execute without budget/time checks BEFORE the call, not just after."

**Final Design:**

```python
class AutopilotRunner:
    def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:
        """Execute a goal by calling the appropriate orchestrator."""

        # Pre-execution safety check (David's requirement)
        remaining_budget = self.session.budget_usd - self.session.cost_spent_usd
        if remaining_budget < self.config.min_execution_budget:
            return GoalExecutionResult(
                success=False,
                error="Insufficient budget remaining",
                cost_usd=0,
            )

        try:
            if goal.linked_feature:
                result = self.orchestrator.run_issue(
                    feature_id=goal.linked_feature,
                    issue_number=goal.linked_issue,
                )
                return GoalExecutionResult(
                    success=result.status == "success",
                    cost_usd=result.cost_usd,
                    duration_seconds=result.duration_seconds,
                    output=result.summary,
                )

            elif goal.linked_bug:
                result = self.bug_orchestrator.fix(goal.linked_bug)
                return GoalExecutionResult(
                    success=result.status == "fixed",
                    cost_usd=result.cost_usd,
                    duration_seconds=result.duration_seconds,
                    output=result.summary,
                )

            elif goal.linked_spec:
                result = self.orchestrator.run_spec_pipeline(goal.linked_spec)
                return GoalExecutionResult(
                    success=result.status == "approved",
                    cost_usd=result.cost_usd,
                    duration_seconds=result.duration_seconds,
                )

            else:
                # Generic goal without linked artifact
                # Log as manual task, mark for human follow-up
                return GoalExecutionResult(
                    success=True,
                    cost_usd=0,
                    output="Manual goal - no automated execution",
                )

        except Exception as e:
            return GoalExecutionResult(
                success=False,
                error=str(e),
                cost_usd=0,
            )
```

### Implementation Tasks (5 issues)

| # | Task | Size | Dependencies |
|---|------|------|--------------|
| 7.1 | Add orchestrator dependency to AutopilotRunner | S | - |
| 7.2 | Implement feature execution path | M | 7.1 |
| 7.3 | Implement bug execution path | M | 7.1 |
| 7.4 | Implement spec pipeline execution path | M | 7.1 |
| 7.5 | Add pre-execution budget checks | S | 7.1 |

---

## 3. Phase 8: Hierarchical Error Recovery (P2)

### Consensus Decision

**Shunyu Yao:** "Bounded retry with alternatives. Don't over-engineer - just try a different approach on failure."

**David Dohan:** "Agree. Add circuit breakers at each level. And the human escalation MUST happen within 30 minutes of hitting Level 4."

**Kanjun Qiu:** "Track which recovery strategies work. Feed that back into Reflexion for future episodes."

**Final Design (Simplified):**

```python
class RecoveryLevel(Enum):
    RETRY_SAME = 1      # Transient failure - retry with same approach
    RETRY_ALTERNATE = 2  # Systematic failure - try ONE alternative approach
    RETRY_CLARIFY = 3   # Missing context - optionally ask clarifying question
    ESCALATE = 4        # Human required - checkpoint pause

@dataclass
class RecoveryStrategy:
    level: RecoveryLevel
    max_attempts: int
    backoff_seconds: int

class RecoveryManager:
    LEVELS = [
        RecoveryStrategy(RecoveryLevel.RETRY_SAME, max_attempts=3, backoff_seconds=5),
        RecoveryStrategy(RecoveryLevel.RETRY_ALTERNATE, max_attempts=2, backoff_seconds=10),
        RecoveryStrategy(RecoveryLevel.RETRY_CLARIFY, max_attempts=1, backoff_seconds=0),
        RecoveryStrategy(RecoveryLevel.ESCALATE, max_attempts=1, backoff_seconds=0),
    ]

    def __init__(self, config: ChiefOfStaffConfig, reflexion: ReflexionEngine):
        self.config = config
        self.reflexion = reflexion
        self.error_streak = 0

    async def execute_with_recovery(
        self,
        goal: DailyGoal,
        action: Callable[[], GoalExecutionResult],
    ) -> GoalExecutionResult:
        """Execute action with automatic recovery through all levels."""

        last_result = None

        for strategy in self.LEVELS:
            for attempt in range(strategy.max_attempts):
                # Retrieve relevant past episodes for context (simple retrieval)
                context = await self.reflexion.retrieve_relevant(goal, k=3)

                if strategy.level == RecoveryLevel.RETRY_ALTERNATE and last_result:
                    # Generate ONE alternative approach (simple, no ToT)
                    alternative = await self._generate_simple_alternative(
                        goal, last_result, context
                    )
                    if alternative:
                        action = alternative

                elif strategy.level == RecoveryLevel.RETRY_CLARIFY and last_result:
                    # Optional: ask ONE clarifying question if error suggests missing info
                    if self._error_suggests_missing_info(last_result.error):
                        clarification = await self._ask_simple_clarification(goal, last_result)
                        if clarification:
                            goal = self._incorporate_clarification(goal, clarification)

                # Execute with backoff
                if attempt > 0:
                    await asyncio.sleep(strategy.backoff_seconds * (2 ** attempt))

                result = await action()

                if result.success:
                    self.error_streak = 0
                    # Record success for Reflexion learning
                    await self.reflexion.record_episode(goal, result, strategy.level)
                    return result

                last_result = result
                self.error_streak += 1

                # Check circuit breaker
                if self.error_streak >= self.config.error_streak_threshold:
                    break

        # All levels exhausted - escalate to human
        return await self._escalate_to_human(goal, last_result)

    async def _generate_simple_alternative(
        self,
        goal: DailyGoal,
        failure: GoalExecutionResult,
        context: list[Episode],
    ) -> Optional[Callable]:
        """Generate ONE alternative approach based on the failure."""
        prompt = f"""
        Goal: {goal.content}
        Failed with error: {failure.error}

        Past similar episodes that worked:
        {[e.reflection for e in context if e.outcome.success][:2]}

        Suggest ONE alternative approach. Be specific and actionable.
        """
        # Simple LLM call, return modified action if viable
        # Return None if no good alternative found

    def _error_suggests_missing_info(self, error: Optional[str]) -> bool:
        """Check if error message suggests we need more information."""
        if not error:
            return False
        missing_info_patterns = ["not found", "missing", "undefined", "unknown"]
        return any(p in error.lower() for p in missing_info_patterns)

    async def _ask_simple_clarification(
        self,
        goal: DailyGoal,
        failure: GoalExecutionResult,
    ) -> Optional[str]:
        """Ask a simple clarifying question if needed."""
        # Only ask if error clearly indicates missing info
        # Return clarification text or None
```

### Implementation Tasks (5 issues)

| # | Task | Size | Dependencies |
|---|------|------|--------------|
| 8.1 | Create RecoveryManager class with level definitions | M | Phase 7 |
| 8.2 | Implement Level 1 (retry same with backoff) | S | 8.1 |
| 8.3 | Implement Level 2 (simple alternative generation) | M | 8.1 |
| 8.4 | Implement Level 3-4 (clarification and escalation) | M | 8.1 |
| 8.5 | Add circuit breakers and error streak tracking | S | 8.1-8.4 |

---

## 4. Phase 9: Episode Memory + Reflexion (P3)

### Consensus Decision

**Jerry Liu:** "JSONL episodes with simple retrieval. Don't add embeddings until you have thousands of episodes - heuristic filtering works fine at startup scale."

**Kanjun Qiu:** "Reflexion is key. After each goal, generate a verbal reflection. Store it. Retrieve relevant reflections before future actions. This is cheap and powerful."

**Harrison Chase:** "Make sure episodes are structured. Goal → Actions → Outcome → Reflection. Then you can query any dimension."

**Final Design (Simplified - No Embeddings):**

```python
@dataclass
class Episode:
    """A complete execution episode for learning."""
    episode_id: str
    timestamp: str
    goal: DailyGoal
    actions: list[Action]
    outcome: Outcome
    reflection: str           # LLM-generated insight
    recovery_level_used: int  # 1-4, for tracking recovery patterns
    cost_usd: float
    duration_seconds: int
    tags: list[str]           # For simple filtering: ["feature", "bug", "spec", etc.]

    def to_dict(self) -> dict:
        return {
            "episode_id": self.episode_id,
            "timestamp": self.timestamp,
            "goal": self.goal.to_dict(),
            "actions": [a.to_dict() for a in self.actions],
            "outcome": self.outcome.to_dict(),
            "reflection": self.reflection,
            "recovery_level_used": self.recovery_level_used,
            "cost_usd": self.cost_usd,
            "duration_seconds": self.duration_seconds,
            "tags": self.tags,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "Episode":
        return cls(
            episode_id=data["episode_id"],
            timestamp=data["timestamp"],
            goal=DailyGoal.from_dict(data["goal"]),
            actions=[Action.from_dict(a) for a in data["actions"]],
            outcome=Outcome.from_dict(data["outcome"]),
            reflection=data["reflection"],
            recovery_level_used=data["recovery_level_used"],
            cost_usd=data["cost_usd"],
            duration_seconds=data["duration_seconds"],
            tags=data.get("tags", []),
        )

@dataclass
class Action:
    """A single action within an episode."""
    action_type: str      # "orchestrator_call", "file_edit", "test_run", etc.
    description: str
    result: str
    cost_usd: float

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "Action":
        return cls(**data)

@dataclass
class Outcome:
    """Outcome of an episode."""
    success: bool
    goal_status: str
    error: Optional[str]
    artifacts_created: list[str]  # Files, specs, etc.

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "Outcome":
        return cls(**data)

class ReflexionEngine:
    """Generates and retrieves episode reflections for learning."""

    def __init__(self, episode_store: "EpisodeStore", llm: LLMClient):
        self.store = episode_store
        self.llm = llm

    async def reflect(self, episode: Episode) -> str:
        """Generate reflection on completed episode."""
        prompt = f"""
        Analyze this development episode:

        Goal: {episode.goal.content}
        Actions taken: {len(episode.actions)} actions
        Outcome: {"SUCCESS" if episode.outcome.success else "FAILURE"}
        {"Error: " + episode.outcome.error if episode.outcome.error else ""}
        Recovery level: {episode.recovery_level_used}
        Cost: ${episode.cost_usd:.2f}
        Duration: {episode.duration_seconds}s

        Generate a concise reflection (2-3 sentences) covering:
        1. What worked well or poorly?
        2. What would you do differently?
        3. Key insight for similar future goals?
        """

        reflection = await self.llm.complete(prompt, max_tokens=200)
        return reflection

    async def retrieve_relevant(
        self,
        goal: DailyGoal,
        k: int = 3,
        success_only: bool = False,
    ) -> list[Episode]:
        """Retrieve most relevant past episodes using simple heuristics."""

        # Load recent episodes (last 100)
        candidates = self.store.load_recent(limit=100)

        # Filter by success if requested
        if success_only:
            candidates = [e for e in candidates if e.outcome.success]

        # Score by relevance (simple heuristics, no embeddings)
        scored = []
        goal_tags = self._extract_tags(goal)

        for episode in candidates:
            score = 0.0

            # Tag overlap (most important)
            tag_overlap = len(set(episode.tags) & set(goal_tags))
            score += tag_overlap * 0.4

            # Recency boost (episodes from last 7 days score higher)
            age_days = self._days_ago(episode.timestamp)
            if age_days < 7:
                score += 0.3 * (1 - age_days / 7)

            # Success bonus
            if episode.outcome.success:
                score += 0.2

            # Low recovery level is good
            if episode.recovery_level_used == 1:
                score += 0.1

            scored.append((episode, score))

        # Sort by score descending
        scored.sort(key=lambda x: x[1], reverse=True)

        return [ep for ep, _ in scored[:k]]

    def _extract_tags(self, goal: DailyGoal) -> list[str]:
        """Extract tags from goal for matching."""
        tags = []
        if goal.linked_feature:
            tags.append("feature")
            tags.append(goal.linked_feature)
        if goal.linked_bug:
            tags.append("bug")
            tags.append(goal.linked_bug)
        if goal.linked_spec:
            tags.append("spec")
        return tags

    def _days_ago(self, timestamp: str) -> int:
        """Calculate days since timestamp."""
        dt = datetime.fromisoformat(timestamp)
        return (datetime.now() - dt).days

    async def record_episode(
        self,
        goal: DailyGoal,
        result: GoalExecutionResult,
        recovery_level: int,
        actions: list[Action],
    ) -> Episode:
        """Record episode and generate reflection."""

        outcome = Outcome(
            success=result.success,
            goal_status=goal.status.value if hasattr(goal.status, 'value') else str(goal.status),
            error=result.error,
            artifacts_created=[],
        )

        tags = self._extract_tags(goal)

        episode = Episode(
            episode_id=f"ep-{uuid.uuid4().hex[:8]}",
            timestamp=datetime.now().isoformat(),
            goal=goal,
            actions=actions,
            outcome=outcome,
            reflection="",
            recovery_level_used=recovery_level,
            cost_usd=result.cost_usd,
            duration_seconds=result.duration_seconds,
            tags=tags,
        )

        # Generate reflection
        episode.reflection = await self.reflect(episode)

        # Store
        await self.store.save(episode)

        return episode

class EpisodeStore:
    """Persistent storage for episodes using plain JSONL."""

    def __init__(self, base_path: Path):
        self.base_path = base_path / "episodes"
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.episodes_file = self.base_path / "episodes.jsonl"

    async def save(self, episode: Episode) -> None:
        """Append episode to JSONL file."""
        with open(self.episodes_file, "a") as f:
            f.write(json.dumps(episode.to_dict()) + "\n")

    def load_recent(self, limit: int = 100) -> list[Episode]:
        """Load most recent episodes from JSONL."""
        if not self.episodes_file.exists():
            return []

        episodes = []
        with open(self.episodes_file, "r") as f:
            for line in f:
                if line.strip():
                    episodes.append(Episode.from_dict(json.loads(line)))

        # Return most recent
        return episodes[-limit:]

    def load_all(self) -> list[Episode]:
        """Load all episodes."""
        return self.load_recent(limit=10000)
```

### Future Enhancement: Embeddings

When episode count exceeds ~500 and retrieval quality degrades, add embedding support:
- Generate embeddings for `goal.content + reflection`
- Store in separate `embeddings.npy` file
- Use cosine similarity for retrieval
- This is a drop-in enhancement to `retrieve_relevant()`

### Implementation Tasks (5 issues)

| # | Task | Size | Dependencies |
|---|------|------|--------------|
| 9.1 | Create Episode, Action, Outcome dataclasses with serialization | S | - |
| 9.2 | Implement EpisodeStore with JSONL persistence | M | 9.1 |
| 9.3 | Implement ReflexionEngine.reflect() | M | 9.1 |
| 9.4 | Implement ReflexionEngine.retrieve_relevant() with heuristic scoring | M | 9.2 |
| 9.5 | Integrate with RecoveryManager | M | Phase 8, 9.3, 9.4 |

---

## 5. Phase 10: Multi-Day Campaigns (P4)

### Consensus Decision

**Shunyu Yao:** "Backward planning from goal state. Define milestones, then decompose into daily goals. Re-plan only when >30% off track."

**Harrison Chase:** "Add state machine for campaign lifecycle. PLANNING → ACTIVE → PAUSED → COMPLETED/FAILED. Persist everything."

**Jerry Liu:** "Campaign context needs to persist across days. Store in a campaign.json with all state."

**David Dohan:** "Budget caps must be per-campaign AND per-day. Daily cap prevents single-day runaway."

**Final Design:**

```python
class CampaignState(Enum):
    PLANNING = "planning"
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class Milestone:
    """A major deliverable within a campaign."""
    milestone_id: str
    name: str
    description: str
    target_day: int           # Day 1, 2, 3, etc.
    success_criteria: list[str]
    status: str = "pending"   # pending, in_progress, done, blocked
    completed_at: Optional[str] = None

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "Milestone":
        return cls(**data)

@dataclass
class DayPlan:
    """Goals planned for a specific day of the campaign."""
    day_number: int
    date: str                 # YYYY-MM-DD
    goals: list[DailyGoal]
    budget_usd: float
    status: str = "pending"   # pending, in_progress, done
    actual_cost_usd: float = 0.0
    notes: str = ""

    def to_dict(self) -> dict:
        return {
            "day_number": self.day_number,
            "date": self.date,
            "goals": [g.to_dict() for g in self.goals],
            "budget_usd": self.budget_usd,
            "status": self.status,
            "actual_cost_usd": self.actual_cost_usd,
            "notes": self.notes,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "DayPlan":
        return cls(
            day_number=data["day_number"],
            date=data["date"],
            goals=[DailyGoal.from_dict(g) for g in data["goals"]],
            budget_usd=data["budget_usd"],
            status=data["status"],
            actual_cost_usd=data.get("actual_cost_usd", 0.0),
            notes=data.get("notes", ""),
        )

@dataclass
class Campaign:
    """Multi-day development campaign."""
    campaign_id: str
    name: str
    goal: str                 # High-level goal
    milestones: list[Milestone]
    day_plans: list[DayPlan]

    # Tracking
    state: CampaignState = CampaignState.PLANNING
    current_day: int = 1
    started_at: Optional[str] = None
    ended_at: Optional[str] = None

    # Budgets (David's requirement)
    total_budget_usd: float = 50.0
    daily_budget_usd: float = 15.0
    spent_usd: float = 0.0

    # Replanning
    original_duration_days: int = 0
    replanning_threshold: float = 0.3  # 30% behind triggers replan
    replan_count: int = 0

    def to_dict(self) -> dict:
        return {
            "campaign_id": self.campaign_id,
            "name": self.name,
            "goal": self.goal,
            "milestones": [m.to_dict() for m in self.milestones],
            "day_plans": [d.to_dict() for d in self.day_plans],
            "state": self.state.value,
            "current_day": self.current_day,
            "started_at": self.started_at,
            "ended_at": self.ended_at,
            "total_budget_usd": self.total_budget_usd,
            "daily_budget_usd": self.daily_budget_usd,
            "spent_usd": self.spent_usd,
            "original_duration_days": self.original_duration_days,
            "replanning_threshold": self.replanning_threshold,
            "replan_count": self.replan_count,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "Campaign":
        return cls(
            campaign_id=data["campaign_id"],
            name=data["name"],
            goal=data["goal"],
            milestones=[Milestone.from_dict(m) for m in data["milestones"]],
            day_plans=[DayPlan.from_dict(d) for d in data["day_plans"]],
            state=CampaignState(data["state"]),
            current_day=data["current_day"],
            started_at=data.get("started_at"),
            ended_at=data.get("ended_at"),
            total_budget_usd=data.get("total_budget_usd", 50.0),
            daily_budget_usd=data.get("daily_budget_usd", 15.0),
            spent_usd=data.get("spent_usd", 0.0),
            original_duration_days=data.get("original_duration_days", 0),
            replanning_threshold=data.get("replanning_threshold", 0.3),
            replan_count=data.get("replan_count", 0),
        )

    def days_behind(self) -> int:
        """Calculate how many days behind schedule."""
        if self.state != CampaignState.ACTIVE:
            return 0

        if self.original_duration_days == 0:
            return 0

        planned_progress = self.current_day / self.original_duration_days
        completed_milestones = len([m for m in self.milestones if m.status == "done"])
        actual_progress = completed_milestones / len(self.milestones) if self.milestones else 0

        if actual_progress < planned_progress:
            return int((planned_progress - actual_progress) * self.original_duration_days)
        return 0

    def needs_replan(self) -> bool:
        """Check if campaign is far enough behind to trigger replan."""
        if len(self.milestones) == 0 or self.original_duration_days == 0:
            return False

        progress_gap = self.days_behind() / self.original_duration_days
        return progress_gap > self.replanning_threshold

class CampaignPlanner:
    """Plans multi-day campaigns using backward planning."""

    def __init__(self, llm: LLMClient):
        self.llm = llm

    async def plan(
        self,
        goal: str,
        deadline_days: Optional[int] = None,
        budget_usd: Optional[float] = None,
    ) -> Campaign:
        """Create campaign plan using backward planning."""

        prompt = f"""
        Goal: {goal}
        {"Deadline: " + str(deadline_days) + " days" if deadline_days else "No fixed deadline"}
        {"Budget: $" + str(budget_usd) if budget_usd else "No fixed budget"}

        Using backward planning from the goal state:

        1. Define the END STATE (what does success look like?)
        2. Identify MILESTONES needed to reach end state
        3. Sequence milestones with dependencies
        4. Estimate days needed for each milestone
        5. Decompose each milestone into daily goals

        Return structured JSON with:
        - milestones: [{{name, description, target_day, success_criteria}}]
        - day_plans: [{{day_number, goals: [{{content, priority, estimated_minutes, linked_feature/bug/spec}}]}}]
        - total_estimated_days
        - total_estimated_cost_usd
        """

        plan_response = await self.llm.complete(prompt, max_tokens=2000)
        plan_data = json.loads(plan_response)

        campaign_id = f"camp-{uuid.uuid4().hex[:8]}"

        milestones = [
            Milestone(
                milestone_id=f"ms-{i+1}",
                name=m["name"],
                description=m["description"],
                target_day=m["target_day"],
                success_criteria=m.get("success_criteria", []),
            )
            for i, m in enumerate(plan_data["milestones"])
        ]

        day_plans = [
            DayPlan(
                day_number=d["day_number"],
                date="",  # Set when campaign starts
                goals=[DailyGoal.from_dict(g) for g in d["goals"]],
                budget_usd=budget_usd / len(plan_data["day_plans"]) if budget_usd else 15.0,
            )
            for d in plan_data["day_plans"]
        ]

        return Campaign(
            campaign_id=campaign_id,
            name=goal[:50],
            goal=goal,
            milestones=milestones,
            day_plans=day_plans,
            total_budget_usd=budget_usd or 50.0,
            daily_budget_usd=budget_usd / len(day_plans) if budget_usd else 15.0,
            original_duration_days=len(day_plans),
        )

    async def replan(self, campaign: Campaign) -> Campaign:
        """Replan remaining days of campaign."""

        completed = [m for m in campaign.milestones if m.status == "done"]
        remaining = [m for m in campaign.milestones if m.status != "done"]

        prompt = f"""
        Campaign: {campaign.name}
        Original goal: {campaign.goal}

        Completed milestones: {[m.name for m in completed]}
        Remaining milestones: {[m.name for m in remaining]}

        Days elapsed: {campaign.current_day}
        Budget spent: ${campaign.spent_usd:.2f}
        Budget remaining: ${campaign.total_budget_usd - campaign.spent_usd:.2f}

        Create revised plan for remaining milestones.
        Adjust estimates based on actual pace so far.

        Return JSON with updated day_plans for remaining work.
        """

        revised = await self.llm.complete(prompt, max_tokens=1500)
        revised_data = json.loads(revised)

        # Update remaining day plans
        new_day_plans = campaign.day_plans[:campaign.current_day]
        for d in revised_data["day_plans"]:
            new_day_plans.append(DayPlan(
                day_number=len(new_day_plans) + 1,
                date="",
                goals=[DailyGoal.from_dict(g) for g in d["goals"]],
                budget_usd=campaign.daily_budget_usd,
            ))

        campaign.day_plans = new_day_plans
        campaign.replan_count += 1

        return campaign

class CampaignExecutor:
    """Executes campaign day by day."""

    def __init__(
        self,
        autopilot: AutopilotRunner,
        planner: CampaignPlanner,
        store: "CampaignStore",
    ):
        self.autopilot = autopilot
        self.planner = planner
        self.store = store

    async def execute_day(self, campaign: Campaign) -> "DayResult":
        """Execute one day of the campaign."""

        if campaign.state != CampaignState.ACTIVE:
            raise ValueError(f"Campaign not active: {campaign.state}")

        if campaign.current_day > len(campaign.day_plans):
            raise ValueError("No more days planned")

        day_plan = campaign.day_plans[campaign.current_day - 1]

        # Run autopilot for today's goals
        result = await self.autopilot.start(
            goals=day_plan.goals,
            budget_usd=min(campaign.daily_budget_usd,
                          campaign.total_budget_usd - campaign.spent_usd),
        )

        # Update campaign state
        day_plan.actual_cost_usd = result.total_cost_usd
        day_plan.status = "done"
        campaign.spent_usd += result.total_cost_usd

        # Check if replan needed
        if campaign.needs_replan():
            campaign = await self.planner.replan(campaign)

        # Advance to next day
        campaign.current_day += 1

        # Check completion
        if campaign.current_day > len(campaign.day_plans):
            all_done = all(m.status == "done" for m in campaign.milestones)
            campaign.state = CampaignState.COMPLETED if all_done else CampaignState.FAILED
            campaign.ended_at = datetime.now().isoformat()

        # Persist
        await self.store.save(campaign)

        return DayResult(
            campaign_id=campaign.campaign_id,
            day=campaign.current_day - 1,
            goals_completed=result.goals_completed,
            cost_usd=result.total_cost_usd,
            campaign_progress=self._calculate_progress(campaign),
        )

    def _calculate_progress(self, campaign: Campaign) -> float:
        """Calculate overall campaign progress 0-1."""
        if not campaign.milestones:
            return 0.0
        done = len([m for m in campaign.milestones if m.status == "done"])
        return done / len(campaign.milestones)

    async def resume(self, campaign_id: str) -> Campaign:
        """Resume a paused or next-day campaign."""
        campaign = await self.store.load(campaign_id)
        if not campaign:
            raise ValueError(f"Campaign not found: {campaign_id}")

        if campaign.state == CampaignState.PAUSED:
            campaign.state = CampaignState.ACTIVE

        return campaign

@dataclass
class DayResult:
    """Result of executing one day of a campaign."""
    campaign_id: str
    day: int
    goals_completed: int
    cost_usd: float
    campaign_progress: float

class CampaignStore:
    """Persistent storage for campaigns."""

    def __init__(self, base_path: Path):
        self.base_path = base_path / "campaigns"
        self.base_path.mkdir(parents=True, exist_ok=True)

    async def save(self, campaign: Campaign) -> None:
        """Save campaign to JSON file."""
        path = self.base_path / f"{campaign.campaign_id}.json"
        with open(path, "w") as f:
            json.dump(campaign.to_dict(), f, indent=2)

    async def load(self, campaign_id: str) -> Optional[Campaign]:
        """Load campaign from JSON file."""
        path = self.base_path / f"{campaign_id}.json"
        if not path.exists():
            return None
        with open(path, "r") as f:
            return Campaign.from_dict(json.load(f))

    async def list_all(self) -> list[Campaign]:
        """List all campaigns."""
        campaigns = []
        for path in self.base_path.glob("*.json"):
            with open(path, "r") as f:
                campaigns.append(Campaign.from_dict(json.load(f)))
        return campaigns
```

### CLI Extensions

```bash
# Campaign management
swarm-attack cos campaign create "Implement auth system" --days 5 --budget 50
swarm-attack cos campaign status [CAMPAIGN_ID]
swarm-attack cos campaign list
swarm-attack cos campaign run [CAMPAIGN_ID]      # Execute today's goals
swarm-attack cos campaign resume [CAMPAIGN_ID]   # Resume paused campaign
swarm-attack cos campaign pause [CAMPAIGN_ID]
swarm-attack cos campaign replan [CAMPAIGN_ID]   # Force replan
```

### Implementation Tasks (7 issues)

| # | Task | Size | Dependencies |
|---|------|------|--------------|
| 10.1 | Create Campaign, Milestone, DayPlan dataclasses | M | - |
| 10.2 | Implement CampaignStore persistence | M | 10.1 |
| 10.3 | Implement CampaignPlanner.plan() | L | 10.1 |
| 10.4 | Implement CampaignPlanner.replan() | M | 10.3 |
| 10.5 | Implement CampaignExecutor.execute_day() | L | 10.1, Phase 7 |
| 10.6 | Add campaign CLI commands | M | 10.2, 10.5 |
| 10.7 | Add campaign progress to standup | S | 10.2 |

---

## 6. Phase 11: Internal Validation Critics (P5)

### Consensus Decision

**Kanjun Qiu:** "Use diverse critics - one for each quality dimension. They should disagree with each other to surface real issues."

**Harrison Chase:** "Majority voting for approval, but ANY security critic veto blocks. Safety is not a democracy."

**David Dohan:** "Track critic accuracy over time. If a critic is consistently wrong, reduce its weight. But never zero - preserve voice."

**Final Design:**

```python
class CriticFocus(Enum):
    COMPLETENESS = "completeness"
    FEASIBILITY = "feasibility"
    SECURITY = "security"         # Veto power
    STYLE = "style"
    COVERAGE = "coverage"
    EDGE_CASES = "edge_cases"

@dataclass
class CriticScore:
    critic_name: str
    focus: CriticFocus
    score: float              # 0-1
    approved: bool
    issues: list[str]
    suggestions: list[str]
    reasoning: str

    def to_dict(self) -> dict:
        return {
            "critic_name": self.critic_name,
            "focus": self.focus.value,
            "score": self.score,
            "approved": self.approved,
            "issues": self.issues,
            "suggestions": self.suggestions,
            "reasoning": self.reasoning,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "CriticScore":
        return cls(
            critic_name=data["critic_name"],
            focus=CriticFocus(data["focus"]),
            score=data["score"],
            approved=data["approved"],
            issues=data["issues"],
            suggestions=data["suggestions"],
            reasoning=data["reasoning"],
        )

@dataclass
class ValidationResult:
    artifact_type: str        # "spec", "code", "test"
    artifact_id: str
    approved: bool
    scores: list[CriticScore]
    blocking_issues: list[str]
    consensus_summary: str
    human_review_required: bool

    def to_dict(self) -> dict:
        return {
            "artifact_type": self.artifact_type,
            "artifact_id": self.artifact_id,
            "approved": self.approved,
            "scores": [s.to_dict() for s in self.scores],
            "blocking_issues": self.blocking_issues,
            "consensus_summary": self.consensus_summary,
            "human_review_required": self.human_review_required,
        }

class Critic:
    """Base class for validation critics."""

    def __init__(self, focus: CriticFocus, llm: LLMClient, weight: float = 1.0):
        self.focus = focus
        self.llm = llm
        self.weight = weight
        self.has_veto = focus == CriticFocus.SECURITY

    async def evaluate(self, artifact: str) -> CriticScore:
        raise NotImplementedError

class SpecCritic(Critic):
    """Evaluates engineering specs."""

    async def evaluate(self, spec_content: str) -> CriticScore:
        focus_prompts = {
            CriticFocus.COMPLETENESS: "How complete is the spec? Missing sections? Gaps in requirements?",
            CriticFocus.FEASIBILITY: "Can this be implemented as written? Unclear requirements? Impossible constraints?",
            CriticFocus.SECURITY: "Security issues? Injection risks? Auth gaps? Data exposure?",
        }

        prompt = f"""
        Evaluate this engineering spec for {self.focus.value}:

        {spec_content[:4000]}

        {focus_prompts.get(self.focus, "")}

        Rate 0-1 and identify specific issues.
        Return JSON: {{"score": 0.0-1.0, "approved": true/false, "issues": [], "suggestions": [], "reasoning": ""}}
        """

        response = await self.llm.complete(prompt)
        data = json.loads(response)

        return CriticScore(
            critic_name=f"SpecCritic-{self.focus.value}",
            focus=self.focus,
            score=data["score"],
            approved=data["approved"],
            issues=data["issues"],
            suggestions=data["suggestions"],
            reasoning=data["reasoning"],
        )

class CodeCritic(Critic):
    """Evaluates code changes."""

    async def evaluate(self, code_diff: str) -> CriticScore:
        focus_prompts = {
            CriticFocus.STYLE: "Code style issues? Naming? Structure? Readability?",
            CriticFocus.SECURITY: "Security vulnerabilities? Injection? Unsafe operations? Secrets exposed?",
        }

        prompt = f"""
        Evaluate this code change for {self.focus.value}:

        {code_diff[:4000]}

        {focus_prompts.get(self.focus, "")}

        Rate 0-1 and identify specific issues.
        Return JSON: {{"score": 0.0-1.0, "approved": true/false, "issues": [], "suggestions": [], "reasoning": ""}}
        """

        response = await self.llm.complete(prompt)
        data = json.loads(response)

        return CriticScore(
            critic_name=f"CodeCritic-{self.focus.value}",
            focus=self.focus,
            score=data["score"],
            approved=data["approved"],
            issues=data["issues"],
            suggestions=data["suggestions"],
            reasoning=data["reasoning"],
        )

class TestCritic(Critic):
    """Evaluates test coverage and quality."""

    async def evaluate(self, test_content: str) -> CriticScore:
        focus_prompts = {
            CriticFocus.COVERAGE: "Test coverage adequate? Missing scenarios? Key paths untested?",
            CriticFocus.EDGE_CASES: "Edge cases covered? Boundary conditions? Error scenarios?",
        }

        prompt = f"""
        Evaluate these tests for {self.focus.value}:

        {test_content[:4000]}

        {focus_prompts.get(self.focus, "")}

        Rate 0-1 and identify specific issues.
        Return JSON: {{"score": 0.0-1.0, "approved": true/false, "issues": [], "suggestions": [], "reasoning": ""}}
        """

        response = await self.llm.complete(prompt)
        data = json.loads(response)

        return CriticScore(
            critic_name=f"TestCritic-{self.focus.value}",
            focus=self.focus,
            score=data["score"],
            approved=data["approved"],
            issues=data["issues"],
            suggestions=data["suggestions"],
            reasoning=data["reasoning"],
        )

class ValidationLayer:
    """Orchestrates multiple critics for consensus building."""

    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.critics = {
            "spec": [
                SpecCritic(CriticFocus.COMPLETENESS, llm),
                SpecCritic(CriticFocus.FEASIBILITY, llm),
                SpecCritic(CriticFocus.SECURITY, llm),
            ],
            "code": [
                CodeCritic(CriticFocus.STYLE, llm),
                CodeCritic(CriticFocus.SECURITY, llm),
            ],
            "test": [
                TestCritic(CriticFocus.COVERAGE, llm),
                TestCritic(CriticFocus.EDGE_CASES, llm),
            ],
        }

    async def validate(
        self,
        artifact: str,
        artifact_type: str,
        artifact_id: str = "",
    ) -> ValidationResult:
        """Run all relevant critics and build consensus."""

        relevant_critics = self.critics.get(artifact_type, [])
        if not relevant_critics:
            return ValidationResult(
                artifact_type=artifact_type,
                artifact_id=artifact_id,
                approved=True,
                scores=[],
                blocking_issues=[],
                consensus_summary="No critics configured for this artifact type",
                human_review_required=False,
            )

        # Run critics in parallel
        scores = await asyncio.gather(*[
            c.evaluate(artifact) for c in relevant_critics
        ])

        # Check for security veto
        security_scores = [s for s in scores if s.focus == CriticFocus.SECURITY]
        if any(not s.approved for s in security_scores):
            return ValidationResult(
                artifact_type=artifact_type,
                artifact_id=artifact_id,
                approved=False,
                scores=list(scores),
                blocking_issues=[
                    issue for s in security_scores if not s.approved for issue in s.issues
                ],
                consensus_summary="BLOCKED: Security concerns require human review",
                human_review_required=True,
            )

        # Majority vote (weighted)
        total_weight = sum(c.weight for c in relevant_critics)
        approval_weight = sum(
            c.weight for c, s in zip(relevant_critics, scores) if s.approved
        )

        approved = (approval_weight / total_weight) >= 0.6  # 60% threshold

        return ValidationResult(
            artifact_type=artifact_type,
            artifact_id=artifact_id,
            approved=approved,
            scores=list(scores),
            blocking_issues=[
                issue for s in scores if not s.approved for issue in s.issues
            ],
            consensus_summary=self._build_summary(list(scores), approved),
            human_review_required=not approved,
        )

    def _build_summary(self, scores: list[CriticScore], approved: bool) -> str:
        avg_score = sum(s.score for s in scores) / len(scores) if scores else 0

        if approved:
            return f"APPROVED by consensus (avg score: {avg_score:.2f})"
        else:
            concerns = [s.focus.value for s in scores if not s.approved]
            return f"NEEDS REVIEW: concerns in {', '.join(concerns)} (avg: {avg_score:.2f})"
```

### Integration Flow (How Validation Gates Work)

The ValidationLayer integrates at three key points in the pipeline:

#### 1. Spec Pipeline Integration

```python
# In Orchestrator.run_spec_pipeline()
class Orchestrator:
    async def run_spec_pipeline(self, feature_id: str) -> SpecResult:
        spec = await self._generate_spec(feature_id)

        # GATE: Validate before human approval
        validation = await self.validation_layer.validate(
            artifact=spec.content,
            artifact_type="spec",
            artifact_id=feature_id,
        )

        if validation.approved:
            # Auto-advance to approval (human still sees it)
            return SpecResult(
                status="ready_for_approval",
                validation=validation,
                auto_approved=True,
            )
        else:
            # Block and surface issues
            return SpecResult(
                status="needs_revision",
                validation=validation,
                blocking_issues=validation.blocking_issues,
            )
```

#### 2. Code Execution Integration

```python
# In AutopilotRunner._execute_goal()
class AutopilotRunner:
    async def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:
        # ... existing execution logic ...

        if goal.linked_feature and result.success:
            # GATE: Validate code before marking complete
            code_diff = await self._get_code_diff(goal.linked_feature)
            validation = await self.validation_layer.validate(
                artifact=code_diff,
                artifact_type="code",
                artifact_id=f"{goal.linked_feature}-{goal.linked_issue}",
            )

            if not validation.approved:
                # Don't mark complete - needs fixes
                return GoalExecutionResult(
                    success=False,
                    error="Validation failed: " + validation.consensus_summary,
                    validation=validation,
                )

        return result
```

#### 3. CLI Validation Command

```bash
# Manual validation check
swarm-attack cos validate spec chief-of-staff-v2
swarm-attack cos validate code swarm_attack/chief_of_staff/
```

```python
# In CLI
@cos_group.command("validate")
@click.argument("artifact_type", type=click.Choice(["spec", "code", "test"]))
@click.argument("path")
def validate_artifact(artifact_type: str, path: str):
    """Run validation critics on an artifact."""
    content = Path(path).read_text()
    result = asyncio.run(validation_layer.validate(content, artifact_type))

    if result.approved:
        click.echo(f"✓ APPROVED: {result.consensus_summary}")
    else:
        click.echo(f"✗ NEEDS REVIEW: {result.consensus_summary}")
        for issue in result.blocking_issues:
            click.echo(f"  - {issue}")
```

#### Gating Rules Summary

| Artifact | Gate Location | Auto-Approve Threshold | Human Required |
|----------|---------------|----------------------|----------------|
| Spec | Before `SPEC_NEEDS_APPROVAL` | 60% critic approval | Security veto OR <60% |
| Code | Before goal marked complete | 60% critic approval | Security veto OR <60% |
| Test | Before Verifier runs | 60% critic approval | Coverage <80% |

### Implementation Tasks (6 issues)

| # | Task | Size | Dependencies |
|---|------|------|--------------|
| 11.1 | Create Critic base class and CriticScore | S | - |
| 11.2 | Implement SpecCritic variants | M | 11.1 |
| 11.3 | Implement CodeCritic variants | M | 11.1 |
| 11.4 | Implement TestCritic variants | M | 11.1 |
| 11.5 | Implement ValidationLayer consensus | M | 11.2-11.4 |
| 11.6 | Integrate validation gates into pipelines | M | 11.5, Phase 7 |

---

## 7. Success Metrics

| Metric | v1 Baseline | v2 Target | Measurement |
|--------|-------------|-----------|-------------|
| Real execution rate | 0% (stub) | 100% | Goals that call orchestrators |
| Automatic recovery | 0% | 70% | Failures resolved without human |
| Human review reduction | 100% manual | <30% | Artifacts bypassing review |
| Multi-day completion | N/A | >75% | Campaigns finished as planned |
| Learning improvement | None | +10%/month | Goal completion rate trend |
| Cost efficiency | Unmeasured | Track | $/completed goal trend |

---

## 8. Risk Mitigations

| Risk | Mitigation | Owner |
|------|------------|-------|
| Runaway execution | Per-day + per-campaign budget caps, mandatory checkpoints | David's design |
| Bad learning loops | Bounded weight changes (±20%/week), rollback on degradation | Kanjun's design |
| Infinite recovery | 4-level cap with forced escalation, circuit breakers | Shunyu's design |
| Context bloat | Episode pruning (keep last 100), recency decay | Jerry's design |

---

## 9. Implementation Roadmap

### Phase 7: Real Execution (MVP - Do First)
**5 issues**

This is the foundation. Without real execution, v2 is just a more elaborate stub.

### Phase 8: Hierarchical Recovery (Core Autonomy)
**5 issues** (simplified from 6)

Enables overnight runs. Combined with Phase 7, you can say "work on this" and trust it to make progress without babysitting.

### Phase 9: Episode Memory + Reflexion (Learning)
**5 issues** (simplified from 6 - no embeddings)

Cheap investment, high payoff. Every execution makes future executions better.

### Phase 10: Multi-Day Campaigns (Strategic)
**7 issues**

Graduate from "daily tasks" to "build this feature over the week."

### Phase 11: Internal Validation (Scale)
**6 issues**

Reduces human bottleneck. Most specs/code auto-approve; you only see the hard cases.

**Total: 28 issues across 5 phases**

---

## 10. Expert Panel Final Statement

**Harrison Chase:** "The roadmap is pragmatic. Phase 7 → 8 → 9 builds a solid autonomous agent. Phases 10-11 add strategic capability. Ship Phase 7-9 first, validate, then proceed."

**Jerry Liu:** "Episode memory with simple retrieval is the right call for startup scale. Add embeddings when you have thousands of episodes, not before."

**Kanjun Qiu:** "Bounded learning is critical. The 4-level recovery and weight caps prevent runaway self-modification. Good safety design."

**David Dohan:** "I'm satisfied with the checkpoint system and escalation paths. The security veto in validation is essential. Don't compromise on that."

**Shunyu Yao:** "Simplified recovery is the right call. One alternative on failure, not three. Keep it lean until you have data showing you need more."

---

*Spec finalized: December 2025*
*Expert Panel: LangChain, LlamaIndex, Imbue, Anthropic, Princeton*
*Ready for implementation*
<<<SPEC_END>>>

<<<RUBRIC_START>>>
{
  "round": 2,
  "previous_scores": {"clarity": 0.65, "coverage": 0.45, "architecture": 0.4, "risk": 0.55},
  "current_scores": {"clarity": 0.85, "coverage": 0.75, "architecture": 0.80, "risk": 0.75},
  "issues_accepted": 1,
  "issues_rejected": 2,
  "issues_deferred": 0,
  "issues_partial": 1,
  "continue_debate": false,
  "ready_for_approval": true,
  "meta": {
    "recommend_human_review": true,
    "review_reason": "Critic has raised parallel execution (R2-1) and weekly planning (R2-2) for the second consecutive round despite clear PRD prioritization (P6=42, P9=42 vs P1-P5 all >48). These are ranked near-bottom by the expert panel. Recommend human arbitration on whether these low-priority items should block v2 approval."
  },
  "dispute_resolutions": []
}
<<<RUBRIC_END>>>