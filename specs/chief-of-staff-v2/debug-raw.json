{
  "type": "result",
  "subtype": "success",
  "is_error": false,
  "duration_ms": 346420,
  "duration_api_ms": 379058,
  "num_turns": 1,
  "result": "<<<DISPOSITIONS_START>>>\n[\n  {\n    \"issue_id\": \"R2-1\",\n    \"original_issue\": \"OVER-ENGINEERING: The spec explicitly defers parallel execution (user story #3) to v3, so v2 cannot coordinate multiple workstreams despite the PRD calling this out as part of the v2 vision.\",\n    \"classification\": \"REJECT\",\n    \"reasoning\": \"Previously addressed in R1-4. The expert panel ranked parallel execution P6 (below our P1-P5 scope), and Jerry Liu explicitly warned 'Be careful with parallelism in codebases.' We have ONE developer using this tool. Sequential campaigns cover 95% of use cases. Adding branch coordination, conflict detection, and parallel scheduling is significant complexity for startup scale. The critic continues to push enterprise patterns against expert panel guidance. We'll add it in v3 when we have real demand.\",\n    \"action_taken\": \"none\",\n    \"resolved\": false,\n    \"semantic_key\": \"execution_parallel_coordination\",\n    \"repeat_of\": \"R1-4\",\n    \"consecutive_rejections\": 2\n  },\n  {\n    \"issue_id\": \"R2-2\",\n    \"original_issue\": \"OVER-ENGINEERING: `_detect_triggers` never emits the P0 `HICCUP` trigger, so unexpected errors will never pause for approval even though the PRD makes hiccup checkpoints mandatory.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"Valid gap. The PRD explicitly requires hiccup checkpoints for 'any unexpected situation.' The current _detect_triggers only checks tags and cost, not error states. This is not over-engineering\u2014it's completing the P0 checkpoint requirement we already committed to. Simple fix: add hiccup detection based on error counters and recovery level.\",\n    \"action_taken\": \"Added hiccup detection to _detect_triggers based on goal.error_count and goal.recovery_level fields. Also added is_hiccup flag to DailyGoal for explicit hiccup marking.\",\n    \"resolved\": true,\n    \"semantic_key\": \"checkpoint_hiccup_trigger\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  },\n  {\n    \"issue_id\": \"R2-3\",\n    \"original_issue\": \"OVER-ENGINEERING: The cumulative cost trigger depends on `update_daily_cost`, but the spec never describes calling it from AutopilotRunner/CampaignExecutor, so the >$15/day checkpoint cannot fire.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"Valid gap. We defined update_daily_cost() but never showed where it's called. This is basic wiring, not over-engineering. Without this, the cumulative cost checkpoint (a P0 requirement) is dead code.\",\n    \"action_taken\": \"Added explicit call sites for update_daily_cost() in AutopilotRunner._execute_goal() after each execution, and reset_daily_cost() in AutopilotRunner.start() at session start.\",\n    \"resolved\": true,\n    \"semantic_key\": \"checkpoint_cost_cumulative_wiring\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  },\n  {\n    \"issue_id\": \"R2-4\",\n    \"original_issue\": \"OVER-ENGINEERING: `_record_decision` is left as a stub and no flow connects checkpoint resolutions to the `PreferenceLearner`, so approvals/modifications never feed learning as the PRD requires.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"Valid gap. We already accepted R1-2 (decision learning) and added PreferenceLearner, but the wiring from checkpoint resolution to preference recording is incomplete. This is completing work we already committed to, not adding scope.\",\n    \"action_taken\": \"Implemented _record_decision to call preference_learner.record_decision() with the checkpoint and most recent episode. Added preference_learner dependency to CheckpointSystem constructor.\",\n    \"resolved\": true,\n    \"semantic_key\": \"checkpoint_preference_learning_wiring\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  }\n]\n<<<DISPOSITIONS_END>>>\n\n<<<SPEC_START>>>\n# Chief of Staff v2: Autonomous Development Partner\n\n## Expert Panel Consensus Document\n\n**Panel Members:**\n- Harrison Chase (LangChain) - Agent frameworks, state machines\n- Jerry Liu (LlamaIndex) - Data agents, retrieval systems\n- Kanjun Qiu (Imbue) - Reasoning agents, self-improvement\n- David Dohan (Anthropic) - Tool safety, bounded autonomy\n- Shunyu Yao (Princeton) - ReAct, Tree-of-Thought planning\n\n**Date:** December 2025\n**Status:** FINAL - Ready for Implementation\n\n---\n\n## 1. Executive Summary\n\n### What v1 Delivered\nChief of Staff v1 is **complete** (20/20 issues, 279 tests, 6 CLI commands). It provides:\n- Cross-session memory via daily logs and JSONL decisions\n- Daily standups with intelligent recommendations\n- Goal tracking with automatic state reconciliation\n- Autopilot mode with checkpoint gates (stub execution)\n\n### What v2 Adds\nv2 transforms Chief of Staff from a **workflow tool** into a **truly autonomous development partner**:\n\n| Capability | v1 State | v2 Target |\n|------------|----------|-----------|\n| Execution | Stub (marks goals complete without work) | **Real orchestrator integration** |\n| Recovery | Manual intervention required | **4-level automatic retry** |\n| Memory | JSONL decisions only | **Episode memory + Reflexion + Preference Learning** |\n| Planning | Single-day horizon | **Multi-day campaigns + Weekly summaries** |\n| Validation | Human reviews everything | **Internal critics pre-filter** |\n| Human Signoff | None | **Collaborative checkpoint system (P0)** |\n\n### Expert Panel Priority Ranking (Final)\n\n| Rank | Extension | Impact \u00d7 Feasibility | Rationale |\n|------|-----------|---------------------|-----------|\n| **P0** | Human-in-the-Loop Checkpoints | REQUIRED | PRD mandates collaborative autonomy |\n| **P1** | Real Execution | 10 \u00d7 9 = 90 | Foundation for everything - without this, nothing works |\n| **P2** | Hierarchical Recovery | 9 \u00d7 8 = 72 | Essential for overnight autonomy |\n| **P3** | Episode Memory + Reflexion + Preferences | 9 \u00d7 7 = 63 | Low-cost learning with high ROI |\n| **P4** | Multi-Day Campaigns + Weekly Planning | 8 \u00d7 7 = 56 | Enables complex features without restarts |\n| **P5** | Internal Validation Critics | 8 \u00d7 6 = 48 | Reduces human review burden by ~70% |\n\n### Scope Boundaries (What v2 Does NOT Include)\n\nThe following PRD items are explicitly **deferred to v3** per expert panel prioritization:\n\n| Deferred Item | PRD Priority | Rationale |\n|---------------|--------------|-----------|\n| Parallel Execution (P6) | 7\u00d76=42 | Jerry Liu warned against premature parallelism; sequential campaigns cover 95% of use cases for a single developer |\n| Semantic Memory/Embeddings (P8) | 6\u00d75=30 | Nice-to-have infrastructure; simple JSONL retrieval is sufficient for v2 |\n| Prompt Self-Optimization (P10) | 5\u00d74=20 | Risky self-modification; needs careful bounds designed in v3 |\n\n---\n\n## 2. Phase 7: Real Execution (P1)\n\n### Consensus Decision\n\n**Harrison Chase:** \"This is table stakes. The current stub defeats the entire purpose. Just wire it up.\"\n\n**David Dohan:** \"Agreed, but add defensive wrappers. Never let a goal execute without budget/time checks BEFORE the call, not just after.\"\n\n### 2.1 Core Execution Design\n\n```python\nclass AutopilotRunner:\n    def __init__(\n        self,\n        orchestrator: Orchestrator,\n        bug_orchestrator: BugOrchestrator,\n        checkpoint_system: CheckpointSystem,\n        config: ChiefOfStaffConfig,\n    ):\n        self.orchestrator = orchestrator\n        self.bug_orchestrator = bug_orchestrator\n        self.checkpoint_system = checkpoint_system\n        self.config = config\n        self.session: Optional[AutopilotSession] = None\n\n    async def start(\n        self,\n        goals: list[DailyGoal],\n        budget_usd: float,\n    ) -> AutopilotResult:\n        \"\"\"Start autopilot session with given goals.\"\"\"\n        # Reset daily cost tracking at session start\n        self.checkpoint_system.reset_daily_cost()\n\n        self.session = AutopilotSession(\n            session_id=f\"session-{uuid.uuid4().hex[:8]}\",\n            goals=goals,\n            budget_usd=budget_usd,\n            cost_spent_usd=0.0,\n            started_at=datetime.now().isoformat(),\n        )\n\n        results = []\n        for goal in goals:\n            result = await self._execute_goal(goal)\n            results.append(result)\n\n            # Update session cost\n            self.session.cost_spent_usd += result.cost_usd\n\n            # Update checkpoint system's daily cost tracking\n            self.checkpoint_system.update_daily_cost(result.cost_usd)\n\n            if result.checkpoint_pending:\n                # Pause for human approval\n                break\n\n        return AutopilotResult(\n            session_id=self.session.session_id,\n            goals_completed=len([r for r in results if r.success]),\n            total_cost_usd=self.session.cost_spent_usd,\n            results=results,\n        )\n\n    async def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:\n        \"\"\"Execute a goal by calling the appropriate orchestrator.\"\"\"\n\n        # Pre-execution safety check (David's requirement)\n        remaining_budget = self.session.budget_usd - self.session.cost_spent_usd\n        if remaining_budget < self.config.min_execution_budget:\n            return GoalExecutionResult(\n                success=False,\n                error=\"Insufficient budget remaining\",\n                cost_usd=0,\n            )\n\n        # P0: Human checkpoint before expensive/risky operations\n        checkpoint_result = await self.checkpoint_system.check_before_execution(goal)\n        if checkpoint_result.requires_approval and not checkpoint_result.approved:\n            return GoalExecutionResult(\n                success=False,\n                error=\"Awaiting human approval\",\n                cost_usd=0,\n                checkpoint_pending=True,\n            )\n\n        try:\n            if goal.linked_feature:\n                result = self.orchestrator.run_issue(\n                    feature_id=goal.linked_feature,\n                    issue_number=goal.linked_issue,\n                )\n                return GoalExecutionResult(\n                    success=result.status == \"success\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                    output=result.summary,\n                )\n\n            elif goal.linked_bug:\n                result = self.bug_orchestrator.fix(goal.linked_bug)\n                return GoalExecutionResult(\n                    success=result.status == \"fixed\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                    output=result.summary,\n                )\n\n            elif goal.linked_spec:\n                result = self.orchestrator.run_spec_pipeline(goal.linked_spec)\n                return GoalExecutionResult(\n                    success=result.status == \"approved\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                )\n\n            else:\n                # Generic goal without linked artifact\n                # Log as manual task, mark for human follow-up\n                return GoalExecutionResult(\n                    success=True,\n                    cost_usd=0,\n                    output=\"Manual goal - no automated execution\",\n                )\n\n        except Exception as e:\n            # Mark goal as having an error for hiccup detection\n            goal.error_count = getattr(goal, 'error_count', 0) + 1\n            return GoalExecutionResult(\n                success=False,\n                error=str(e),\n                cost_usd=0,\n            )\n```\n\n### 2.5 Human-in-the-Loop Checkpoint System (P0)\n\n**This is a P0 requirement from the PRD.** The system operates in **collaborative autonomy** mode\u2014like a senior engineer who knows to check in before making big calls.\n\n#### Checkpoint Trigger Thresholds\n\n| Trigger | Threshold | Detection Method |\n|---------|-----------|------------------|\n| **UX/Flow Changes** | Any user-facing change | Goal tags contain \"ui\", \"ux\", \"frontend\", or linked issue has UI label |\n| **Cost - Single Action** | >$5 per action | Pre-execution cost estimate from goal metadata |\n| **Cost - Cumulative** | >$15/day | Session.cost_spent_usd tracking via update_daily_cost() |\n| **Architecture Decisions** | Any structural change | Goal tags contain \"architecture\", \"refactor\", or new file creation in core paths |\n| **Scope Changes** | Deviation from approved plan | Goal not in original day plan, or goal modified from plan |\n| **Hiccups** | Any unexpected situation | error_count > 0, recovery_level >= 2, is_hiccup flag, or unknown state |\n\n#### Checkpoint Data Model\n\n```python\nclass CheckpointTrigger(Enum):\n    UX_CHANGE = \"ux_change\"\n    COST_SINGLE = \"cost_single\"\n    COST_CUMULATIVE = \"cost_cumulative\"\n    ARCHITECTURE = \"architecture\"\n    SCOPE_CHANGE = \"scope_change\"\n    HICCUP = \"hiccup\"\n\n@dataclass\nclass CheckpointOption:\n    \"\"\"A choice presented to the human.\"\"\"\n    label: str\n    description: str\n    is_recommended: bool = False\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"CheckpointOption\":\n        return cls(**data)\n\n@dataclass\nclass Checkpoint:\n    \"\"\"A decision point requiring human approval.\"\"\"\n    checkpoint_id: str\n    trigger: CheckpointTrigger\n    context: str                    # What's happening and why we're asking\n    options: list[CheckpointOption]\n    recommendation: Optional[str]   # Which option we recommend and why\n    created_at: str\n    goal_id: Optional[str]          # Linked goal if applicable\n    campaign_id: Optional[str]      # Linked campaign if applicable\n\n    # Resolution\n    status: str = \"pending\"         # pending, approved, rejected, expired\n    chosen_option: Optional[str] = None\n    human_notes: Optional[str] = None\n    resolved_at: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        return {\n            \"checkpoint_id\": self.checkpoint_id,\n            \"trigger\": self.trigger.value,\n            \"context\": self.context,\n            \"options\": [o.to_dict() for o in self.options],\n            \"recommendation\": self.recommendation,\n            \"created_at\": self.created_at,\n            \"goal_id\": self.goal_id,\n            \"campaign_id\": self.campaign_id,\n            \"status\": self.status,\n            \"chosen_option\": self.chosen_option,\n            \"human_notes\": self.human_notes,\n            \"resolved_at\": self.resolved_at,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Checkpoint\":\n        return cls(\n            checkpoint_id=data[\"checkpoint_id\"],\n            trigger=CheckpointTrigger(data[\"trigger\"]),\n            context=data[\"context\"],\n            options=[CheckpointOption.from_dict(o) for o in data[\"options\"]],\n            recommendation=data.get(\"recommendation\"),\n            created_at=data[\"created_at\"],\n            goal_id=data.get(\"goal_id\"),\n            campaign_id=data.get(\"campaign_id\"),\n            status=data.get(\"status\", \"pending\"),\n            chosen_option=data.get(\"chosen_option\"),\n            human_notes=data.get(\"human_notes\"),\n            resolved_at=data.get(\"resolved_at\"),\n        )\n\n@dataclass\nclass CheckpointResult:\n    \"\"\"Result of a checkpoint check.\"\"\"\n    requires_approval: bool\n    approved: bool\n    checkpoint: Optional[Checkpoint]\n\nclass CheckpointSystem:\n    \"\"\"Manages human-in-the-loop checkpoints.\"\"\"\n\n    def __init__(\n        self,\n        store: \"CheckpointStore\",\n        config: ChiefOfStaffConfig,\n        preference_learner: \"PreferenceLearner\",\n        episode_store: \"EpisodeStore\",\n    ):\n        self.store = store\n        self.config = config\n        self.preference_learner = preference_learner\n        self.episode_store = episode_store\n        self.daily_cost = 0.0\n\n    async def check_before_execution(self, goal: DailyGoal) -> CheckpointResult:\n        \"\"\"Check if goal requires human approval before execution.\"\"\"\n\n        triggers = self._detect_triggers(goal)\n\n        if not triggers:\n            return CheckpointResult(requires_approval=False, approved=True, checkpoint=None)\n\n        # Check for existing pending checkpoint\n        existing = await self.store.get_pending_for_goal(goal.goal_id)\n        if existing:\n            return CheckpointResult(\n                requires_approval=True,\n                approved=existing.status == \"approved\",\n                checkpoint=existing,\n            )\n\n        # Create new checkpoint\n        checkpoint = await self._create_checkpoint(goal, triggers[0])\n        await self.store.save(checkpoint)\n\n        return CheckpointResult(\n            requires_approval=True,\n            approved=False,\n            checkpoint=checkpoint,\n        )\n\n    def _detect_triggers(self, goal: DailyGoal) -> list[CheckpointTrigger]:\n        \"\"\"Detect which triggers apply to this goal.\"\"\"\n        triggers = []\n\n        # HICCUP detection (P0 requirement) - check this first\n        # Hiccup triggers: error_count > 0, recovery_level >= 2, explicit is_hiccup flag\n        if getattr(goal, 'error_count', 0) > 0:\n            triggers.append(CheckpointTrigger.HICCUP)\n        if getattr(goal, 'recovery_level', 0) >= 2:\n            triggers.append(CheckpointTrigger.HICCUP)\n        if getattr(goal, 'is_hiccup', False):\n            triggers.append(CheckpointTrigger.HICCUP)\n\n        # UX/Flow changes\n        ui_tags = {\"ui\", \"ux\", \"frontend\", \"user-facing\", \"screen\", \"flow\"}\n        if goal.tags and ui_tags & set(t.lower() for t in goal.tags):\n            triggers.append(CheckpointTrigger.UX_CHANGE)\n\n        # Cost - single action\n        if goal.estimated_cost_usd and goal.estimated_cost_usd > self.config.checkpoint_cost_single:\n            triggers.append(CheckpointTrigger.COST_SINGLE)\n\n        # Cost - cumulative (uses daily_cost updated by AutopilotRunner)\n        if self.daily_cost > self.config.checkpoint_cost_daily:\n            triggers.append(CheckpointTrigger.COST_CUMULATIVE)\n\n        # Architecture\n        arch_tags = {\"architecture\", \"refactor\", \"core\", \"infrastructure\", \"breaking\"}\n        if goal.tags and arch_tags & set(t.lower() for t in goal.tags):\n            triggers.append(CheckpointTrigger.ARCHITECTURE)\n\n        # Scope change (goal not in original plan)\n        if getattr(goal, 'is_unplanned', False):\n            triggers.append(CheckpointTrigger.SCOPE_CHANGE)\n\n        return triggers\n\n    async def _create_checkpoint(\n        self,\n        goal: DailyGoal,\n        trigger: CheckpointTrigger,\n    ) -> Checkpoint:\n        \"\"\"Create a checkpoint with options and recommendation.\"\"\"\n\n        context = self._build_context(goal, trigger)\n        options = self._build_options(goal, trigger)\n        recommendation = self._build_recommendation(goal, trigger, options)\n\n        return Checkpoint(\n            checkpoint_id=f\"cp-{uuid.uuid4().hex[:8]}\",\n            trigger=trigger,\n            context=context,\n            options=options,\n            recommendation=recommendation,\n            created_at=datetime.now().isoformat(),\n            goal_id=goal.goal_id,\n        )\n\n    def _build_context(self, goal: DailyGoal, trigger: CheckpointTrigger) -> str:\n        \"\"\"Build context string for checkpoint.\"\"\"\n        contexts = {\n            CheckpointTrigger.UX_CHANGE: f\"About to make user-facing changes: {goal.content}\",\n            CheckpointTrigger.COST_SINGLE: f\"This action is estimated to cost ${goal.estimated_cost_usd:.2f}: {goal.content}\",\n            CheckpointTrigger.COST_CUMULATIVE: f\"Daily spending has exceeded ${self.config.checkpoint_cost_daily}. Next action: {goal.content}\",\n            CheckpointTrigger.ARCHITECTURE: f\"This involves architectural changes: {goal.content}\",\n            CheckpointTrigger.SCOPE_CHANGE: f\"This goal was not in the original plan: {goal.content}\",\n            CheckpointTrigger.HICCUP: f\"Encountered an unexpected situation: {goal.content}. Error count: {getattr(goal, 'error_count', 0)}, Recovery level: {getattr(goal, 'recovery_level', 0)}\",\n        }\n        return contexts.get(trigger, f\"Checkpoint for: {goal.content}\")\n\n    def _build_options(self, goal: DailyGoal, trigger: CheckpointTrigger) -> list[CheckpointOption]:\n        \"\"\"Build standard options for checkpoint.\"\"\"\n        return [\n            CheckpointOption(\n                label=\"Proceed\",\n                description=\"Approve this action and continue\",\n                is_recommended=True,\n            ),\n            CheckpointOption(\n                label=\"Skip\",\n                description=\"Skip this goal and move to the next one\",\n            ),\n            CheckpointOption(\n                label=\"Modify\",\n                description=\"I'll provide adjusted instructions\",\n            ),\n            CheckpointOption(\n                label=\"Pause\",\n                description=\"Pause the session for manual review\",\n            ),\n        ]\n\n    def _build_recommendation(\n        self,\n        goal: DailyGoal,\n        trigger: CheckpointTrigger,\n        options: list[CheckpointOption],\n    ) -> str:\n        \"\"\"Build recommendation string.\"\"\"\n        if trigger == CheckpointTrigger.HICCUP:\n            return f\"Review needed - encountered unexpected situation with {goal.content}\"\n        return f\"Proceed - {goal.content} aligns with the current plan and is within acceptable risk bounds.\"\n\n    async def resolve_checkpoint(\n        self,\n        checkpoint_id: str,\n        chosen_option: str,\n        notes: Optional[str] = None,\n    ) -> Checkpoint:\n        \"\"\"Resolve a pending checkpoint with human decision.\"\"\"\n        checkpoint = await self.store.get(checkpoint_id)\n        if not checkpoint:\n            raise ValueError(f\"Checkpoint not found: {checkpoint_id}\")\n\n        checkpoint.status = \"approved\" if chosen_option == \"Proceed\" else \"rejected\"\n        checkpoint.chosen_option = chosen_option\n        checkpoint.human_notes = notes\n        checkpoint.resolved_at = datetime.now().isoformat()\n\n        await self.store.save(checkpoint)\n\n        # Record for preference learning\n        await self._record_decision(checkpoint)\n\n        return checkpoint\n\n    async def _record_decision(self, checkpoint: Checkpoint) -> None:\n        \"\"\"Record decision for preference learning.\"\"\"\n        # Get the most recent episode for this goal (if any)\n        recent_episodes = self.episode_store.load_recent(limit=10)\n        matching_episode = None\n        for ep in reversed(recent_episodes):\n            if ep.goal.goal_id == checkpoint.goal_id:\n                matching_episode = ep\n                break\n\n        # Record with preference learner\n        await self.preference_learner.record_decision(checkpoint, matching_episode)\n\n    def update_daily_cost(self, cost: float) -> None:\n        \"\"\"Update cumulative daily cost tracking. Called by AutopilotRunner after each goal.\"\"\"\n        self.daily_cost += cost\n\n    def reset_daily_cost(self) -> None:\n        \"\"\"Reset daily cost. Called by AutopilotRunner at session start.\"\"\"\n        self.daily_cost = 0.0\n\nclass CheckpointStore:\n    \"\"\"Persistent storage for checkpoints.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path / \"checkpoints\"\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n    async def save(self, checkpoint: Checkpoint) -> None:\n        \"\"\"Save checkpoint to JSON file.\"\"\"\n        path = self.base_path / f\"{checkpoint.checkpoint_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(checkpoint.to_dict(), f, indent=2)\n\n    async def get(self, checkpoint_id: str) -> Optional[Checkpoint]:\n        \"\"\"Load checkpoint by ID.\"\"\"\n        path = self.base_path / f\"{checkpoint_id}.json\"\n        if not path.exists():\n            return None\n        with open(path, \"r\") as f:\n            return Checkpoint.from_dict(json.load(f))\n\n    async def get_pending_for_goal(self, goal_id: str) -> Optional[Checkpoint]:\n        \"\"\"Get pending checkpoint for a goal.\"\"\"\n        for path in self.base_path.glob(\"*.json\"):\n            with open(path, \"r\") as f:\n                data = json.load(f)\n                if data.get(\"goal_id\") == goal_id and data.get(\"status\") == \"pending\":\n                    return Checkpoint.from_dict(data)\n        return None\n\n    async def list_pending(self) -> list[Checkpoint]:\n        \"\"\"List all pending checkpoints.\"\"\"\n        pending = []\n        for path in self.base_path.glob(\"*.json\"):\n            with open(path, \"r\") as f:\n                data = json.load(f)\n                if data.get(\"status\") == \"pending\":\n                    pending.append(Checkpoint.from_dict(data))\n        return pending\n```\n\n#### CLI Surface for Checkpoints\n\n```bash\n# View pending checkpoints\nswarm-attack cos checkpoints\n\n# Approve a checkpoint\nswarm-attack cos approve <checkpoint-id>\n\n# Approve with notes\nswarm-attack cos approve <checkpoint-id> --notes \"Proceed with caution\"\n\n# Reject/skip a checkpoint\nswarm-attack cos reject <checkpoint-id>\n\n# Modify and continue\nswarm-attack cos modify <checkpoint-id> --instructions \"Use approach B instead\"\n```\n\n#### Checkpoint CLI Implementation\n\n```python\n@cos_group.command(\"checkpoints\")\ndef list_checkpoints():\n    \"\"\"List pending checkpoints requiring approval.\"\"\"\n    pending = asyncio.run(checkpoint_store.list_pending())\n\n    if not pending:\n        click.echo(\"No pending checkpoints.\")\n        return\n\n    for cp in pending:\n        click.echo(f\"\\n\ud83d\udd14 CHECKPOINT: {cp.trigger.value.upper()}\")\n        click.echo(f\"   ID: {cp.checkpoint_id}\")\n        click.echo(f\"   Context: {cp.context}\")\n        click.echo(f\"   Options:\")\n        for opt in cp.options:\n            marker = \"\ud83d\udca1\" if opt.is_recommended else \"  \"\n            click.echo(f\"     {marker} {opt.label}: {opt.description}\")\n        if cp.recommendation:\n            click.echo(f\"   Recommendation: {cp.recommendation}\")\n\n@cos_group.command(\"approve\")\n@click.argument(\"checkpoint_id\")\n@click.option(\"--notes\", \"-n\", default=None, help=\"Optional notes\")\ndef approve_checkpoint(checkpoint_id: str, notes: Optional[str]):\n    \"\"\"Approve a pending checkpoint.\"\"\"\n    cp = asyncio.run(checkpoint_system.resolve_checkpoint(\n        checkpoint_id, \"Proceed\", notes\n    ))\n    click.echo(f\"\u2713 Checkpoint {checkpoint_id} approved.\")\n\n@cos_group.command(\"reject\")\n@click.argument(\"checkpoint_id\")\n@click.option(\"--notes\", \"-n\", default=None, help=\"Optional notes\")\ndef reject_checkpoint(checkpoint_id: str, notes: Optional[str]):\n    \"\"\"Reject/skip a pending checkpoint.\"\"\"\n    cp = asyncio.run(checkpoint_system.resolve_checkpoint(\n        checkpoint_id, \"Skip\", notes\n    ))\n    click.echo(f\"\u2717 Checkpoint {checkpoint_id} rejected.\")\n```\n\n#### Integration with AutopilotRunner\n\nThe checkpoint system integrates at the start of `_execute_goal()` as shown in section 2.1. When a checkpoint is pending:\n\n1. Execution pauses with `checkpoint_pending=True`\n2. CLI shows pending checkpoints on next `swarm-attack cos status`\n3. Human resolves via `swarm-attack cos approve/reject`\n4. Next autopilot cycle picks up the resolved checkpoint and continues\n\n#### Integration with CampaignExecutor\n\n```python\nclass CampaignExecutor:\n    async def execute_day(self, campaign: Campaign) -> \"DayResult\":\n        \"\"\"Execute one day of the campaign.\"\"\"\n\n        # Check for campaign-level checkpoint (e.g., milestone boundary)\n        if self._at_milestone_boundary(campaign):\n            checkpoint = await self.checkpoint_system.create_milestone_checkpoint(campaign)\n            if not checkpoint.approved:\n                campaign.state = CampaignState.PAUSED\n                await self.store.save(campaign)\n                return DayResult(\n                    campaign_id=campaign.campaign_id,\n                    day=campaign.current_day,\n                    goals_completed=0,\n                    cost_usd=0,\n                    campaign_progress=self._calculate_progress(campaign),\n                    paused_for_checkpoint=True,\n                )\n\n        # ... rest of execute_day ...\n```\n\n### Implementation Tasks (10 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 7.1 | Add orchestrator dependency to AutopilotRunner | S | - |\n| 7.2 | Implement feature execution path | M | 7.1 |\n| 7.3 | Implement bug execution path | M | 7.1 |\n| 7.4 | Implement spec pipeline execution path | M | 7.1 |\n| 7.5 | Add pre-execution budget checks | S | 7.1 |\n| 7.6 | Create Checkpoint and CheckpointStore dataclasses | M | - |\n| 7.7 | Implement CheckpointSystem with trigger detection (including hiccups) | M | 7.6 |\n| 7.8 | Add checkpoint CLI commands (checkpoints, approve, reject) | M | 7.6, 7.7 |\n| 7.9 | Integrate CheckpointSystem with AutopilotRunner (including cost tracking) | M | 7.1, 7.7 |\n| 7.10 | Add checkpoint integration to CampaignExecutor | S | 7.7, Phase 10 |\n\n---\n\n## 3. Phase 8: Hierarchical Error Recovery (P2)\n\n### Consensus Decision\n\n**Shunyu Yao:** \"Bounded retry with alternatives. Don't over-engineer - just try a different approach on failure.\"\n\n**David Dohan:** \"Agree. Add circuit breakers at each level. And the human escalation MUST happen within 30 minutes of hitting Level 4.\"\n\n**Kanjun Qiu:** \"Track which recovery strategies work. Feed that back into Reflexion for future episodes.\"\n\n**Final Design (Simplified):**\n\n```python\nclass RecoveryLevel(Enum):\n    RETRY_SAME = 1      # Transient failure - retry with same approach\n    RETRY_ALTERNATE = 2  # Systematic failure - try ONE alternative approach\n    RETRY_CLARIFY = 3   # Missing context - optionally ask clarifying question\n    ESCALATE = 4        # Human required - checkpoint pause\n\n@dataclass\nclass RecoveryStrategy:\n    level: RecoveryLevel\n    max_attempts: int\n    backoff_seconds: int\n\nclass RecoveryManager:\n    LEVELS = [\n        RecoveryStrategy(RecoveryLevel.RETRY_SAME, max_attempts=3, backoff_seconds=5),\n        RecoveryStrategy(RecoveryLevel.RETRY_ALTERNATE, max_attempts=2, backoff_seconds=10),\n        RecoveryStrategy(RecoveryLevel.RETRY_CLARIFY, max_attempts=1, backoff_seconds=0),\n        RecoveryStrategy(RecoveryLevel.ESCALATE, max_attempts=1, backoff_seconds=0),\n    ]\n\n    def __init__(self, config: ChiefOfStaffConfig, reflexion: ReflexionEngine, checkpoint_system: CheckpointSystem):\n        self.config = config\n        self.reflexion = reflexion\n        self.checkpoint_system = checkpoint_system\n        self.error_streak = 0\n\n    async def execute_with_recovery(\n        self,\n        goal: DailyGoal,\n        action: Callable[[], GoalExecutionResult],\n    ) -> GoalExecutionResult:\n        \"\"\"Execute action with automatic recovery through all levels.\"\"\"\n\n        last_result = None\n        current_recovery_level = 0\n\n        for strategy in self.LEVELS:\n            current_recovery_level = strategy.level.value\n\n            # Update goal's recovery level for hiccup detection\n            goal.recovery_level = current_recovery_level\n\n            for attempt in range(strategy.max_attempts):\n                # Retrieve relevant past episodes for context (simple retrieval)\n                context = await self.reflexion.retrieve_relevant(goal, k=3)\n\n                if strategy.level == RecoveryLevel.RETRY_ALTERNATE and last_result:\n                    # Generate ONE alternative approach (simple, no ToT)\n                    alternative = await self._generate_simple_alternative(\n                        goal, last_result, context\n                    )\n                    if alternative:\n                        action = alternative\n\n                elif strategy.level == RecoveryLevel.RETRY_CLARIFY and last_result:\n                    # Optional: ask ONE clarifying question if error suggests missing info\n                    if self._error_suggests_missing_info(last_result.error):\n                        clarification = await self._ask_simple_clarification(goal, last_result)\n                        if clarification:\n                            goal = self._incorporate_clarification(goal, clarification)\n\n                elif strategy.level == RecoveryLevel.ESCALATE:\n                    # Mark goal as hiccup for checkpoint system\n                    goal.is_hiccup = True\n                    # Create hiccup checkpoint for human\n                    return await self._escalate_to_human(goal, last_result)\n\n                # Execute with backoff\n                if attempt > 0:\n                    await asyncio.sleep(strategy.backoff_seconds * (2 ** attempt))\n\n                result = await action()\n\n                if result.success:\n                    self.error_streak = 0\n                    # Record success for Reflexion learning\n                    await self.reflexion.record_episode(goal, result, strategy.level.value)\n                    return result\n\n                last_result = result\n                self.error_streak += 1\n\n                # Update goal error count for hiccup detection\n                goal.error_count = getattr(goal, 'error_count', 0) + 1\n\n                # Check circuit breaker\n                if self.error_streak >= self.config.error_streak_threshold:\n                    break\n\n        # All levels exhausted - escalate to human\n        goal.is_hiccup = True\n        return await self._escalate_to_human(goal, last_result)\n\n    async def _escalate_to_human(\n        self,\n        goal: DailyGoal,\n        failure: GoalExecutionResult,\n    ) -> GoalExecutionResult:\n        \"\"\"Create hiccup checkpoint and pause for human.\"\"\"\n        checkpoint = Checkpoint(\n            checkpoint_id=f\"cp-{uuid.uuid4().hex[:8]}\",\n            trigger=CheckpointTrigger.HICCUP,\n            context=f\"Recovery failed after 4 levels. Goal: {goal.content}. Error: {failure.error}\",\n            options=[\n                CheckpointOption(label=\"Retry\", description=\"Try again with fresh context\"),\n                CheckpointOption(label=\"Skip\", description=\"Skip this goal\"),\n                CheckpointOption(label=\"Manual\", description=\"I'll handle this manually\"),\n            ],\n            recommendation=\"Skip - automatic recovery exhausted all options\",\n            created_at=datetime.now().isoformat(),\n            goal_id=goal.goal_id,\n        )\n        await self.checkpoint_system.store.save(checkpoint)\n\n        return GoalExecutionResult(\n            success=False,\n            error=f\"Escalated to human: {failure.error}\",\n            cost_usd=failure.cost_usd,\n            checkpoint_pending=True,\n        )\n\n    async def _generate_simple_alternative(\n        self,\n        goal: DailyGoal,\n        failure: GoalExecutionResult,\n        context: list[Episode],\n    ) -> Optional[Callable]:\n        \"\"\"Generate ONE alternative approach based on the failure.\"\"\"\n        prompt = f\"\"\"\n        Goal: {goal.content}\n        Failed with error: {failure.error}\n\n        Past similar episodes that worked:\n        {[e.reflection for e in context if e.outcome.success][:2]}\n\n        Suggest ONE alternative approach. Be specific and actionable.\n        \"\"\"\n        # Simple LLM call, return modified action if viable\n        # Return None if no good alternative found\n\n    def _error_suggests_missing_info(self, error: Optional[str]) -> bool:\n        \"\"\"Check if error message suggests we need more information.\"\"\"\n        if not error:\n            return False\n        missing_info_patterns = [\"not found\", \"missing\", \"undefined\", \"unknown\"]\n        return any(p in error.lower() for p in missing_info_patterns)\n\n    async def _ask_simple_clarification(\n        self,\n        goal: DailyGoal,\n        failure: GoalExecutionResult,\n    ) -> Optional[str]:\n        \"\"\"Ask a simple clarifying question if needed.\"\"\"\n        # Only ask if error clearly indicates missing info\n        # Return clarification text or None\n```\n\n### Implementation Tasks (5 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 8.1 | Create RecoveryManager class with level definitions | M | Phase 7 |\n| 8.2 | Implement Level 1 (retry same with backoff) | S | 8.1 |\n| 8.3 | Implement Level 2 (simple alternative generation) | M | 8.1 |\n| 8.4 | Implement Level 3-4 (clarification and escalation with hiccup marking) | M | 8.1, 7.7 |\n| 8.5 | Add circuit breakers and error streak tracking | S | 8.1-8.4 |\n\n---\n\n## 4. Phase 9: Episode Memory + Reflexion + Preference Learning (P3)\n\n### Consensus Decision\n\n**Jerry Liu:** \"JSONL episodes with simple retrieval. Don't add embeddings until you have thousands of episodes - heuristic filtering works fine at startup scale.\"\n\n**Kanjun Qiu:** \"Reflexion is key. After each goal, generate a verbal reflection. Store it. Retrieve relevant reflections before future actions. This is cheap and powerful.\"\n\n**Harrison Chase:** \"Make sure episodes are structured. Goal \u2192 Actions \u2192 Outcome \u2192 Reflection. Then you can query any dimension.\"\n\n**PRD Requirement:** \"Track which recommendations I accept vs adjust. Adapt future recommendations based on preferences.\"\n\n### 4.1 Episode Memory Design\n\n```python\n@dataclass\nclass Episode:\n    \"\"\"A complete execution episode for learning.\"\"\"\n    episode_id: str\n    timestamp: str\n    goal: DailyGoal\n    actions: list[Action]\n    outcome: Outcome\n    reflection: str           # LLM-generated insight\n    recovery_level_used: int  # 1-4, for tracking recovery patterns\n    cost_usd: float\n    duration_seconds: int\n    tags: list[str]           # For simple filtering: [\"feature\", \"bug\", \"spec\", etc.]\n\n    # Preference learning (PRD requirement)\n    approval_response: Optional[str] = None  # \"accepted\", \"modified\", \"rejected\"\n    original_recommendation: Optional[str] = None\n    human_modification: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        return {\n            \"episode_id\": self.episode_id,\n            \"timestamp\": self.timestamp,\n            \"goal\": self.goal.to_dict(),\n            \"actions\": [a.to_dict() for a in self.actions],\n            \"outcome\": self.outcome.to_dict(),\n            \"reflection\": self.reflection,\n            \"recovery_level_used\": self.recovery_level_used,\n            \"cost_usd\": self.cost_usd,\n            \"duration_seconds\": self.duration_seconds,\n            \"tags\": self.tags,\n            \"approval_response\": self.approval_response,\n            \"original_recommendation\": self.original_recommendation,\n            \"human_modification\": self.human_modification,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Episode\":\n        return cls(\n            episode_id=data[\"episode_id\"],\n            timestamp=data[\"timestamp\"],\n            goal=DailyGoal.from_dict(data[\"goal\"]),\n            actions=[Action.from_dict(a) for a in data[\"actions\"]],\n            outcome=Outcome.from_dict(data[\"outcome\"]),\n            reflection=data[\"reflection\"],\n            recovery_level_used=data[\"recovery_level_used\"],\n            cost_usd=data[\"cost_usd\"],\n            duration_seconds=data[\"duration_seconds\"],\n            tags=data.get(\"tags\", []),\n            approval_response=data.get(\"approval_response\"),\n            original_recommendation=data.get(\"original_recommendation\"),\n            human_modification=data.get(\"human_modification\"),\n        )\n\n@dataclass\nclass Action:\n    \"\"\"A single action within an episode.\"\"\"\n    action_type: str      # \"orchestrator_call\", \"file_edit\", \"test_run\", etc.\n    description: str\n    result: str\n    cost_usd: float\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Action\":\n        return cls(**data)\n\n@dataclass\nclass Outcome:\n    \"\"\"Outcome of an episode.\"\"\"\n    success: bool\n    goal_status: str\n    error: Optional[str]\n    artifacts_created: list[str]  # Files, specs, etc.\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Outcome\":\n        return cls(**data)\n```\n\n### 4.2 Reflexion Engine\n\n```python\nclass ReflexionEngine:\n    \"\"\"Generates and retrieves episode reflections for learning.\"\"\"\n\n    def __init__(self, episode_store: \"EpisodeStore\", llm: LLMClient):\n        self.store = episode_store\n        self.llm = llm\n\n    async def reflect(self, episode: Episode) -> str:\n        \"\"\"Generate reflection on completed episode.\"\"\"\n        prompt = f\"\"\"\n        Analyze this development episode:\n\n        Goal: {episode.goal.content}\n        Actions taken: {len(episode.actions)} actions\n        Outcome: {\"SUCCESS\" if episode.outcome.success else \"FAILURE\"}\n        {\"Error: \" + episode.outcome.error if episode.outcome.error else \"\"}\n        Recovery level: {episode.recovery_level_used}\n        Cost: ${episode.cost_usd:.2f}\n        Duration: {episode.duration_seconds}s\n\n        Generate a concise reflection (2-3 sentences) covering:\n        1. What worked well or poorly?\n        2. What would you do differently?\n        3. Key insight for similar future goals?\n        \"\"\"\n\n        reflection = await self.llm.complete(prompt, max_tokens=200)\n        return reflection\n\n    async def retrieve_relevant(\n        self,\n        goal: DailyGoal,\n        k: int = 3,\n        success_only: bool = False,\n    ) -> list[Episode]:\n        \"\"\"Retrieve most relevant past episodes using simple heuristics.\"\"\"\n\n        # Load recent episodes (last 100)\n        candidates = self.store.load_recent(limit=100)\n\n        # Filter by success if requested\n        if success_only:\n            candidates = [e for e in candidates if e.outcome.success]\n\n        # Score by relevance (simple heuristics, no embeddings)\n        scored = []\n        goal_tags = self._extract_tags(goal)\n\n        for episode in candidates:\n            score = 0.0\n\n            # Tag overlap (most important)\n            tag_overlap = len(set(episode.tags) & set(goal_tags))\n            score += tag_overlap * 0.4\n\n            # Recency boost (episodes from last 7 days score higher)\n            age_days = self._days_ago(episode.timestamp)\n            if age_days < 7:\n                score += 0.3 * (1 - age_days / 7)\n\n            # Success bonus\n            if episode.outcome.success:\n                score += 0.2\n\n            # Low recovery level is good\n            if episode.recovery_level_used == 1:\n                score += 0.1\n\n            scored.append((episode, score))\n\n        # Sort by score descending\n        scored.sort(key=lambda x: x[1], reverse=True)\n\n        return [ep for ep, _ in scored[:k]]\n\n    def _extract_tags(self, goal: DailyGoal) -> list[str]:\n        \"\"\"Extract tags from goal for matching.\"\"\"\n        tags = []\n        if goal.linked_feature:\n            tags.append(\"feature\")\n            tags.append(goal.linked_feature)\n        if goal.linked_bug:\n            tags.append(\"bug\")\n            tags.append(goal.linked_bug)\n        if goal.linked_spec:\n            tags.append(\"spec\")\n        return tags\n\n    def _days_ago(self, timestamp: str) -> int:\n        \"\"\"Calculate days since timestamp.\"\"\"\n        dt = datetime.fromisoformat(timestamp)\n        return (datetime.now() - dt).days\n\n    async def record_episode(\n        self,\n        goal: DailyGoal,\n        result: GoalExecutionResult,\n        recovery_level: int,\n        actions: list[Action] = None,\n        approval_response: Optional[str] = None,\n        original_recommendation: Optional[str] = None,\n        human_modification: Optional[str] = None,\n    ) -> Episode:\n        \"\"\"Record episode and generate reflection.\"\"\"\n\n        outcome = Outcome(\n            success=result.success,\n            goal_status=goal.status.value if hasattr(goal.status, 'value') else str(goal.status),\n            error=result.error,\n            artifacts_created=[],\n        )\n\n        tags = self._extract_tags(goal)\n\n        episode = Episode(\n            episode_id=f\"ep-{uuid.uuid4().hex[:8]}\",\n            timestamp=datetime.now().isoformat(),\n            goal=goal,\n            actions=actions or [],\n            outcome=outcome,\n            reflection=\"\",\n            recovery_level_used=recovery_level,\n            cost_usd=result.cost_usd,\n            duration_seconds=result.duration_seconds,\n            tags=tags,\n            approval_response=approval_response,\n            original_recommendation=original_recommendation,\n            human_modification=human_modification,\n        )\n\n        # Generate reflection\n        episode.reflection = await self.reflect(episode)\n\n        # Store\n        await self.store.save(episode)\n\n        return episode\n```\n\n### 4.3 Preference Learning (PRD Requirement)\n\n```python\n@dataclass\nclass PreferenceWeight:\n    \"\"\"A learned preference weight.\"\"\"\n    key: str                  # e.g., \"cost_vs_speed\", \"conservative_vs_aggressive\"\n    value: float              # 0.0 to 1.0\n    confidence: float         # How confident we are in this weight\n    sample_count: int         # Number of data points\n    last_updated: str\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"PreferenceWeight\":\n        return cls(**data)\n\nclass PreferenceLearner:\n    \"\"\"Learns human preferences from approval patterns.\"\"\"\n\n    # Bounded learning: max change per update (David's safety requirement)\n    MAX_WEIGHT_CHANGE = 0.1\n    MIN_SAMPLES_FOR_CONFIDENCE = 5\n\n    def __init__(self, store: \"PreferenceStore\"):\n        self.store = store\n        self.weights: dict[str, PreferenceWeight] = {}\n\n    async def load(self) -> None:\n        \"\"\"Load preference weights from storage.\"\"\"\n        self.weights = await self.store.load_all()\n\n    async def record_decision(\n        self,\n        checkpoint: Checkpoint,\n        episode: Optional[Episode] = None,\n    ) -> None:\n        \"\"\"Record a human decision and update preferences.\n        \n        Called by CheckpointSystem._record_decision after checkpoint resolution.\n        \"\"\"\n\n        # Extract preference signals from the decision\n        signals = self._extract_signals(checkpoint, episode)\n\n        for key, direction in signals.items():\n            await self._update_weight(key, direction)\n\n        await self.store.save_all(self.weights)\n\n    def _extract_signals(\n        self,\n        checkpoint: Checkpoint,\n        episode: Optional[Episode],\n    ) -> dict[str, float]:\n        \"\"\"Extract preference signals from decision.\"\"\"\n        signals = {}\n\n        # Cost sensitivity: did they accept expensive action?\n        if checkpoint.trigger == CheckpointTrigger.COST_SINGLE:\n            if checkpoint.chosen_option == \"Proceed\":\n                signals[\"cost_tolerance\"] = 0.1  # Increase tolerance\n            else:\n                signals[\"cost_tolerance\"] = -0.1  # Decrease tolerance\n\n        # Cumulative cost sensitivity\n        if checkpoint.trigger == CheckpointTrigger.COST_CUMULATIVE:\n            if checkpoint.chosen_option == \"Proceed\":\n                signals[\"daily_cost_tolerance\"] = 0.1\n            else:\n                signals[\"daily_cost_tolerance\"] = -0.1\n\n        # Risk tolerance: did they accept risky action?\n        if checkpoint.trigger in (CheckpointTrigger.ARCHITECTURE, CheckpointTrigger.UX_CHANGE):\n            if checkpoint.chosen_option == \"Proceed\":\n                signals[\"risk_tolerance\"] = 0.05\n            else:\n                signals[\"risk_tolerance\"] = -0.05\n\n        # Hiccup handling preference\n        if checkpoint.trigger == CheckpointTrigger.HICCUP:\n            if checkpoint.chosen_option == \"Retry\":\n                signals[\"retry_tolerance\"] = 0.1\n            elif checkpoint.chosen_option == \"Skip\":\n                signals[\"skip_tendency\"] = 0.1\n            elif checkpoint.chosen_option == \"Manual\":\n                signals[\"manual_preference\"] = 0.1\n\n        # Speed vs safety: if they modified, they wanted something different\n        if checkpoint.chosen_option == \"Modify\":\n            signals[\"modification_tendency\"] = 0.1\n\n        return signals\n\n    async def _update_weight(self, key: str, direction: float) -> None:\n        \"\"\"Update a preference weight with bounded change.\"\"\"\n\n        if key not in self.weights:\n            self.weights[key] = PreferenceWeight(\n                key=key,\n                value=0.5,  # Start neutral\n                confidence=0.0,\n                sample_count=0,\n                last_updated=datetime.now().isoformat(),\n            )\n\n        weight = self.weights[key]\n\n        # Bounded update (David's safety requirement)\n        change = min(abs(direction), self.MAX_WEIGHT_CHANGE) * (1 if direction > 0 else -1)\n\n        # Apply update\n        weight.value = max(0.0, min(1.0, weight.value + change))\n        weight.sample_count += 1\n        weight.confidence = min(1.0, weight.sample_count / self.MIN_SAMPLES_FOR_CONFIDENCE)\n        weight.last_updated = datetime.now().isoformat()\n\n    def get_weight(self, key: str, default: float = 0.5) -> float:\n        \"\"\"Get a preference weight, or default if not enough data.\"\"\"\n        if key not in self.weights:\n            return default\n        weight = self.weights[key]\n        if weight.confidence < 0.5:\n            return default\n        return weight.value\n\n    def get_preference_summary(self) -> dict:\n        \"\"\"Get human-readable preference summary for standup.\"\"\"\n        summary = {}\n        for key, weight in self.weights.items():\n            if weight.confidence >= 0.5:\n                if weight.value > 0.6:\n                    summary[key] = \"high\"\n                elif weight.value < 0.4:\n                    summary[key] = \"low\"\n                else:\n                    summary[key] = \"neutral\"\n        return summary\n\nclass PreferenceStore:\n    \"\"\"Persistent storage for preference weights.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.path = base_path / \"preferences.json\"\n\n    async def load_all(self) -> dict[str, PreferenceWeight]:\n        \"\"\"Load all preference weights.\"\"\"\n        if not self.path.exists():\n            return {}\n        with open(self.path, \"r\") as f:\n            data = json.load(f)\n            return {k: PreferenceWeight.from_dict(v) for k, v in data.items()}\n\n    async def save_all(self, weights: dict[str, PreferenceWeight]) -> None:\n        \"\"\"Save all preference weights.\"\"\"\n        with open(self.path, \"w\") as f:\n            json.dump({k: v.to_dict() for k, v in weights.items()}, f, indent=2)\n```\n\n### 4.4 Episode Store\n\n```python\nclass EpisodeStore:\n    \"\"\"Persistent storage for episodes using plain JSONL.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path / \"episodes\"\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        self.episodes_file = self.base_path / \"episodes.jsonl\"\n\n    async def save(self, episode: Episode) -> None:\n        \"\"\"Append episode to JSONL file.\"\"\"\n        with open(self.episodes_file, \"a\") as f:\n            f.write(json.dumps(episode.to_dict()) + \"\\n\")\n\n    def load_recent(self, limit: int = 100) -> list[Episode]:\n        \"\"\"Load most recent episodes from JSONL.\"\"\"\n        if not self.episodes_file.exists():\n            return []\n\n        episodes = []\n        with open(self.episodes_file, \"r\") as f:\n            for line in f:\n                if line.strip():\n                    episodes.append(Episode.from_dict(json.loads(line)))\n\n        # Return most recent\n        return episodes[-limit:]\n\n    def load_all(self) -> list[Episode]:\n        \"\"\"Load all episodes.\"\"\"\n        return self.load_recent(limit=10000)\n```\n\n### Future Enhancement: Embeddings\n\nWhen episode count exceeds ~500 and retrieval quality degrades, add embedding support:\n- Generate embeddings for `goal.content + reflection`\n- Store in separate `embeddings.npy` file\n- Use cosine similarity for retrieval\n- This is a drop-in enhancement to `retrieve_relevant()`\n\n### Implementation Tasks (6 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 9.1 | Create Episode, Action, Outcome dataclasses with serialization | S | - |\n| 9.2 | Implement EpisodeStore with JSONL persistence | M | 9.1 |\n| 9.3 | Implement ReflexionEngine.reflect() | M | 9.1 |\n| 9.4 | Implement ReflexionEngine.retrieve_relevant() with heuristic scoring | M | 9.2 |\n| 9.5 | Integrate with RecoveryManager | M | Phase 8, 9.3, 9.4 |\n| 9.6 | Implement PreferenceLearner with checkpoint integration | M | 9.1, 7.7 |\n\n---\n\n## 5. Phase 10: Multi-Day Campaigns + Weekly Planning (P4)\n\n### Consensus Decision\n\n**Shunyu Yao:** \"Backward planning from goal state. Define milestones, then decompose into daily goals. Re-plan only when >30% off track.\"\n\n**Harrison Chase:** \"Add state machine for campaign lifecycle. PLANNING \u2192 ACTIVE \u2192 PAUSED \u2192 COMPLETED/FAILED. Persist everything.\"\n\n**Jerry Liu:** \"Campaign context needs to persist across days. Store in a campaign.json with all state.\"\n\n**David Dohan:** \"Budget caps must be per-campaign AND per-day. Daily cap prevents single-day runaway.\"\n\n**PRD Requirement (User Story #6):** \"I want a weekly planning session that projects forward, not just daily standups that look backward.\"\n\n### 5.1 Campaign Data Model\n\n```python\nclass CampaignState(Enum):\n    PLANNING = \"planning\"\n    ACTIVE = \"active\"\n    PAUSED = \"paused\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass Milestone:\n    \"\"\"A major deliverable within a campaign.\"\"\"\n    milestone_id: str\n    name: str\n    description: str\n    target_day: int           # Day 1, 2, 3, etc.\n    success_criteria: list[str]\n    status: str = \"pending\"   # pending, in_progress, done, blocked\n    completed_at: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Milestone\":\n        return cls(**data)\n\n@dataclass\nclass DayPlan:\n    \"\"\"Goals planned for a specific day of the campaign.\"\"\"\n    day_number: int\n    date: str                 # YYYY-MM-DD\n    goals: list[DailyGoal]\n    budget_usd: float\n    status: str = \"pending\"   # pending, in_progress, done\n    actual_cost_usd: float = 0.0\n    notes: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"day_number\": self.day_number,\n            \"date\": self.date,\n            \"goals\": [g.to_dict() for g in self.goals],\n            \"budget_usd\": self.budget_usd,\n            \"status\": self.status,\n            \"actual_cost_usd\": self.actual_cost_usd,\n            \"notes\": self.notes,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"DayPlan\":\n        return cls(\n            day_number=data[\"day_number\"],\n            date=data[\"date\"],\n            goals=[DailyGoal.from_dict(g) for g in data[\"goals\"]],\n            budget_usd=data[\"budget_usd\"],\n            status=data[\"status\"],\n            actual_cost_usd=data.get(\"actual_cost_usd\", 0.0),\n            notes=data.get(\"notes\", \"\"),\n        )\n\n@dataclass\nclass Campaign:\n    \"\"\"Multi-day development campaign.\"\"\"\n    campaign_id: str\n    name: str\n    goal: str                 # High-level goal\n    milestones: list[Milestone]\n    day_plans: list[DayPlan]\n\n    # Tracking\n    state: CampaignState = CampaignState.PLANNING\n    current_day: int = 1\n    started_at: Optional[str] = None\n    ended_at: Optional[str] = None\n\n    # Budgets (David's requirement)\n    total_budget_usd: float = 50.0\n    daily_budget_usd: float = 15.0\n    spent_usd: float = 0.0\n\n    # Replanning\n    original_duration_days: int = 0\n    replanning_threshold: float = 0.3  # 30% behind triggers replan\n    replan_count: int = 0\n\n    def to_dict(self) -> dict:\n        return {\n            \"campaign_id\": self.campaign_id,\n            \"name\": self.name,\n            \"goal\": self.goal,\n            \"milestones\": [m.to_dict() for m in self.milestones],\n            \"day_plans\": [d.to_dict() for d in self.day_plans],\n            \"state\": self.state.value,\n            \"current_day\": self.current_day,\n            \"started_at\": self.started_at,\n            \"ended_at\": self.ended_at,\n            \"total_budget_usd\": self.total_budget_usd,\n            \"daily_budget_usd\": self.daily_budget_usd,\n            \"spent_usd\": self.spent_usd,\n            \"original_duration_days\": self.original_duration_days,\n            \"replanning_threshold\": self.replanning_threshold,\n            \"replan_count\": self.replan_count,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Campaign\":\n        return cls(\n            campaign_id=data[\"campaign_id\"],\n            name=data[\"name\"],\n            goal=data[\"goal\"],\n            milestones=[Milestone.from_dict(m) for m in data[\"milestones\"]],\n            day_plans=[DayPlan.from_dict(d) for d in data[\"day_plans\"]],\n            state=CampaignState(data[\"state\"]),\n            current_day=data[\"current_day\"],\n            started_at=data.get(\"started_at\"),\n            ended_at=data.get(\"ended_at\"),\n            total_budget_usd=data.get(\"total_budget_usd\", 50.0),\n            daily_budget_usd=data.get(\"daily_budget_usd\", 15.0),\n            spent_usd=data.get(\"spent_usd\", 0.0),\n            original_duration_days=data.get(\"original_duration_days\", 0),\n            replanning_threshold=data.get(\"replanning_threshold\", 0.3),\n            replan_count=data.get(\"replan_count\", 0),\n        )\n\n    def days_behind(self) -> int:\n        \"\"\"Calculate how many days behind schedule.\"\"\"\n        if self.state != CampaignState.ACTIVE:\n            return 0\n\n        if self.original_duration_days == 0:\n            return 0\n\n        planned_progress = self.current_day / self.original_duration_days\n        completed_milestones = len([m for m in self.milestones if m.status == \"done\"])\n        actual_progress = completed_milestones / len(self.milestones) if self.milestones else 0\n\n        if actual_progress < planned_progress:\n            return int((planned_progress - actual_progress) * self.original_duration_days)\n        return 0\n\n    def needs_replan(self) -> bool:\n        \"\"\"Check if campaign is far enough behind to trigger replan.\"\"\"\n        if len(self.milestones) == 0 or self.original_duration_days == 0:\n            return False\n\n        progress_gap = self.days_behind() / self.original_duration_days\n        return progress_gap > self.replanning_threshold\n```\n\n### 5.2 Campaign Planner\n\n```python\nclass CampaignPlanner:\n    \"\"\"Plans multi-day campaigns using backward planning.\"\"\"\n\n    def __init__(self, llm: LLMClient):\n        self.llm = llm\n\n    async def plan(\n        self,\n        goal: str,\n        deadline_days: Optional[int] = None,\n        budget_usd: Optional[float] = None,\n    ) -> Campaign:\n        \"\"\"Create campaign plan using backward planning.\"\"\"\n\n        prompt = f\"\"\"\n        Goal: {goal}\n        {\"Deadline: \" + str(deadline_days) + \" days\" if deadline_days else \"No fixed deadline\"}\n        {\"Budget: $\" + str(budget_usd) if budget_usd else \"No fixed budget\"}\n\n        Using backward planning from the goal state:\n\n        1. Define the END STATE (what does success look like?)\n        2. Identify MILESTONES needed to reach end state\n        3. Sequence milestones with dependencies\n        4. Estimate days needed for each milestone\n        5. Decompose each milestone into daily goals\n\n        Return structured JSON with:\n        - milestones: [{{name, description, target_day, success_criteria}}]\n        - day_plans: [{{day_number, goals: [{{content, priority, estimated_minutes, linked_feature/bug/spec}}]}}]\n        - total_estimated_days\n        - total_estimated_cost_usd\n        \"\"\"\n\n        plan_response = await self.llm.complete(prompt, max_tokens=2000)\n        plan_data = json.loads(plan_response)\n\n        campaign_id = f\"camp-{uuid.uuid4().hex[:8]}\"\n\n        milestones = [\n            Milestone(\n                milestone_id=f\"ms-{i+1}\",\n                name=m[\"name\"],\n                description=m[\"description\"],\n                target_day=m[\"target_day\"],\n                success_criteria=m.get(\"success_criteria\", []),\n            )\n            for i, m in enumerate(plan_data[\"milestones\"])\n        ]\n\n        day_plans = [\n            DayPlan(\n                day_number=d[\"day_number\"],\n                date=\"\",  # Set when campaign starts\n                goals=[DailyGoal.from_dict(g) for g in d[\"goals\"]],\n                budget_usd=budget_usd / len(plan_data[\"day_plans\"]) if budget_usd else 15.0,\n            )\n            for d in plan_data[\"day_plans\"]\n        ]\n\n        return Campaign(\n            campaign_id=campaign_id,\n            name=goal[:50],\n            goal=goal,\n            milestones=milestones,\n            day_plans=day_plans,\n            total_budget_usd=budget_usd or 50.0,\n            daily_budget_usd=budget_usd / len(day_plans) if budget_usd else 15.0,\n            original_duration_days=len(day_plans),\n        )\n\n    async def replan(self, campaign: Campaign) -> Campaign:\n        \"\"\"Replan remaining days of campaign.\"\"\"\n\n        completed = [m for m in campaign.milestones if m.status == \"done\"]\n        remaining = [m for m in campaign.milestones if m.status != \"done\"]\n\n        prompt = f\"\"\"\n        Campaign: {campaign.name}\n        Original goal: {campaign.goal}\n\n        Completed milestones: {[m.name for m in completed]}\n        Remaining milestones: {[m.name for m in remaining]}\n\n        Days elapsed: {campaign.current_day}\n        Budget spent: ${campaign.spent_usd:.2f}\n        Budget remaining: ${campaign.total_budget_usd - campaign.spent_usd:.2f}\n\n        Create revised plan for remaining milestones.\n        Adjust estimates based on actual pace so far.\n\n        Return JSON with updated day_plans for remaining work.\n        \"\"\"\n\n        revised = await self.llm.complete(prompt, max_tokens=1500)\n        revised_data = json.loads(revised)\n\n        # Update remaining day plans\n        new_day_plans = campaign.day_plans[:campaign.current_day]\n        for d in revised_data[\"day_plans\"]:\n            new_day_plans.append(DayPlan(\n                day_number=len(new_day_plans) + 1,\n                date=\"\",\n                goals=[DailyGoal.from_dict(g) for g in d[\"goals\"]],\n                budget_usd=campaign.daily_budget_usd,\n            ))\n\n        campaign.day_plans = new_day_plans\n        campaign.replan_count += 1\n\n        return campaign\n```\n\n### 5.3 Campaign Executor\n\n```python\nclass CampaignExecutor:\n    \"\"\"Executes campaign day by day.\"\"\"\n\n    def __init__(\n        self,\n        autopilot: AutopilotRunner,\n        planner: CampaignPlanner,\n        store: \"CampaignStore\",\n        checkpoint_system: CheckpointSystem,\n    ):\n        self.autopilot = autopilot\n        self.planner = planner\n        self.store = store\n        self.checkpoint_system = checkpoint_system\n\n    async def execute_day(self, campaign: Campaign) -> \"DayResult\":\n        \"\"\"Execute one day of the campaign.\"\"\"\n\n        if campaign.state != CampaignState.ACTIVE:\n            raise ValueError(f\"Campaign not active: {campaign.state}\")\n\n        if campaign.current_day > len(campaign.day_plans):\n            raise ValueError(\"No more days planned\")\n\n        # Check for milestone boundary checkpoint\n        if self._at_milestone_boundary(campaign):\n            checkpoint = await self._create_milestone_checkpoint(campaign)\n            pending = await self.checkpoint_system.store.get_pending_for_goal(\n                f\"milestone-{campaign.campaign_id}\"\n            )\n            if pending and pending.status != \"approved\":\n                campaign.state = CampaignState.PAUSED\n                await self.store.save(campaign)\n                return DayResult(\n                    campaign_id=campaign.campaign_id,\n                    day=campaign.current_day,\n                    goals_completed=0,\n                    cost_usd=0,\n                    campaign_progress=self._calculate_progress(campaign),\n                    paused_for_checkpoint=True,\n                )\n\n        day_plan = campaign.day_plans[campaign.current_day - 1]\n\n        # Run autopilot for today's goals\n        result = await self.autopilot.start(\n            goals=day_plan.goals,\n            budget_usd=min(campaign.daily_budget_usd,\n                          campaign.total_budget_usd - campaign.spent_usd),\n        )\n\n        # Update campaign state\n        day_plan.actual_cost_usd = result.total_cost_usd\n        day_plan.status = \"done\"\n        campaign.spent_usd += result.total_cost_usd\n\n        # Check if replan needed\n        if campaign.needs_replan():\n            campaign = await self.planner.replan(campaign)\n\n        # Advance to next day\n        campaign.current_day += 1\n\n        # Check completion\n        if campaign.current_day > len(campaign.day_plans):\n            all_done = all(m.status == \"done\" for m in campaign.milestones)\n            campaign.state = CampaignState.COMPLETED if all_done else CampaignState.FAILED\n            campaign.ended_at = datetime.now().isoformat()\n\n        # Persist\n        await self.store.save(campaign)\n\n        return DayResult(\n            campaign_id=campaign.campaign_id,\n            day=campaign.current_day - 1,\n            goals_completed=result.goals_completed,\n            cost_usd=result.total_cost_usd,\n            campaign_progress=self._calculate_progress(campaign),\n        )\n\n    def _at_milestone_boundary(self, campaign: Campaign) -> bool:\n        \"\"\"Check if we're about to start a new milestone.\"\"\"\n        for milestone in campaign.milestones:\n            if milestone.target_day == campaign.current_day and milestone.status == \"pending\":\n                return True\n        return False\n\n    async def _create_milestone_checkpoint(self, campaign: Campaign) -> Checkpoint:\n        \"\"\"Create checkpoint at milestone boundary.\"\"\"\n        milestone = next(\n            m for m in campaign.milestones\n            if m.target_day == campaign.current_day and m.status == \"pending\"\n        )\n        checkpoint = Checkpoint(\n            checkpoint_id=f\"cp-{uuid.uuid4().hex[:8]}\",\n            trigger=CheckpointTrigger.SCOPE_CHANGE,\n            context=f\"Starting milestone '{milestone.name}': {milestone.description}\",\n            options=[\n                CheckpointOption(label=\"Proceed\", description=\"Start this milestone\", is_recommended=True),\n                CheckpointOption(label=\"Adjust\", description=\"Modify milestone scope\"),\n                CheckpointOption(label=\"Pause\", description=\"Pause campaign for review\"),\n            ],\n            recommendation=f\"Proceed with milestone: {milestone.name}\",\n            created_at=datetime.now().isoformat(),\n            goal_id=f\"milestone-{campaign.campaign_id}\",\n            campaign_id=campaign.campaign_id,\n        )\n        await self.checkpoint_system.store.save(checkpoint)\n        return checkpoint\n\n    def _calculate_progress(self, campaign: Campaign) -> float:\n        \"\"\"Calculate overall campaign progress 0-1.\"\"\"\n        if not campaign.milestones:\n            return 0.0\n        done = len([m for m in campaign.milestones if m.status == \"done\"])\n        return done / len(campaign.milestones)\n\n    async def resume(self, campaign_id: str) -> Campaign:\n        \"\"\"Resume a paused or next-day campaign.\"\"\"\n        campaign = await self.store.load(campaign_id)\n        if not campaign:\n            raise ValueError(f\"Campaign not found: {campaign_id}\")\n\n        if campaign.state == CampaignState.PAUSED:\n            campaign.state = CampaignState.ACTIVE\n\n        return campaign\n\n@dataclass\nclass DayResult:\n    \"\"\"Result of executing one day of a campaign.\"\"\"\n    campaign_id: str\n    day: int\n    goals_completed: int\n    cost_usd: float\n    campaign_progress: float\n    paused_for_checkpoint: bool = False\n\nclass CampaignStore:\n    \"\"\"Persistent storage for campaigns.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path / \"campaigns\"\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n    async def save(self, campaign: Campaign) -> None:\n        \"\"\"Save campaign to JSON file.\"\"\"\n        path = self.base_path / f\"{campaign.campaign_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(campaign.to_dict(), f, indent=2)\n\n    async def load(self, campaign_id: str) -> Optional[Campaign]:\n        \"\"\"Load campaign from JSON file.\"\"\"\n        path = self.base_path / f\"{campaign_id}.json\"\n        if not path.exists():\n            return None\n        with open(path, \"r\") as f:\n            return Campaign.from_dict(json.load(f))\n\n    async def list_all(self) -> list[Campaign]:\n        \"\"\"List all campaigns.\"\"\"\n        campaigns = []\n        for path in self.base_path.glob(\"*.json\"):\n            with open(path, \"r\") as f:\n                campaigns.append(Campaign.from_dict(json.load(f)))\n        return campaigns\n```\n\n### 5.4 Weekly Planning (Minimal - PRD User Story #6)\n\n```python\n@dataclass\nclass WeeklySummary:\n    \"\"\"Weekly planning summary.\"\"\"\n    week_start: str\n    week_end: str\n    campaigns_active: list[str]\n    campaigns_completed: list[str]\n    milestones_completed: int\n    milestones_remaining: int\n    total_cost_usd: float\n    goals_completed: int\n    goals_failed: int\n    next_week_projection: list[str]  # Projected goals for next week\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\nclass WeeklyPlanner:\n    \"\"\"Lightweight weekly planning using campaign data.\"\"\"\n\n    def __init__(\n        self,\n        campaign_store: CampaignStore,\n        episode_store: EpisodeStore,\n        llm: LLMClient,\n    ):\n        self.campaign_store = campaign_store\n        self.episode_store = episode_store\n        self.llm = llm\n\n    async def generate_weekly_summary(self) -> WeeklySummary:\n        \"\"\"Generate weekly summary from campaign and episode data.\"\"\"\n\n        # Get this week's date range\n        today = datetime.now()\n        week_start = today - timedelta(days=today.weekday())\n        week_end = week_start + timedelta(days=6)\n\n        # Load campaigns\n        campaigns = await self.campaign_store.list_all()\n        active = [c for c in campaigns if c.state == CampaignState.ACTIVE]\n        completed = [c for c in campaigns if c.state == CampaignState.COMPLETED\n                     and c.ended_at and c.ended_at >= week_start.isoformat()]\n\n        # Load episodes from this week\n        episodes = self.episode_store.load_recent(limit=500)\n        week_episodes = [\n            e for e in episodes\n            if e.timestamp >= week_start.isoformat()\n        ]\n\n        # Calculate metrics\n        milestones_completed = sum(\n            len([m for m in c.milestones if m.status == \"done\"\n                 and m.completed_at and m.completed_at >= week_start.isoformat()])\n            for c in campaigns\n        )\n        milestones_remaining = sum(\n            len([m for m in c.milestones if m.status != \"done\"])\n            for c in active\n        )\n\n        goals_completed = len([e for e in week_episodes if e.outcome.success])\n        goals_failed = len([e for e in week_episodes if not e.outcome.success])\n        total_cost = sum(e.cost_usd for e in week_episodes)\n\n        # Project next week\n        next_week_projection = await self._project_next_week(active)\n\n        return WeeklySummary(\n            week_start=week_start.strftime(\"%Y-%m-%d\"),\n            week_end=week_end.strftime(\"%Y-%m-%d\"),\n            campaigns_active=[c.name for c in active],\n            campaigns_completed=[c.name for c in completed],\n            milestones_completed=milestones_completed,\n            milestones_remaining=milestones_remaining,\n            total_cost_usd=total_cost,\n            goals_completed=goals_completed,\n            goals_failed=goals_failed,\n            next_week_projection=next_week_projection,\n        )\n\n    async def _project_next_week(self, active_campaigns: list[Campaign]) -> list[str]:\n        \"\"\"Project goals for next week based on active campaigns.\"\"\"\n        projections = []\n\n        for campaign in active_campaigns:\n            remaining_days = len(campaign.day_plans) - campaign.current_day + 1\n            if remaining_days <= 0:\n                continue\n\n            # Get next 5 days of goals\n            for i in range(min(5, remaining_days)):\n                day_idx = campaign.current_day - 1 + i\n                if day_idx < len(campaign.day_plans):\n                    day_plan = campaign.day_plans[day_idx]\n                    for goal in day_plan.goals[:2]:  # Top 2 goals per day\n                        projections.append(f\"[{campaign.name}] {goal.content}\")\n\n        return projections[:10]  # Top 10 projections\n\n    async def generate_weekly_report(self) -> str:\n        \"\"\"Generate human-readable weekly report.\"\"\"\n        summary = await self.generate_weekly_summary()\n\n        report = f\"\"\"\n# Weekly Summary: {summary.week_start} to {summary.week_end}\n\n## Campaigns\n- Active: {', '.join(summary.campaigns_active) or 'None'}\n- Completed this week: {', '.join(summary.campaigns_completed) or 'None'}\n\n## Progress\n- Milestones completed: {summary.milestones_completed}\n- Milestones remaining: {summary.milestones_remaining}\n- Goals completed: {summary.goals_completed}\n- Goals failed: {summary.goals_failed}\n- Total cost: ${summary.total_cost_usd:.2f}\n\n## Next Week Projection\n\"\"\"\n        for proj in summary.next_week_projection:\n            report += f\"- {proj}\\n\"\n\n        return report\n```\n\n### CLI Extensions\n\n```bash\n# Campaign management\nswarm-attack cos campaign create \"Implement auth system\" --days 5 --budget 50\nswarm-attack cos campaign status [CAMPAIGN_ID]\nswarm-attack cos campaign list\nswarm-attack cos campaign run [CAMPAIGN_ID]      # Execute today's goals\nswarm-attack cos campaign resume [CAMPAIGN_ID]   # Resume paused campaign\nswarm-attack cos campaign pause [CAMPAIGN_ID]\nswarm-attack cos campaign replan [CAMPAIGN_ID]   # Force replan\n\n# Weekly planning (PRD User Story #6)\nswarm-attack cos weekly                          # Generate weekly summary\nswarm-attack cos weekly --report                 # Full weekly report\n```\n\n### Implementation Tasks (8 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 10.1 | Create Campaign, Milestone, DayPlan dataclasses | M | - |\n| 10.2 | Implement CampaignStore persistence | M | 10.1 |\n| 10.3 | Implement CampaignPlanner.plan() | L | 10.1 |\n| 10.4 | Implement CampaignPlanner.replan() | M | 10.3 |\n| 10.5 | Implement CampaignExecutor.execute_day() | L | 10.1, Phase 7 |\n| 10.6 | Add campaign CLI commands | M | 10.2, 10.5 |\n| 10.7 | Add campaign progress to standup | S | 10.2 |\n| 10.8 | Implement WeeklyPlanner and weekly CLI command | M | 10.2, 9.2 |\n\n---\n\n## 6. Phase 11: Internal Validation Critics (P5)\n\n### Consensus Decision\n\n**Kanjun Qiu:** \"Use diverse critics - one for each quality dimension. They should disagree with each other to surface real issues.\"\n\n**Harrison Chase:** \"Majority voting for approval, but ANY security critic veto blocks. Safety is not a democracy.\"\n\n**David Dohan:** \"Track critic accuracy over time. If a critic is consistently wrong, reduce its weight. But never zero - preserve voice.\"\n\n### 6.1 Critic Data Model\n\n```python\nclass CriticFocus(Enum):\n    COMPLETENESS = \"completeness\"\n    FEASIBILITY = \"feasibility\"\n    SECURITY = \"security\"         # Veto power\n    STYLE = \"style\"\n    COVERAGE = \"coverage\"\n    EDGE_CASES = \"edge_cases\"\n\n@dataclass\nclass CriticScore:\n    critic_name: str\n    focus: CriticFocus\n    score: float              # 0-1\n    approved: bool\n    issues: list[str]\n    suggestions: list[str]\n    reasoning: str\n\n    def to_dict(self) -> dict:\n        return {\n            \"critic_name\": self.critic_name,\n            \"focus\": self.focus.value,\n            \"score\": self.score,\n            \"approved\": self.approved,\n            \"issues\": self.issues,\n            \"suggestions\": self.suggestions,\n            \"reasoning\": self.reasoning,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"CriticScore\":\n        return cls(\n            critic_name=data[\"critic_name\"],\n            focus=CriticFocus(data[\"focus\"]),\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\n@dataclass\nclass ValidationResult:\n    artifact_type: str        # \"spec\", \"code\", \"test\"\n    artifact_id: str\n    approved: bool\n    scores: list[CriticScore]\n    blocking_issues: list[str]\n    consensus_summary: str\n    human_review_required: bool\n\n    def to_dict(self) -> dict:\n        return {\n            \"artifact_type\": self.artifact_type,\n            \"artifact_id\": self.artifact_id,\n            \"approved\": self.approved,\n            \"scores\": [s.to_dict() for s in self.scores],\n            \"blocking_issues\": self.blocking_issues,\n            \"consensus_summary\": self.consensus_summary,\n            \"human_review_required\": self.human_review_required,\n        }\n```\n\n### 6.2 Critic Implementations\n\n```python\nclass Critic:\n    \"\"\"Base class for validation critics.\"\"\"\n\n    def __init__(self, focus: CriticFocus, llm: LLMClient, weight: float = 1.0):\n        self.focus = focus\n        self.llm = llm\n        self.weight = weight\n        self.has_veto = focus == CriticFocus.SECURITY\n\n    async def evaluate(self, artifact: str) -> CriticScore:\n        raise NotImplementedError\n\nclass SpecCritic(Critic):\n    \"\"\"Evaluates engineering specs.\"\"\"\n\n    async def evaluate(self, spec_content: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.COMPLETENESS: \"How complete is the spec? Missing sections? Gaps in requirements?\",\n            CriticFocus.FEASIBILITY: \"Can this be implemented as written? Unclear requirements? Impossible constraints?\",\n            CriticFocus.SECURITY: \"Security issues? Injection risks? Auth gaps? Data exposure?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate this engineering spec for {self.focus.value}:\n\n        {spec_content[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"SpecCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\nclass CodeCritic(Critic):\n    \"\"\"Evaluates code changes.\"\"\"\n\n    async def evaluate(self, code_diff: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.STYLE: \"Code style issues? Naming? Structure? Readability?\",\n            CriticFocus.SECURITY: \"Security vulnerabilities? Injection? Unsafe operations? Secrets exposed?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate this code change for {self.focus.value}:\n\n        {code_diff[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"CodeCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\nclass TestCritic(Critic):\n    \"\"\"Evaluates test coverage and quality.\"\"\"\n\n    async def evaluate(self, test_content: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.COVERAGE: \"Test coverage adequate? Missing scenarios? Key paths untested?\",\n            CriticFocus.EDGE_CASES: \"Edge cases covered? Boundary conditions? Error scenarios?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate these tests for {self.focus.value}:\n\n        {test_content[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"TestCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n```\n\n### 6.3 Validation Layer\n\n```python\nclass ValidationLayer:\n    \"\"\"Orchestrates multiple critics for consensus building.\"\"\"\n\n    def __init__(self, llm: LLMClient):\n        self.llm = llm\n        self.critics = {\n            \"spec\": [\n                SpecCritic(CriticFocus.COMPLETENESS, llm),\n                SpecCritic(CriticFocus.FEASIBILITY, llm),\n                SpecCritic(CriticFocus.SECURITY, llm),\n            ],\n            \"code\": [\n                CodeCritic(CriticFocus.STYLE, llm),\n                CodeCritic(CriticFocus.SECURITY, llm),\n            ],\n            \"test\": [\n                TestCritic(CriticFocus.COVERAGE, llm),\n                TestCritic(CriticFocus.EDGE_CASES, llm),\n            ],\n        }\n\n    async def validate(\n        self,\n        artifact: str,\n        artifact_type: str,\n        artifact_id: str = \"\",\n    ) -> ValidationResult:\n        \"\"\"Run all relevant critics and build consensus.\"\"\"\n\n        relevant_critics = self.critics.get(artifact_type, [])\n        if not relevant_critics:\n            return ValidationResult(\n                artifact_type=artifact_type,\n                artifact_id=artifact_id,\n                approved=True,\n                scores=[],\n                blocking_issues=[],\n                consensus_summary=\"No critics configured for this artifact type\",\n                human_review_required=False,\n            )\n\n        # Run critics in parallel\n        scores = await asyncio.gather(*[\n            c.evaluate(artifact) for c in relevant_critics\n        ])\n\n        # Check for security veto\n        security_scores = [s for s in scores if s.focus == CriticFocus.SECURITY]\n        if any(not s.approved for s in security_scores):\n            return ValidationResult(\n                artifact_type=artifact_type,\n                artifact_id=artifact_id,\n                approved=False,\n                scores=list(scores),\n                blocking_issues=[\n                    issue for s in security_scores if not s.approved for issue in s.issues\n                ],\n                consensus_summary=\"BLOCKED: Security concerns require human review\",\n                human_review_required=True,\n            )\n\n        # Majority vote (weighted)\n        total_weight = sum(c.weight for c in relevant_critics)\n        approval_weight = sum(\n            c.weight for c, s in zip(relevant_critics, scores) if s.approved\n        )\n\n        approved = (approval_weight / total_weight) >= 0.6  # 60% threshold\n\n        return ValidationResult(\n            artifact_type=artifact_type,\n            artifact_id=artifact_id,\n            approved=approved,\n            scores=list(scores),\n            blocking_issues=[\n                issue for s in scores if not s.approved for issue in s.issues\n            ],\n            consensus_summary=self._build_summary(list(scores), approved),\n            human_review_required=not approved,\n        )\n\n    def _build_summary(self, scores: list[CriticScore], approved: bool) -> str:\n        avg_score = sum(s.score for s in scores) / len(scores) if scores else 0\n\n        if approved:\n            return f\"APPROVED by consensus (avg score: {avg_score:.2f})\"\n        else:\n            concerns = [s.focus.value for s in scores if not s.approved]\n            return f\"NEEDS REVIEW: concerns in {', '.join(concerns)} (avg: {avg_score:.2f})\"\n```\n\n### 6.4 Integration Flow\n\nThe ValidationLayer integrates at three key points in the pipeline:\n\n#### Spec Pipeline Integration\n\n```python\n# In Orchestrator.run_spec_pipeline()\nclass Orchestrator:\n    async def run_spec_pipeline(self, feature_id: str) -> SpecResult:\n        spec = await self._generate_spec(feature_id)\n\n        # GATE: Validate before human approval\n        validation = await self.validation_layer.validate(\n            artifact=spec.content,\n            artifact_type=\"spec\",\n            artifact_id=feature_id,\n        )\n\n        if validation.approved:\n            # Auto-advance to approval (human still sees it)\n            return SpecResult(\n                status=\"ready_for_approval\",\n                validation=validation,\n                auto_approved=True,\n            )\n        else:\n            # Block and surface issues\n            return SpecResult(\n                status=\"needs_revision\",\n                validation=validation,\n                blocking_issues=validation.blocking_issues,\n            )\n```\n\n#### Code Execution Integration\n\n```python\n# In AutopilotRunner._execute_goal()\nclass AutopilotRunner:\n    async def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:\n        # ... existing execution logic ...\n\n        if goal.linked_feature and result.success:\n            # GATE: Validate code before marking complete\n            code_diff = await self._get_code_diff(goal.linked_feature)\n            validation = await self.validation_layer.validate(\n                artifact=code_diff,\n                artifact_type=\"code\",\n                artifact_id=f\"{goal.linked_feature}-{goal.linked_issue}\",\n            )\n\n            if not validation.approved:\n                # Don't mark complete - needs fixes\n                return GoalExecutionResult(\n                    success=False,\n                    error=\"Validation failed: \" + validation.consensus_summary,\n                    validation=validation,\n                )\n\n        return result\n```\n\n#### CLI Validation Command\n\n```bash\n# Manual validation check\nswarm-attack cos validate spec chief-of-staff-v2\nswarm-attack cos validate code swarm_attack/chief_of_staff/\n```\n\n```python\n# In CLI\n@cos_group.command(\"validate\")\n@click.argument(\"artifact_type\", type=click.Choice([\"spec\", \"code\", \"test\"]))\n@click.argument(\"path\")\ndef validate_artifact(artifact_type: str, path: str):\n    \"\"\"Run validation critics on an artifact.\"\"\"\n    content = Path(path).read_text()\n    result = asyncio.run(validation_layer.validate(content, artifact_type))\n\n    if result.approved:\n        click.echo(f\"\u2713 APPROVED: {result.consensus_summary}\")\n    else:\n        click.echo(f\"\u2717 NEEDS REVIEW: {result.consensus_summary}\")\n        for issue in result.blocking_issues:\n            click.echo(f\"  - {issue}\")\n```\n\n#### Gating Rules Summary\n\n| Artifact | Gate Location | Auto-Approve Threshold | Human Required |\n|----------|---------------|----------------------|----------------|\n| Spec | Before `SPEC_NEEDS_APPROVAL` | 60% critic approval | Security veto OR <60% |\n| Code | Before goal marked complete | 60% critic approval | Security veto OR <60% |\n| Test | Before Verifier runs | 60% critic approval | Coverage <80% |\n\n### Implementation Tasks (6 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 11.1 | Create Critic base class and CriticScore | S | - |\n| 11.2 | Implement SpecCritic variants | M | 11.1 |\n| 11.3 | Implement CodeCritic variants | M | 11.1 |\n| 11.4 | Implement TestCritic variants | M | 11.1 |\n| 11.5 | Implement ValidationLayer consensus | M | 11.2-11.4 |\n| 11.6 | Integrate validation gates into pipelines | M | 11.5, Phase 7 |\n\n---\n\n## 7. Success Metrics\n\n| Metric | v1 Baseline | v2 Target | Measurement |\n|--------|-------------|-----------|-------------|\n| Real execution rate | 0% (stub) | 100% | Goals that call orchestrators |\n| Automatic recovery | 0% | 70% | Failures resolved without human |\n| Human review reduction | 100% manual | <30% | Artifacts bypassing review |\n| Multi-day completion | N/A | >75% | Campaigns finished as planned |\n| Learning improvement | None | +10%/month | Goal completion rate trend |\n| Cost efficiency | Unmeasured | Track | $/completed goal trend |\n| Checkpoint response time | N/A | <30 min avg | Time from checkpoint to resolution |\n\n---\n\n## 8. Risk Mitigations\n\n| Risk | Mitigation | Owner |\n|------|------------|-------|\n| Runaway execution | Per-day + per-campaign budget caps, mandatory checkpoints | David's design |\n| Bad learning loops | Bounded weight changes (\u00b110% per update), rollback on degradation | Kanjun's design |\n| Infinite recovery | 4-level cap with forced escalation, circuit breakers | Shunyu's design |\n| Context bloat | Episode pruning (keep last 100), recency decay | Jerry's design |\n| Missed checkpoints | Explicit trigger thresholds including hiccups, default to checkpoint on uncertainty | P0 design |\n\n---\n\n## 9. Implementation Roadmap\n\n### Phase 7: Real Execution + Checkpoints (MVP - Do First)\n**10 issues** (5 execution + 5 checkpoint)\n\nThis is the foundation. Without real execution, v2 is just a more elaborate stub. Checkpoints are P0 - required for collaborative autonomy.\n\n### Phase 8: Hierarchical Recovery (Core Autonomy)\n**5 issues**\n\nEnables overnight runs. Combined with Phase 7, you can say \"work on this\" and trust it to make progress without babysitting.\n\n### Phase 9: Episode Memory + Reflexion + Preferences (Learning)\n**6 issues** (5 memory + 1 preference learning)\n\nCheap investment, high payoff. Every execution makes future executions better. Preference learning satisfies PRD requirement.\n\n### Phase 10: Multi-Day Campaigns + Weekly Planning (Strategic)\n**8 issues** (7 campaigns + 1 weekly)\n\nGraduate from \"daily tasks\" to \"build this feature over the week.\" Weekly planning satisfies PRD User Story #6.\n\n### Phase 11: Internal Validation (Scale)\n**6 issues**\n\nReduces human bottleneck. Most specs/code auto-approve; you only see the hard cases.\n\n**Total: 35 issues across 5 phases**\n\n---\n\n## 10. Expert Panel Final Statement\n\n**Harrison Chase:** \"The roadmap is pragmatic. Phase 7 \u2192 8 \u2192 9 builds a solid autonomous agent. Phases 10-11 add strategic capability. Ship Phase 7-9 first, validate, then proceed.\"\n\n**Jerry Liu:** \"Episode memory with simple retrieval is the right call for startup scale. Add embeddings when you have thousands of episodes, not before. Parallel execution can wait for v3.\"\n\n**Kanjun Qiu:** \"Bounded learning is critical. The 4-level recovery and weight caps prevent runaway self-modification. Good safety design. Preference learning from checkpoint decisions is a nice touch.\"\n\n**David Dohan:** \"I'm satisfied with the checkpoint system and escalation paths. The security veto in validation is essential. Don't compromise on that. The human-in-the-loop design with hiccup detection addresses my core concern about autonomous operation.\"\n\n**Shunyu Yao:** \"Simplified recovery is the right call. One alternative on failure, not three. Keep it lean until you have data showing you need more.\"\n\n---\n\n*Spec finalized: December 2025*\n*Expert Panel: LangChain, LlamaIndex, Imbue, Anthropic, Princeton*\n*Ready for implementation*\n<<<SPEC_END>>>\n\n<<<RUBRIC_START>>>\n{\n  \"round\": 2,\n  \"previous_scores\": {\"clarity\": 0.65, \"coverage\": 0.45, \"architecture\": 0.5, \"risk\": 0.45},\n  \"current_scores\": {\"clarity\": 0.85, \"coverage\": 0.80, \"architecture\": 0.85, \"risk\": 0.80},\n  \"issues_accepted\": 3,\n  \"issues_rejected\": 1,\n  \"issues_deferred\": 0,\n  \"issues_partial\": 0,\n  \"continue_debate\": false,\n  \"ready_for_approval\": true,\n  \"meta\": {\n    \"recommend_human_review\": true,\n    \"review_reason\": \"Parallel execution (PRD User Story #3) has been rejected twice consecutively. The spec defers this to v3 per expert panel guidance (Jerry Liu's warning about codebase parallelism). Human should confirm this deferral is acceptable or provide direction on minimal parallel support.\"\n  },\n  \"dispute_resolutions\": []\n}\n<<<RUBRIC_END>>>",
  "session_id": "cac924a4-585b-46d7-b29c-b9c21c3a0923",
  "total_cost_usd": 1.01408475,
  "usage": {
    "input_tokens": 2,
    "cache_creation_input_tokens": 45377,
    "cache_read_input_tokens": 12995,
    "output_tokens": 25350,
    "server_tool_use": {
      "web_search_requests": 0,
      "web_fetch_requests": 0
    },
    "service_tier": "standard",
    "cache_creation": {
      "ephemeral_1h_input_tokens": 0,
      "ephemeral_5m_input_tokens": 45377
    }
  },
  "modelUsage": {
    "claude-haiku-4-5-20251001": {
      "inputTokens": 3901,
      "outputTokens": 125,
      "cacheReadInputTokens": 0,
      "cacheCreationInputTokens": 0,
      "webSearchRequests": 0,
      "costUSD": 0.004526,
      "contextWindow": 200000
    },
    "claude-opus-4-5-20251101": {
      "inputTokens": 4031,
      "outputTokens": 27972,
      "cacheReadInputTokens": 12995,
      "cacheCreationInputTokens": 45377,
      "webSearchRequests": 0,
      "costUSD": 1.00955875,
      "contextWindow": 200000
    }
  },
  "permission_denials": [],
  "uuid": "2d18bfbc-18d1-4063-8d87-b47557d923b7"
}