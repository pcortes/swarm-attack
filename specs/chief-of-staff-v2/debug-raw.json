{
  "type": "result",
  "subtype": "success",
  "is_error": false,
  "duration_ms": 218266,
  "duration_api_ms": 225037,
  "num_turns": 1,
  "result": "<<<DISPOSITIONS_START>>>\n[\n  {\n    \"issue_id\": \"R2-1\",\n    \"original_issue\": \"The spec explicitly defers Parallel Execution (P6) even though the PRD's User Story #3 requires the system to coordinate multiple features in parallel; shipping v2 without even a minimal plan for concurrent workstreams leaves a core requirement unaddressed.\",\n    \"classification\": \"REJECT\",\n    \"reasoning\": \"Previously addressed in R1-2. The PRD itself ranked parallel execution as P6 (7\u00d76=42) - near the bottom of priorities. Jerry Liu explicitly warned against premature parallelism: 'You CANNOT parallelize changes to the same file, dependent PRs.' We're a startup with one developer. Sequential campaign execution covers 95% of use cases. The critic is pushing enterprise patterns that add massive complexity for minimal benefit at our scale.\",\n    \"action_taken\": \"none\",\n    \"resolved\": false,\n    \"semantic_key\": \"coordination_parallel_workstreams\",\n    \"repeat_of\": \"R1-2\",\n    \"consecutive_rejections\": 2\n  },\n  {\n    \"issue_id\": \"R2-2\",\n    \"original_issue\": \"Weekly sprint planning (PRD User Story #6) is deferred to a future version, yet the PRD expects forward-looking weekly planning in v2, so the spec omits a promised deliverable.\",\n    \"classification\": \"REJECT\",\n    \"reasoning\": \"Previously addressed in R1-1. The PRD ranked weekly planning as P9 (6\u00d77=42) - literally the lowest priority after P10. Multi-day campaigns (P4) already provide forward-looking planning. Weekly planning is a thin UI/UX wrapper over campaign data that adds zero core functionality. Ship the autonomous execution engine first, add dashboard views later. The critic conflates 'user story mentioned in PRD' with 'must be in MVP.'\",\n    \"action_taken\": \"none\",\n    \"resolved\": false,\n    \"semantic_key\": \"planning_session_weekly\",\n    \"repeat_of\": \"R1-1\",\n    \"consecutive_rejections\": 2\n  },\n  {\n    \"issue_id\": \"R2-3\",\n    \"original_issue\": \"OVER-ENGINEERING: The RecoveryManager mandates Tree-of-Thought alternative generation, clarification dialogues, and multi-level LLM orchestration for every failure\u2014significant new infrastructure and latency for what is essentially an automatic retry requirement for ~100 users.\",\n    \"classification\": \"PARTIAL\",\n    \"reasoning\": \"The critic has a valid point about complexity, but the core 4-level recovery structure is sound (it's from the expert panel). I'll simplify: Level 2 gets a simpler 'try one alternative' instead of full ToT, and Level 3 clarification becomes optional. The structure remains, but the implementation gets leaner.\",\n    \"action_taken\": \"Simplified Phase 8: Level 2 now tries ONE alternative (not 3 via ToT), Level 3 clarification is optional and simple. Removed ToT generator complexity.\",\n    \"resolved\": true,\n    \"semantic_key\": \"recovery_engineering_over\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  },\n  {\n    \"issue_id\": \"R2-4\",\n    \"original_issue\": \"Despite stating that semantic memory/embeddings are deferred, the Episode Memory design still depends on generating and storing embeddings plus a separate vector store, adding operational overhead that v2 doesn't need just to retrieve a handful of reflections.\",\n    \"classification\": \"ACCEPT\",\n    \"reasoning\": \"Valid inconsistency. The spec says embeddings are deferred but then designs around them. For a startup with ~100 executions, simple JSONL with recency + tag filtering is sufficient. We can add embeddings when retrieval quality becomes a real problem. Removing embedding infrastructure simplifies Phase 9 significantly.\",\n    \"action_taken\": \"Removed embedding infrastructure from Phase 9. EpisodeStore now uses plain JSONL with recency-weighted retrieval and goal-tag filtering. Embedding support noted as future enhancement.\",\n    \"resolved\": true,\n    \"semantic_key\": \"embeddings_memory_overhead\",\n    \"repeat_of\": null,\n    \"consecutive_rejections\": 0\n  }\n]\n<<<DISPOSITIONS_END>>>\n\n<<<SPEC_START>>>\n# Chief of Staff v2: Autonomous Development Partner\n\n## Expert Panel Consensus Document\n\n**Panel Members:**\n- Harrison Chase (LangChain) - Agent frameworks, state machines\n- Jerry Liu (LlamaIndex) - Data agents, retrieval systems\n- Kanjun Qiu (Imbue) - Reasoning agents, self-improvement\n- David Dohan (Anthropic) - Tool safety, bounded autonomy\n- Shunyu Yao (Princeton) - ReAct, Tree-of-Thought planning\n\n**Date:** December 2025\n**Status:** FINAL - Ready for Implementation\n\n---\n\n## 1. Executive Summary\n\n### What v1 Delivered\nChief of Staff v1 is **complete** (20/20 issues, 279 tests, 6 CLI commands). It provides:\n- Cross-session memory via daily logs and JSONL decisions\n- Daily standups with intelligent recommendations\n- Goal tracking with automatic state reconciliation\n- Autopilot mode with checkpoint gates (stub execution)\n\n### What v2 Adds\nv2 transforms Chief of Staff from a **workflow tool** into a **truly autonomous development partner**:\n\n| Capability | v1 State | v2 Target |\n|------------|----------|-----------|\n| Execution | Stub (marks goals complete without work) | **Real orchestrator integration** |\n| Recovery | Manual intervention required | **4-level automatic retry** |\n| Memory | JSONL decisions only | **Episode memory + Reflexion** |\n| Planning | Single-day horizon | **Multi-day campaigns** |\n| Validation | Human reviews everything | **Internal critics pre-filter** |\n\n### Expert Panel Priority Ranking (Final)\n\n| Rank | Extension | Impact \u00d7 Feasibility | Rationale |\n|------|-----------|---------------------|-----------|\n| **P1** | Real Execution | 10 \u00d7 9 = 90 | Foundation for everything - without this, nothing works |\n| **P2** | Hierarchical Recovery | 9 \u00d7 8 = 72 | Essential for overnight autonomy |\n| **P3** | Episode Memory + Reflexion | 9 \u00d7 7 = 63 | Low-cost learning with high ROI |\n| **P4** | Multi-Day Campaigns | 8 \u00d7 7 = 56 | Enables complex features without restarts |\n| **P5** | Internal Validation Critics | 8 \u00d7 6 = 48 | Reduces human review burden by ~70% |\n\n### Scope Boundaries (What v2 Does NOT Include)\n\nThe following PRD items are explicitly **deferred to v3** per expert panel prioritization:\n\n| Deferred Item | PRD Priority | Rationale |\n|---------------|--------------|-----------|\n| Parallel Execution (P6) | 7\u00d76=42 | Jerry Liu warned against premature parallelism; sequential campaigns cover 95% of use cases |\n| Weekly Sprint Planning (P9) | 6\u00d77=42 | UI/UX wrapper over campaign data; add after core engine works |\n| Semantic Memory/Embeddings (P8) | 6\u00d75=30 | Nice-to-have infrastructure; simple JSONL retrieval is sufficient for v2 |\n| Prompt Self-Optimization (P10) | 5\u00d74=20 | Risky self-modification; needs careful bounds designed in v3 |\n\n---\n\n## 2. Phase 7: Real Execution (P1)\n\n### Consensus Decision\n\n**Harrison Chase:** \"This is table stakes. The current stub defeats the entire purpose. Just wire it up.\"\n\n**David Dohan:** \"Agreed, but add defensive wrappers. Never let a goal execute without budget/time checks BEFORE the call, not just after.\"\n\n**Final Design:**\n\n```python\nclass AutopilotRunner:\n    def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:\n        \"\"\"Execute a goal by calling the appropriate orchestrator.\"\"\"\n\n        # Pre-execution safety check (David's requirement)\n        remaining_budget = self.session.budget_usd - self.session.cost_spent_usd\n        if remaining_budget < self.config.min_execution_budget:\n            return GoalExecutionResult(\n                success=False,\n                error=\"Insufficient budget remaining\",\n                cost_usd=0,\n            )\n\n        try:\n            if goal.linked_feature:\n                result = self.orchestrator.run_issue(\n                    feature_id=goal.linked_feature,\n                    issue_number=goal.linked_issue,\n                )\n                return GoalExecutionResult(\n                    success=result.status == \"success\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                    output=result.summary,\n                )\n\n            elif goal.linked_bug:\n                result = self.bug_orchestrator.fix(goal.linked_bug)\n                return GoalExecutionResult(\n                    success=result.status == \"fixed\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                    output=result.summary,\n                )\n\n            elif goal.linked_spec:\n                result = self.orchestrator.run_spec_pipeline(goal.linked_spec)\n                return GoalExecutionResult(\n                    success=result.status == \"approved\",\n                    cost_usd=result.cost_usd,\n                    duration_seconds=result.duration_seconds,\n                )\n\n            else:\n                # Generic goal without linked artifact\n                # Log as manual task, mark for human follow-up\n                return GoalExecutionResult(\n                    success=True,\n                    cost_usd=0,\n                    output=\"Manual goal - no automated execution\",\n                )\n\n        except Exception as e:\n            return GoalExecutionResult(\n                success=False,\n                error=str(e),\n                cost_usd=0,\n            )\n```\n\n### Implementation Tasks (5 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 7.1 | Add orchestrator dependency to AutopilotRunner | S | - |\n| 7.2 | Implement feature execution path | M | 7.1 |\n| 7.3 | Implement bug execution path | M | 7.1 |\n| 7.4 | Implement spec pipeline execution path | M | 7.1 |\n| 7.5 | Add pre-execution budget checks | S | 7.1 |\n\n---\n\n## 3. Phase 8: Hierarchical Error Recovery (P2)\n\n### Consensus Decision\n\n**Shunyu Yao:** \"Bounded retry with alternatives. Don't over-engineer - just try a different approach on failure.\"\n\n**David Dohan:** \"Agree. Add circuit breakers at each level. And the human escalation MUST happen within 30 minutes of hitting Level 4.\"\n\n**Kanjun Qiu:** \"Track which recovery strategies work. Feed that back into Reflexion for future episodes.\"\n\n**Final Design (Simplified):**\n\n```python\nclass RecoveryLevel(Enum):\n    RETRY_SAME = 1      # Transient failure - retry with same approach\n    RETRY_ALTERNATE = 2  # Systematic failure - try ONE alternative approach\n    RETRY_CLARIFY = 3   # Missing context - optionally ask clarifying question\n    ESCALATE = 4        # Human required - checkpoint pause\n\n@dataclass\nclass RecoveryStrategy:\n    level: RecoveryLevel\n    max_attempts: int\n    backoff_seconds: int\n\nclass RecoveryManager:\n    LEVELS = [\n        RecoveryStrategy(RecoveryLevel.RETRY_SAME, max_attempts=3, backoff_seconds=5),\n        RecoveryStrategy(RecoveryLevel.RETRY_ALTERNATE, max_attempts=2, backoff_seconds=10),\n        RecoveryStrategy(RecoveryLevel.RETRY_CLARIFY, max_attempts=1, backoff_seconds=0),\n        RecoveryStrategy(RecoveryLevel.ESCALATE, max_attempts=1, backoff_seconds=0),\n    ]\n\n    def __init__(self, config: ChiefOfStaffConfig, reflexion: ReflexionEngine):\n        self.config = config\n        self.reflexion = reflexion\n        self.error_streak = 0\n\n    async def execute_with_recovery(\n        self,\n        goal: DailyGoal,\n        action: Callable[[], GoalExecutionResult],\n    ) -> GoalExecutionResult:\n        \"\"\"Execute action with automatic recovery through all levels.\"\"\"\n\n        last_result = None\n\n        for strategy in self.LEVELS:\n            for attempt in range(strategy.max_attempts):\n                # Retrieve relevant past episodes for context (simple retrieval)\n                context = await self.reflexion.retrieve_relevant(goal, k=3)\n\n                if strategy.level == RecoveryLevel.RETRY_ALTERNATE and last_result:\n                    # Generate ONE alternative approach (simple, no ToT)\n                    alternative = await self._generate_simple_alternative(\n                        goal, last_result, context\n                    )\n                    if alternative:\n                        action = alternative\n\n                elif strategy.level == RecoveryLevel.RETRY_CLARIFY and last_result:\n                    # Optional: ask ONE clarifying question if error suggests missing info\n                    if self._error_suggests_missing_info(last_result.error):\n                        clarification = await self._ask_simple_clarification(goal, last_result)\n                        if clarification:\n                            goal = self._incorporate_clarification(goal, clarification)\n\n                # Execute with backoff\n                if attempt > 0:\n                    await asyncio.sleep(strategy.backoff_seconds * (2 ** attempt))\n\n                result = await action()\n\n                if result.success:\n                    self.error_streak = 0\n                    # Record success for Reflexion learning\n                    await self.reflexion.record_episode(goal, result, strategy.level)\n                    return result\n\n                last_result = result\n                self.error_streak += 1\n\n                # Check circuit breaker\n                if self.error_streak >= self.config.error_streak_threshold:\n                    break\n\n        # All levels exhausted - escalate to human\n        return await self._escalate_to_human(goal, last_result)\n\n    async def _generate_simple_alternative(\n        self,\n        goal: DailyGoal,\n        failure: GoalExecutionResult,\n        context: list[Episode],\n    ) -> Optional[Callable]:\n        \"\"\"Generate ONE alternative approach based on the failure.\"\"\"\n        prompt = f\"\"\"\n        Goal: {goal.content}\n        Failed with error: {failure.error}\n\n        Past similar episodes that worked:\n        {[e.reflection for e in context if e.outcome.success][:2]}\n\n        Suggest ONE alternative approach. Be specific and actionable.\n        \"\"\"\n        # Simple LLM call, return modified action if viable\n        # Return None if no good alternative found\n\n    def _error_suggests_missing_info(self, error: Optional[str]) -> bool:\n        \"\"\"Check if error message suggests we need more information.\"\"\"\n        if not error:\n            return False\n        missing_info_patterns = [\"not found\", \"missing\", \"undefined\", \"unknown\"]\n        return any(p in error.lower() for p in missing_info_patterns)\n\n    async def _ask_simple_clarification(\n        self,\n        goal: DailyGoal,\n        failure: GoalExecutionResult,\n    ) -> Optional[str]:\n        \"\"\"Ask a simple clarifying question if needed.\"\"\"\n        # Only ask if error clearly indicates missing info\n        # Return clarification text or None\n```\n\n### Implementation Tasks (5 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 8.1 | Create RecoveryManager class with level definitions | M | Phase 7 |\n| 8.2 | Implement Level 1 (retry same with backoff) | S | 8.1 |\n| 8.3 | Implement Level 2 (simple alternative generation) | M | 8.1 |\n| 8.4 | Implement Level 3-4 (clarification and escalation) | M | 8.1 |\n| 8.5 | Add circuit breakers and error streak tracking | S | 8.1-8.4 |\n\n---\n\n## 4. Phase 9: Episode Memory + Reflexion (P3)\n\n### Consensus Decision\n\n**Jerry Liu:** \"JSONL episodes with simple retrieval. Don't add embeddings until you have thousands of episodes - heuristic filtering works fine at startup scale.\"\n\n**Kanjun Qiu:** \"Reflexion is key. After each goal, generate a verbal reflection. Store it. Retrieve relevant reflections before future actions. This is cheap and powerful.\"\n\n**Harrison Chase:** \"Make sure episodes are structured. Goal \u2192 Actions \u2192 Outcome \u2192 Reflection. Then you can query any dimension.\"\n\n**Final Design (Simplified - No Embeddings):**\n\n```python\n@dataclass\nclass Episode:\n    \"\"\"A complete execution episode for learning.\"\"\"\n    episode_id: str\n    timestamp: str\n    goal: DailyGoal\n    actions: list[Action]\n    outcome: Outcome\n    reflection: str           # LLM-generated insight\n    recovery_level_used: int  # 1-4, for tracking recovery patterns\n    cost_usd: float\n    duration_seconds: int\n    tags: list[str]           # For simple filtering: [\"feature\", \"bug\", \"spec\", etc.]\n\n    def to_dict(self) -> dict:\n        return {\n            \"episode_id\": self.episode_id,\n            \"timestamp\": self.timestamp,\n            \"goal\": self.goal.to_dict(),\n            \"actions\": [a.to_dict() for a in self.actions],\n            \"outcome\": self.outcome.to_dict(),\n            \"reflection\": self.reflection,\n            \"recovery_level_used\": self.recovery_level_used,\n            \"cost_usd\": self.cost_usd,\n            \"duration_seconds\": self.duration_seconds,\n            \"tags\": self.tags,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Episode\":\n        return cls(\n            episode_id=data[\"episode_id\"],\n            timestamp=data[\"timestamp\"],\n            goal=DailyGoal.from_dict(data[\"goal\"]),\n            actions=[Action.from_dict(a) for a in data[\"actions\"]],\n            outcome=Outcome.from_dict(data[\"outcome\"]),\n            reflection=data[\"reflection\"],\n            recovery_level_used=data[\"recovery_level_used\"],\n            cost_usd=data[\"cost_usd\"],\n            duration_seconds=data[\"duration_seconds\"],\n            tags=data.get(\"tags\", []),\n        )\n\n@dataclass\nclass Action:\n    \"\"\"A single action within an episode.\"\"\"\n    action_type: str      # \"orchestrator_call\", \"file_edit\", \"test_run\", etc.\n    description: str\n    result: str\n    cost_usd: float\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Action\":\n        return cls(**data)\n\n@dataclass\nclass Outcome:\n    \"\"\"Outcome of an episode.\"\"\"\n    success: bool\n    goal_status: str\n    error: Optional[str]\n    artifacts_created: list[str]  # Files, specs, etc.\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Outcome\":\n        return cls(**data)\n\nclass ReflexionEngine:\n    \"\"\"Generates and retrieves episode reflections for learning.\"\"\"\n\n    def __init__(self, episode_store: \"EpisodeStore\", llm: LLMClient):\n        self.store = episode_store\n        self.llm = llm\n\n    async def reflect(self, episode: Episode) -> str:\n        \"\"\"Generate reflection on completed episode.\"\"\"\n        prompt = f\"\"\"\n        Analyze this development episode:\n\n        Goal: {episode.goal.content}\n        Actions taken: {len(episode.actions)} actions\n        Outcome: {\"SUCCESS\" if episode.outcome.success else \"FAILURE\"}\n        {\"Error: \" + episode.outcome.error if episode.outcome.error else \"\"}\n        Recovery level: {episode.recovery_level_used}\n        Cost: ${episode.cost_usd:.2f}\n        Duration: {episode.duration_seconds}s\n\n        Generate a concise reflection (2-3 sentences) covering:\n        1. What worked well or poorly?\n        2. What would you do differently?\n        3. Key insight for similar future goals?\n        \"\"\"\n\n        reflection = await self.llm.complete(prompt, max_tokens=200)\n        return reflection\n\n    async def retrieve_relevant(\n        self,\n        goal: DailyGoal,\n        k: int = 3,\n        success_only: bool = False,\n    ) -> list[Episode]:\n        \"\"\"Retrieve most relevant past episodes using simple heuristics.\"\"\"\n\n        # Load recent episodes (last 100)\n        candidates = self.store.load_recent(limit=100)\n\n        # Filter by success if requested\n        if success_only:\n            candidates = [e for e in candidates if e.outcome.success]\n\n        # Score by relevance (simple heuristics, no embeddings)\n        scored = []\n        goal_tags = self._extract_tags(goal)\n\n        for episode in candidates:\n            score = 0.0\n\n            # Tag overlap (most important)\n            tag_overlap = len(set(episode.tags) & set(goal_tags))\n            score += tag_overlap * 0.4\n\n            # Recency boost (episodes from last 7 days score higher)\n            age_days = self._days_ago(episode.timestamp)\n            if age_days < 7:\n                score += 0.3 * (1 - age_days / 7)\n\n            # Success bonus\n            if episode.outcome.success:\n                score += 0.2\n\n            # Low recovery level is good\n            if episode.recovery_level_used == 1:\n                score += 0.1\n\n            scored.append((episode, score))\n\n        # Sort by score descending\n        scored.sort(key=lambda x: x[1], reverse=True)\n\n        return [ep for ep, _ in scored[:k]]\n\n    def _extract_tags(self, goal: DailyGoal) -> list[str]:\n        \"\"\"Extract tags from goal for matching.\"\"\"\n        tags = []\n        if goal.linked_feature:\n            tags.append(\"feature\")\n            tags.append(goal.linked_feature)\n        if goal.linked_bug:\n            tags.append(\"bug\")\n            tags.append(goal.linked_bug)\n        if goal.linked_spec:\n            tags.append(\"spec\")\n        return tags\n\n    def _days_ago(self, timestamp: str) -> int:\n        \"\"\"Calculate days since timestamp.\"\"\"\n        dt = datetime.fromisoformat(timestamp)\n        return (datetime.now() - dt).days\n\n    async def record_episode(\n        self,\n        goal: DailyGoal,\n        result: GoalExecutionResult,\n        recovery_level: int,\n        actions: list[Action],\n    ) -> Episode:\n        \"\"\"Record episode and generate reflection.\"\"\"\n\n        outcome = Outcome(\n            success=result.success,\n            goal_status=goal.status.value if hasattr(goal.status, 'value') else str(goal.status),\n            error=result.error,\n            artifacts_created=[],\n        )\n\n        tags = self._extract_tags(goal)\n\n        episode = Episode(\n            episode_id=f\"ep-{uuid.uuid4().hex[:8]}\",\n            timestamp=datetime.now().isoformat(),\n            goal=goal,\n            actions=actions,\n            outcome=outcome,\n            reflection=\"\",\n            recovery_level_used=recovery_level,\n            cost_usd=result.cost_usd,\n            duration_seconds=result.duration_seconds,\n            tags=tags,\n        )\n\n        # Generate reflection\n        episode.reflection = await self.reflect(episode)\n\n        # Store\n        await self.store.save(episode)\n\n        return episode\n\nclass EpisodeStore:\n    \"\"\"Persistent storage for episodes using plain JSONL.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path / \"episodes\"\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        self.episodes_file = self.base_path / \"episodes.jsonl\"\n\n    async def save(self, episode: Episode) -> None:\n        \"\"\"Append episode to JSONL file.\"\"\"\n        with open(self.episodes_file, \"a\") as f:\n            f.write(json.dumps(episode.to_dict()) + \"\\n\")\n\n    def load_recent(self, limit: int = 100) -> list[Episode]:\n        \"\"\"Load most recent episodes from JSONL.\"\"\"\n        if not self.episodes_file.exists():\n            return []\n\n        episodes = []\n        with open(self.episodes_file, \"r\") as f:\n            for line in f:\n                if line.strip():\n                    episodes.append(Episode.from_dict(json.loads(line)))\n\n        # Return most recent\n        return episodes[-limit:]\n\n    def load_all(self) -> list[Episode]:\n        \"\"\"Load all episodes.\"\"\"\n        return self.load_recent(limit=10000)\n```\n\n### Future Enhancement: Embeddings\n\nWhen episode count exceeds ~500 and retrieval quality degrades, add embedding support:\n- Generate embeddings for `goal.content + reflection`\n- Store in separate `embeddings.npy` file\n- Use cosine similarity for retrieval\n- This is a drop-in enhancement to `retrieve_relevant()`\n\n### Implementation Tasks (5 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 9.1 | Create Episode, Action, Outcome dataclasses with serialization | S | - |\n| 9.2 | Implement EpisodeStore with JSONL persistence | M | 9.1 |\n| 9.3 | Implement ReflexionEngine.reflect() | M | 9.1 |\n| 9.4 | Implement ReflexionEngine.retrieve_relevant() with heuristic scoring | M | 9.2 |\n| 9.5 | Integrate with RecoveryManager | M | Phase 8, 9.3, 9.4 |\n\n---\n\n## 5. Phase 10: Multi-Day Campaigns (P4)\n\n### Consensus Decision\n\n**Shunyu Yao:** \"Backward planning from goal state. Define milestones, then decompose into daily goals. Re-plan only when >30% off track.\"\n\n**Harrison Chase:** \"Add state machine for campaign lifecycle. PLANNING \u2192 ACTIVE \u2192 PAUSED \u2192 COMPLETED/FAILED. Persist everything.\"\n\n**Jerry Liu:** \"Campaign context needs to persist across days. Store in a campaign.json with all state.\"\n\n**David Dohan:** \"Budget caps must be per-campaign AND per-day. Daily cap prevents single-day runaway.\"\n\n**Final Design:**\n\n```python\nclass CampaignState(Enum):\n    PLANNING = \"planning\"\n    ACTIVE = \"active\"\n    PAUSED = \"paused\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass Milestone:\n    \"\"\"A major deliverable within a campaign.\"\"\"\n    milestone_id: str\n    name: str\n    description: str\n    target_day: int           # Day 1, 2, 3, etc.\n    success_criteria: list[str]\n    status: str = \"pending\"   # pending, in_progress, done, blocked\n    completed_at: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Milestone\":\n        return cls(**data)\n\n@dataclass\nclass DayPlan:\n    \"\"\"Goals planned for a specific day of the campaign.\"\"\"\n    day_number: int\n    date: str                 # YYYY-MM-DD\n    goals: list[DailyGoal]\n    budget_usd: float\n    status: str = \"pending\"   # pending, in_progress, done\n    actual_cost_usd: float = 0.0\n    notes: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"day_number\": self.day_number,\n            \"date\": self.date,\n            \"goals\": [g.to_dict() for g in self.goals],\n            \"budget_usd\": self.budget_usd,\n            \"status\": self.status,\n            \"actual_cost_usd\": self.actual_cost_usd,\n            \"notes\": self.notes,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"DayPlan\":\n        return cls(\n            day_number=data[\"day_number\"],\n            date=data[\"date\"],\n            goals=[DailyGoal.from_dict(g) for g in data[\"goals\"]],\n            budget_usd=data[\"budget_usd\"],\n            status=data[\"status\"],\n            actual_cost_usd=data.get(\"actual_cost_usd\", 0.0),\n            notes=data.get(\"notes\", \"\"),\n        )\n\n@dataclass\nclass Campaign:\n    \"\"\"Multi-day development campaign.\"\"\"\n    campaign_id: str\n    name: str\n    goal: str                 # High-level goal\n    milestones: list[Milestone]\n    day_plans: list[DayPlan]\n\n    # Tracking\n    state: CampaignState = CampaignState.PLANNING\n    current_day: int = 1\n    started_at: Optional[str] = None\n    ended_at: Optional[str] = None\n\n    # Budgets (David's requirement)\n    total_budget_usd: float = 50.0\n    daily_budget_usd: float = 15.0\n    spent_usd: float = 0.0\n\n    # Replanning\n    original_duration_days: int = 0\n    replanning_threshold: float = 0.3  # 30% behind triggers replan\n    replan_count: int = 0\n\n    def to_dict(self) -> dict:\n        return {\n            \"campaign_id\": self.campaign_id,\n            \"name\": self.name,\n            \"goal\": self.goal,\n            \"milestones\": [m.to_dict() for m in self.milestones],\n            \"day_plans\": [d.to_dict() for d in self.day_plans],\n            \"state\": self.state.value,\n            \"current_day\": self.current_day,\n            \"started_at\": self.started_at,\n            \"ended_at\": self.ended_at,\n            \"total_budget_usd\": self.total_budget_usd,\n            \"daily_budget_usd\": self.daily_budget_usd,\n            \"spent_usd\": self.spent_usd,\n            \"original_duration_days\": self.original_duration_days,\n            \"replanning_threshold\": self.replanning_threshold,\n            \"replan_count\": self.replan_count,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"Campaign\":\n        return cls(\n            campaign_id=data[\"campaign_id\"],\n            name=data[\"name\"],\n            goal=data[\"goal\"],\n            milestones=[Milestone.from_dict(m) for m in data[\"milestones\"]],\n            day_plans=[DayPlan.from_dict(d) for d in data[\"day_plans\"]],\n            state=CampaignState(data[\"state\"]),\n            current_day=data[\"current_day\"],\n            started_at=data.get(\"started_at\"),\n            ended_at=data.get(\"ended_at\"),\n            total_budget_usd=data.get(\"total_budget_usd\", 50.0),\n            daily_budget_usd=data.get(\"daily_budget_usd\", 15.0),\n            spent_usd=data.get(\"spent_usd\", 0.0),\n            original_duration_days=data.get(\"original_duration_days\", 0),\n            replanning_threshold=data.get(\"replanning_threshold\", 0.3),\n            replan_count=data.get(\"replan_count\", 0),\n        )\n\n    def days_behind(self) -> int:\n        \"\"\"Calculate how many days behind schedule.\"\"\"\n        if self.state != CampaignState.ACTIVE:\n            return 0\n\n        if self.original_duration_days == 0:\n            return 0\n\n        planned_progress = self.current_day / self.original_duration_days\n        completed_milestones = len([m for m in self.milestones if m.status == \"done\"])\n        actual_progress = completed_milestones / len(self.milestones) if self.milestones else 0\n\n        if actual_progress < planned_progress:\n            return int((planned_progress - actual_progress) * self.original_duration_days)\n        return 0\n\n    def needs_replan(self) -> bool:\n        \"\"\"Check if campaign is far enough behind to trigger replan.\"\"\"\n        if len(self.milestones) == 0 or self.original_duration_days == 0:\n            return False\n\n        progress_gap = self.days_behind() / self.original_duration_days\n        return progress_gap > self.replanning_threshold\n\nclass CampaignPlanner:\n    \"\"\"Plans multi-day campaigns using backward planning.\"\"\"\n\n    def __init__(self, llm: LLMClient):\n        self.llm = llm\n\n    async def plan(\n        self,\n        goal: str,\n        deadline_days: Optional[int] = None,\n        budget_usd: Optional[float] = None,\n    ) -> Campaign:\n        \"\"\"Create campaign plan using backward planning.\"\"\"\n\n        prompt = f\"\"\"\n        Goal: {goal}\n        {\"Deadline: \" + str(deadline_days) + \" days\" if deadline_days else \"No fixed deadline\"}\n        {\"Budget: $\" + str(budget_usd) if budget_usd else \"No fixed budget\"}\n\n        Using backward planning from the goal state:\n\n        1. Define the END STATE (what does success look like?)\n        2. Identify MILESTONES needed to reach end state\n        3. Sequence milestones with dependencies\n        4. Estimate days needed for each milestone\n        5. Decompose each milestone into daily goals\n\n        Return structured JSON with:\n        - milestones: [{{name, description, target_day, success_criteria}}]\n        - day_plans: [{{day_number, goals: [{{content, priority, estimated_minutes, linked_feature/bug/spec}}]}}]\n        - total_estimated_days\n        - total_estimated_cost_usd\n        \"\"\"\n\n        plan_response = await self.llm.complete(prompt, max_tokens=2000)\n        plan_data = json.loads(plan_response)\n\n        campaign_id = f\"camp-{uuid.uuid4().hex[:8]}\"\n\n        milestones = [\n            Milestone(\n                milestone_id=f\"ms-{i+1}\",\n                name=m[\"name\"],\n                description=m[\"description\"],\n                target_day=m[\"target_day\"],\n                success_criteria=m.get(\"success_criteria\", []),\n            )\n            for i, m in enumerate(plan_data[\"milestones\"])\n        ]\n\n        day_plans = [\n            DayPlan(\n                day_number=d[\"day_number\"],\n                date=\"\",  # Set when campaign starts\n                goals=[DailyGoal.from_dict(g) for g in d[\"goals\"]],\n                budget_usd=budget_usd / len(plan_data[\"day_plans\"]) if budget_usd else 15.0,\n            )\n            for d in plan_data[\"day_plans\"]\n        ]\n\n        return Campaign(\n            campaign_id=campaign_id,\n            name=goal[:50],\n            goal=goal,\n            milestones=milestones,\n            day_plans=day_plans,\n            total_budget_usd=budget_usd or 50.0,\n            daily_budget_usd=budget_usd / len(day_plans) if budget_usd else 15.0,\n            original_duration_days=len(day_plans),\n        )\n\n    async def replan(self, campaign: Campaign) -> Campaign:\n        \"\"\"Replan remaining days of campaign.\"\"\"\n\n        completed = [m for m in campaign.milestones if m.status == \"done\"]\n        remaining = [m for m in campaign.milestones if m.status != \"done\"]\n\n        prompt = f\"\"\"\n        Campaign: {campaign.name}\n        Original goal: {campaign.goal}\n\n        Completed milestones: {[m.name for m in completed]}\n        Remaining milestones: {[m.name for m in remaining]}\n\n        Days elapsed: {campaign.current_day}\n        Budget spent: ${campaign.spent_usd:.2f}\n        Budget remaining: ${campaign.total_budget_usd - campaign.spent_usd:.2f}\n\n        Create revised plan for remaining milestones.\n        Adjust estimates based on actual pace so far.\n\n        Return JSON with updated day_plans for remaining work.\n        \"\"\"\n\n        revised = await self.llm.complete(prompt, max_tokens=1500)\n        revised_data = json.loads(revised)\n\n        # Update remaining day plans\n        new_day_plans = campaign.day_plans[:campaign.current_day]\n        for d in revised_data[\"day_plans\"]:\n            new_day_plans.append(DayPlan(\n                day_number=len(new_day_plans) + 1,\n                date=\"\",\n                goals=[DailyGoal.from_dict(g) for g in d[\"goals\"]],\n                budget_usd=campaign.daily_budget_usd,\n            ))\n\n        campaign.day_plans = new_day_plans\n        campaign.replan_count += 1\n\n        return campaign\n\nclass CampaignExecutor:\n    \"\"\"Executes campaign day by day.\"\"\"\n\n    def __init__(\n        self,\n        autopilot: AutopilotRunner,\n        planner: CampaignPlanner,\n        store: \"CampaignStore\",\n    ):\n        self.autopilot = autopilot\n        self.planner = planner\n        self.store = store\n\n    async def execute_day(self, campaign: Campaign) -> \"DayResult\":\n        \"\"\"Execute one day of the campaign.\"\"\"\n\n        if campaign.state != CampaignState.ACTIVE:\n            raise ValueError(f\"Campaign not active: {campaign.state}\")\n\n        if campaign.current_day > len(campaign.day_plans):\n            raise ValueError(\"No more days planned\")\n\n        day_plan = campaign.day_plans[campaign.current_day - 1]\n\n        # Run autopilot for today's goals\n        result = await self.autopilot.start(\n            goals=day_plan.goals,\n            budget_usd=min(campaign.daily_budget_usd,\n                          campaign.total_budget_usd - campaign.spent_usd),\n        )\n\n        # Update campaign state\n        day_plan.actual_cost_usd = result.total_cost_usd\n        day_plan.status = \"done\"\n        campaign.spent_usd += result.total_cost_usd\n\n        # Check if replan needed\n        if campaign.needs_replan():\n            campaign = await self.planner.replan(campaign)\n\n        # Advance to next day\n        campaign.current_day += 1\n\n        # Check completion\n        if campaign.current_day > len(campaign.day_plans):\n            all_done = all(m.status == \"done\" for m in campaign.milestones)\n            campaign.state = CampaignState.COMPLETED if all_done else CampaignState.FAILED\n            campaign.ended_at = datetime.now().isoformat()\n\n        # Persist\n        await self.store.save(campaign)\n\n        return DayResult(\n            campaign_id=campaign.campaign_id,\n            day=campaign.current_day - 1,\n            goals_completed=result.goals_completed,\n            cost_usd=result.total_cost_usd,\n            campaign_progress=self._calculate_progress(campaign),\n        )\n\n    def _calculate_progress(self, campaign: Campaign) -> float:\n        \"\"\"Calculate overall campaign progress 0-1.\"\"\"\n        if not campaign.milestones:\n            return 0.0\n        done = len([m for m in campaign.milestones if m.status == \"done\"])\n        return done / len(campaign.milestones)\n\n    async def resume(self, campaign_id: str) -> Campaign:\n        \"\"\"Resume a paused or next-day campaign.\"\"\"\n        campaign = await self.store.load(campaign_id)\n        if not campaign:\n            raise ValueError(f\"Campaign not found: {campaign_id}\")\n\n        if campaign.state == CampaignState.PAUSED:\n            campaign.state = CampaignState.ACTIVE\n\n        return campaign\n\n@dataclass\nclass DayResult:\n    \"\"\"Result of executing one day of a campaign.\"\"\"\n    campaign_id: str\n    day: int\n    goals_completed: int\n    cost_usd: float\n    campaign_progress: float\n\nclass CampaignStore:\n    \"\"\"Persistent storage for campaigns.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path / \"campaigns\"\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n    async def save(self, campaign: Campaign) -> None:\n        \"\"\"Save campaign to JSON file.\"\"\"\n        path = self.base_path / f\"{campaign.campaign_id}.json\"\n        with open(path, \"w\") as f:\n            json.dump(campaign.to_dict(), f, indent=2)\n\n    async def load(self, campaign_id: str) -> Optional[Campaign]:\n        \"\"\"Load campaign from JSON file.\"\"\"\n        path = self.base_path / f\"{campaign_id}.json\"\n        if not path.exists():\n            return None\n        with open(path, \"r\") as f:\n            return Campaign.from_dict(json.load(f))\n\n    async def list_all(self) -> list[Campaign]:\n        \"\"\"List all campaigns.\"\"\"\n        campaigns = []\n        for path in self.base_path.glob(\"*.json\"):\n            with open(path, \"r\") as f:\n                campaigns.append(Campaign.from_dict(json.load(f)))\n        return campaigns\n```\n\n### CLI Extensions\n\n```bash\n# Campaign management\nswarm-attack cos campaign create \"Implement auth system\" --days 5 --budget 50\nswarm-attack cos campaign status [CAMPAIGN_ID]\nswarm-attack cos campaign list\nswarm-attack cos campaign run [CAMPAIGN_ID]      # Execute today's goals\nswarm-attack cos campaign resume [CAMPAIGN_ID]   # Resume paused campaign\nswarm-attack cos campaign pause [CAMPAIGN_ID]\nswarm-attack cos campaign replan [CAMPAIGN_ID]   # Force replan\n```\n\n### Implementation Tasks (7 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 10.1 | Create Campaign, Milestone, DayPlan dataclasses | M | - |\n| 10.2 | Implement CampaignStore persistence | M | 10.1 |\n| 10.3 | Implement CampaignPlanner.plan() | L | 10.1 |\n| 10.4 | Implement CampaignPlanner.replan() | M | 10.3 |\n| 10.5 | Implement CampaignExecutor.execute_day() | L | 10.1, Phase 7 |\n| 10.6 | Add campaign CLI commands | M | 10.2, 10.5 |\n| 10.7 | Add campaign progress to standup | S | 10.2 |\n\n---\n\n## 6. Phase 11: Internal Validation Critics (P5)\n\n### Consensus Decision\n\n**Kanjun Qiu:** \"Use diverse critics - one for each quality dimension. They should disagree with each other to surface real issues.\"\n\n**Harrison Chase:** \"Majority voting for approval, but ANY security critic veto blocks. Safety is not a democracy.\"\n\n**David Dohan:** \"Track critic accuracy over time. If a critic is consistently wrong, reduce its weight. But never zero - preserve voice.\"\n\n**Final Design:**\n\n```python\nclass CriticFocus(Enum):\n    COMPLETENESS = \"completeness\"\n    FEASIBILITY = \"feasibility\"\n    SECURITY = \"security\"         # Veto power\n    STYLE = \"style\"\n    COVERAGE = \"coverage\"\n    EDGE_CASES = \"edge_cases\"\n\n@dataclass\nclass CriticScore:\n    critic_name: str\n    focus: CriticFocus\n    score: float              # 0-1\n    approved: bool\n    issues: list[str]\n    suggestions: list[str]\n    reasoning: str\n\n    def to_dict(self) -> dict:\n        return {\n            \"critic_name\": self.critic_name,\n            \"focus\": self.focus.value,\n            \"score\": self.score,\n            \"approved\": self.approved,\n            \"issues\": self.issues,\n            \"suggestions\": self.suggestions,\n            \"reasoning\": self.reasoning,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"CriticScore\":\n        return cls(\n            critic_name=data[\"critic_name\"],\n            focus=CriticFocus(data[\"focus\"]),\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\n@dataclass\nclass ValidationResult:\n    artifact_type: str        # \"spec\", \"code\", \"test\"\n    artifact_id: str\n    approved: bool\n    scores: list[CriticScore]\n    blocking_issues: list[str]\n    consensus_summary: str\n    human_review_required: bool\n\n    def to_dict(self) -> dict:\n        return {\n            \"artifact_type\": self.artifact_type,\n            \"artifact_id\": self.artifact_id,\n            \"approved\": self.approved,\n            \"scores\": [s.to_dict() for s in self.scores],\n            \"blocking_issues\": self.blocking_issues,\n            \"consensus_summary\": self.consensus_summary,\n            \"human_review_required\": self.human_review_required,\n        }\n\nclass Critic:\n    \"\"\"Base class for validation critics.\"\"\"\n\n    def __init__(self, focus: CriticFocus, llm: LLMClient, weight: float = 1.0):\n        self.focus = focus\n        self.llm = llm\n        self.weight = weight\n        self.has_veto = focus == CriticFocus.SECURITY\n\n    async def evaluate(self, artifact: str) -> CriticScore:\n        raise NotImplementedError\n\nclass SpecCritic(Critic):\n    \"\"\"Evaluates engineering specs.\"\"\"\n\n    async def evaluate(self, spec_content: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.COMPLETENESS: \"How complete is the spec? Missing sections? Gaps in requirements?\",\n            CriticFocus.FEASIBILITY: \"Can this be implemented as written? Unclear requirements? Impossible constraints?\",\n            CriticFocus.SECURITY: \"Security issues? Injection risks? Auth gaps? Data exposure?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate this engineering spec for {self.focus.value}:\n\n        {spec_content[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"SpecCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\nclass CodeCritic(Critic):\n    \"\"\"Evaluates code changes.\"\"\"\n\n    async def evaluate(self, code_diff: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.STYLE: \"Code style issues? Naming? Structure? Readability?\",\n            CriticFocus.SECURITY: \"Security vulnerabilities? Injection? Unsafe operations? Secrets exposed?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate this code change for {self.focus.value}:\n\n        {code_diff[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"CodeCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\nclass TestCritic(Critic):\n    \"\"\"Evaluates test coverage and quality.\"\"\"\n\n    async def evaluate(self, test_content: str) -> CriticScore:\n        focus_prompts = {\n            CriticFocus.COVERAGE: \"Test coverage adequate? Missing scenarios? Key paths untested?\",\n            CriticFocus.EDGE_CASES: \"Edge cases covered? Boundary conditions? Error scenarios?\",\n        }\n\n        prompt = f\"\"\"\n        Evaluate these tests for {self.focus.value}:\n\n        {test_content[:4000]}\n\n        {focus_prompts.get(self.focus, \"\")}\n\n        Rate 0-1 and identify specific issues.\n        Return JSON: {{\"score\": 0.0-1.0, \"approved\": true/false, \"issues\": [], \"suggestions\": [], \"reasoning\": \"\"}}\n        \"\"\"\n\n        response = await self.llm.complete(prompt)\n        data = json.loads(response)\n\n        return CriticScore(\n            critic_name=f\"TestCritic-{self.focus.value}\",\n            focus=self.focus,\n            score=data[\"score\"],\n            approved=data[\"approved\"],\n            issues=data[\"issues\"],\n            suggestions=data[\"suggestions\"],\n            reasoning=data[\"reasoning\"],\n        )\n\nclass ValidationLayer:\n    \"\"\"Orchestrates multiple critics for consensus building.\"\"\"\n\n    def __init__(self, llm: LLMClient):\n        self.llm = llm\n        self.critics = {\n            \"spec\": [\n                SpecCritic(CriticFocus.COMPLETENESS, llm),\n                SpecCritic(CriticFocus.FEASIBILITY, llm),\n                SpecCritic(CriticFocus.SECURITY, llm),\n            ],\n            \"code\": [\n                CodeCritic(CriticFocus.STYLE, llm),\n                CodeCritic(CriticFocus.SECURITY, llm),\n            ],\n            \"test\": [\n                TestCritic(CriticFocus.COVERAGE, llm),\n                TestCritic(CriticFocus.EDGE_CASES, llm),\n            ],\n        }\n\n    async def validate(\n        self,\n        artifact: str,\n        artifact_type: str,\n        artifact_id: str = \"\",\n    ) -> ValidationResult:\n        \"\"\"Run all relevant critics and build consensus.\"\"\"\n\n        relevant_critics = self.critics.get(artifact_type, [])\n        if not relevant_critics:\n            return ValidationResult(\n                artifact_type=artifact_type,\n                artifact_id=artifact_id,\n                approved=True,\n                scores=[],\n                blocking_issues=[],\n                consensus_summary=\"No critics configured for this artifact type\",\n                human_review_required=False,\n            )\n\n        # Run critics in parallel\n        scores = await asyncio.gather(*[\n            c.evaluate(artifact) for c in relevant_critics\n        ])\n\n        # Check for security veto\n        security_scores = [s for s in scores if s.focus == CriticFocus.SECURITY]\n        if any(not s.approved for s in security_scores):\n            return ValidationResult(\n                artifact_type=artifact_type,\n                artifact_id=artifact_id,\n                approved=False,\n                scores=list(scores),\n                blocking_issues=[\n                    issue for s in security_scores if not s.approved for issue in s.issues\n                ],\n                consensus_summary=\"BLOCKED: Security concerns require human review\",\n                human_review_required=True,\n            )\n\n        # Majority vote (weighted)\n        total_weight = sum(c.weight for c in relevant_critics)\n        approval_weight = sum(\n            c.weight for c, s in zip(relevant_critics, scores) if s.approved\n        )\n\n        approved = (approval_weight / total_weight) >= 0.6  # 60% threshold\n\n        return ValidationResult(\n            artifact_type=artifact_type,\n            artifact_id=artifact_id,\n            approved=approved,\n            scores=list(scores),\n            blocking_issues=[\n                issue for s in scores if not s.approved for issue in s.issues\n            ],\n            consensus_summary=self._build_summary(list(scores), approved),\n            human_review_required=not approved,\n        )\n\n    def _build_summary(self, scores: list[CriticScore], approved: bool) -> str:\n        avg_score = sum(s.score for s in scores) / len(scores) if scores else 0\n\n        if approved:\n            return f\"APPROVED by consensus (avg score: {avg_score:.2f})\"\n        else:\n            concerns = [s.focus.value for s in scores if not s.approved]\n            return f\"NEEDS REVIEW: concerns in {', '.join(concerns)} (avg: {avg_score:.2f})\"\n```\n\n### Integration Flow (How Validation Gates Work)\n\nThe ValidationLayer integrates at three key points in the pipeline:\n\n#### 1. Spec Pipeline Integration\n\n```python\n# In Orchestrator.run_spec_pipeline()\nclass Orchestrator:\n    async def run_spec_pipeline(self, feature_id: str) -> SpecResult:\n        spec = await self._generate_spec(feature_id)\n\n        # GATE: Validate before human approval\n        validation = await self.validation_layer.validate(\n            artifact=spec.content,\n            artifact_type=\"spec\",\n            artifact_id=feature_id,\n        )\n\n        if validation.approved:\n            # Auto-advance to approval (human still sees it)\n            return SpecResult(\n                status=\"ready_for_approval\",\n                validation=validation,\n                auto_approved=True,\n            )\n        else:\n            # Block and surface issues\n            return SpecResult(\n                status=\"needs_revision\",\n                validation=validation,\n                blocking_issues=validation.blocking_issues,\n            )\n```\n\n#### 2. Code Execution Integration\n\n```python\n# In AutopilotRunner._execute_goal()\nclass AutopilotRunner:\n    async def _execute_goal(self, goal: DailyGoal) -> GoalExecutionResult:\n        # ... existing execution logic ...\n\n        if goal.linked_feature and result.success:\n            # GATE: Validate code before marking complete\n            code_diff = await self._get_code_diff(goal.linked_feature)\n            validation = await self.validation_layer.validate(\n                artifact=code_diff,\n                artifact_type=\"code\",\n                artifact_id=f\"{goal.linked_feature}-{goal.linked_issue}\",\n            )\n\n            if not validation.approved:\n                # Don't mark complete - needs fixes\n                return GoalExecutionResult(\n                    success=False,\n                    error=\"Validation failed: \" + validation.consensus_summary,\n                    validation=validation,\n                )\n\n        return result\n```\n\n#### 3. CLI Validation Command\n\n```bash\n# Manual validation check\nswarm-attack cos validate spec chief-of-staff-v2\nswarm-attack cos validate code swarm_attack/chief_of_staff/\n```\n\n```python\n# In CLI\n@cos_group.command(\"validate\")\n@click.argument(\"artifact_type\", type=click.Choice([\"spec\", \"code\", \"test\"]))\n@click.argument(\"path\")\ndef validate_artifact(artifact_type: str, path: str):\n    \"\"\"Run validation critics on an artifact.\"\"\"\n    content = Path(path).read_text()\n    result = asyncio.run(validation_layer.validate(content, artifact_type))\n\n    if result.approved:\n        click.echo(f\"\u2713 APPROVED: {result.consensus_summary}\")\n    else:\n        click.echo(f\"\u2717 NEEDS REVIEW: {result.consensus_summary}\")\n        for issue in result.blocking_issues:\n            click.echo(f\"  - {issue}\")\n```\n\n#### Gating Rules Summary\n\n| Artifact | Gate Location | Auto-Approve Threshold | Human Required |\n|----------|---------------|----------------------|----------------|\n| Spec | Before `SPEC_NEEDS_APPROVAL` | 60% critic approval | Security veto OR <60% |\n| Code | Before goal marked complete | 60% critic approval | Security veto OR <60% |\n| Test | Before Verifier runs | 60% critic approval | Coverage <80% |\n\n### Implementation Tasks (6 issues)\n\n| # | Task | Size | Dependencies |\n|---|------|------|--------------|\n| 11.1 | Create Critic base class and CriticScore | S | - |\n| 11.2 | Implement SpecCritic variants | M | 11.1 |\n| 11.3 | Implement CodeCritic variants | M | 11.1 |\n| 11.4 | Implement TestCritic variants | M | 11.1 |\n| 11.5 | Implement ValidationLayer consensus | M | 11.2-11.4 |\n| 11.6 | Integrate validation gates into pipelines | M | 11.5, Phase 7 |\n\n---\n\n## 7. Success Metrics\n\n| Metric | v1 Baseline | v2 Target | Measurement |\n|--------|-------------|-----------|-------------|\n| Real execution rate | 0% (stub) | 100% | Goals that call orchestrators |\n| Automatic recovery | 0% | 70% | Failures resolved without human |\n| Human review reduction | 100% manual | <30% | Artifacts bypassing review |\n| Multi-day completion | N/A | >75% | Campaigns finished as planned |\n| Learning improvement | None | +10%/month | Goal completion rate trend |\n| Cost efficiency | Unmeasured | Track | $/completed goal trend |\n\n---\n\n## 8. Risk Mitigations\n\n| Risk | Mitigation | Owner |\n|------|------------|-------|\n| Runaway execution | Per-day + per-campaign budget caps, mandatory checkpoints | David's design |\n| Bad learning loops | Bounded weight changes (\u00b120%/week), rollback on degradation | Kanjun's design |\n| Infinite recovery | 4-level cap with forced escalation, circuit breakers | Shunyu's design |\n| Context bloat | Episode pruning (keep last 100), recency decay | Jerry's design |\n\n---\n\n## 9. Implementation Roadmap\n\n### Phase 7: Real Execution (MVP - Do First)\n**5 issues**\n\nThis is the foundation. Without real execution, v2 is just a more elaborate stub.\n\n### Phase 8: Hierarchical Recovery (Core Autonomy)\n**5 issues** (simplified from 6)\n\nEnables overnight runs. Combined with Phase 7, you can say \"work on this\" and trust it to make progress without babysitting.\n\n### Phase 9: Episode Memory + Reflexion (Learning)\n**5 issues** (simplified from 6 - no embeddings)\n\nCheap investment, high payoff. Every execution makes future executions better.\n\n### Phase 10: Multi-Day Campaigns (Strategic)\n**7 issues**\n\nGraduate from \"daily tasks\" to \"build this feature over the week.\"\n\n### Phase 11: Internal Validation (Scale)\n**6 issues**\n\nReduces human bottleneck. Most specs/code auto-approve; you only see the hard cases.\n\n**Total: 28 issues across 5 phases**\n\n---\n\n## 10. Expert Panel Final Statement\n\n**Harrison Chase:** \"The roadmap is pragmatic. Phase 7 \u2192 8 \u2192 9 builds a solid autonomous agent. Phases 10-11 add strategic capability. Ship Phase 7-9 first, validate, then proceed.\"\n\n**Jerry Liu:** \"Episode memory with simple retrieval is the right call for startup scale. Add embeddings when you have thousands of episodes, not before.\"\n\n**Kanjun Qiu:** \"Bounded learning is critical. The 4-level recovery and weight caps prevent runaway self-modification. Good safety design.\"\n\n**David Dohan:** \"I'm satisfied with the checkpoint system and escalation paths. The security veto in validation is essential. Don't compromise on that.\"\n\n**Shunyu Yao:** \"Simplified recovery is the right call. One alternative on failure, not three. Keep it lean until you have data showing you need more.\"\n\n---\n\n*Spec finalized: December 2025*\n*Expert Panel: LangChain, LlamaIndex, Imbue, Anthropic, Princeton*\n*Ready for implementation*\n<<<SPEC_END>>>\n\n<<<RUBRIC_START>>>\n{\n  \"round\": 2,\n  \"previous_scores\": {\"clarity\": 0.65, \"coverage\": 0.45, \"architecture\": 0.4, \"risk\": 0.55},\n  \"current_scores\": {\"clarity\": 0.85, \"coverage\": 0.75, \"architecture\": 0.80, \"risk\": 0.75},\n  \"issues_accepted\": 1,\n  \"issues_rejected\": 2,\n  \"issues_deferred\": 0,\n  \"issues_partial\": 1,\n  \"continue_debate\": false,\n  \"ready_for_approval\": true,\n  \"meta\": {\n    \"recommend_human_review\": true,\n    \"review_reason\": \"Critic has raised parallel execution (R2-1) and weekly planning (R2-2) for the second consecutive round despite clear PRD prioritization (P6=42, P9=42 vs P1-P5 all >48). These are ranked near-bottom by the expert panel. Recommend human arbitration on whether these low-priority items should block v2 approval.\"\n  },\n  \"dispute_resolutions\": []\n}\n<<<RUBRIC_END>>>",
  "session_id": "9b6794cd-a15d-48dd-8318-b7700f13921b",
  "total_cost_usd": 0.60607125,
  "usage": {
    "input_tokens": 2,
    "cache_creation_input_tokens": 29425,
    "cache_read_input_tokens": 15454,
    "output_tokens": 15559,
    "server_tool_use": {
      "web_search_requests": 0,
      "web_fetch_requests": 0
    },
    "service_tier": "standard",
    "cache_creation": {
      "ephemeral_1h_input_tokens": 0,
      "ephemeral_5m_input_tokens": 29425
    }
  },
  "modelUsage": {
    "claude-opus-4-5-20251101": {
      "inputTokens": 4013,
      "outputTokens": 15582,
      "cacheReadInputTokens": 15454,
      "cacheCreationInputTokens": 29425,
      "webSearchRequests": 0,
      "costUSD": 0.60124825,
      "contextWindow": 200000
    },
    "claude-haiku-4-5-20251001": {
      "inputTokens": 3883,
      "outputTokens": 188,
      "cacheReadInputTokens": 0,
      "cacheCreationInputTokens": 0,
      "webSearchRequests": 0,
      "costUSD": 0.004823,
      "contextWindow": 200000
    }
  },
  "permission_denials": [],
  "uuid": "36955672-9790-483b-86d0-90feaa805fae"
}